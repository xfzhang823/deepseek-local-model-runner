{
  "module_config": [
    {
      "prev_op": "Qwen2RMSNorm - Input Size: 1536, Epsilon: 1e-06",
      "layers": [
        "Linear - 1536 → 1536",
        "Linear - 1536 → 256",
        "Linear - 1536 → 256"
      ],
      "input_activation": "Tensor[bfloat16] - Shape: (batch_size, seq_len, 1536)",
      "module2inspect": {
        "type": "Qwen2SdpaAttention",
        "sub_layers": [
          "q_proj - Linear(1536 → 1536)",
          "k_proj - Linear(1536 → 256)",
          "v_proj - Linear(1536 → 256)",
          "o_proj - Linear(1536 → 1536)",
          "rotary_emb - Qwen2RotaryEmbedding"
        ]
      },
      "kwargs": {
        "cache_position": "Tensor[int] - Shape: (1024)",
        "inputs_embeds": null,
        "position_ids": "Tensor[int] - Shape: (1, 1024)",
        "past_key_value": null,
        "output_attentions": false,
        "use_cache": false,
        "position_embeddings": [
          "Tensor[bfloat16] - Shape: (1, 1024, 1536)",
          "Tensor[bfloat16] - Shape: (1, 1024, 1536)"
        ]
      }
    },
    {
      "prev_op": "Qwen2RMSNorm - Input Size: 1536, Epsilon: 1e-06",
      "layers": [
        "Linear - 1536 → 8960",
        "Linear - 1536 → 8960"
      ],
      "input_activation": "Tensor[bfloat16] - Shape: (batch_size, seq_len, 1536)",
      "module2inspect": {
        "type": "Qwen2MLP",
        "sub_layers": [
          "gate_proj - Linear(1536 → 8960)",
          "up_proj - Linear(1536 → 8960)",
          "down_proj - Linear(8960 → 1536)",
          "act_fn - SiLU"
        ]
      }
    },
    {
      "prev_op": "Linear - 1536 → 8960",
      "layers": [
        "Linear - 8960 → 1536"
      ],
      "input_activation": "Tensor[bfloat16] - Shape: (batch_size, seq_len, 8960)",
      "module2inspect": {
        "type": "Qwen2MLP - Down Projection"
      }
    }
  ]
}
