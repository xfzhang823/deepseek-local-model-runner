{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2065fa2e",
   "metadata": {},
   "source": [
    "# Utils Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c514ddf",
   "metadata": {},
   "source": [
    "## Show Scrollable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8ceec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import HTML\n",
    "from IPython.display import display\n",
    "from pprint import pformat\n",
    "\n",
    "\n",
    "def show_scrollable(content, height=\"300px\"):\n",
    "    \"\"\"\n",
    "    Show scrollable box in Jupyter with preserved indentation and dark theme.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(content, str):\n",
    "        content = pformat(content)\n",
    "\n",
    "    html = f\"\"\"\n",
    "    <div style=\"\n",
    "        height: {height};\n",
    "        overflow: auto;\n",
    "        background-color: black;\n",
    "        color: white;\n",
    "        border: 1px solid #444;\n",
    "        padding: 10px;\n",
    "        font-family: monospace;\n",
    "        white-space: pre;  /* <‚Äî THIS PRESERVES INDENTATION */\n",
    "    \">\n",
    "        {content}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c4a93",
   "metadata": {},
   "source": [
    "## Inspect Safe Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07a5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "\n",
    "\n",
    "def head_tensor(t: torch.Tensor, max_preview: int = 10) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Slice the first `max_preview` elements along every dimension.\n",
    "    \"\"\"\n",
    "    slices = tuple(slice(0, min(dim, max_preview)) for dim in t.shape)\n",
    "    return t[slices]\n",
    "\n",
    "\n",
    "def natural_key(text):\n",
    "    # Breaks text into chunks of digits and non-digits: \"layers.10\" ‚Üí [\"layers.\", 10]\n",
    "    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", text)]\n",
    "\n",
    "\n",
    "def inspect_safetensors(path: str | Path, max_preview: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Return a readable, naturally sorted summary of a .safetensors file as a string.\n",
    "\n",
    "    Args:\n",
    "        path (str | Path): Path to the .safetensors file.\n",
    "        max_preview (int): Max number of values to preview for small tensors.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted summary string.\n",
    "    \"\"\"\n",
    "    path = Path(path).expanduser()\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    state_dict = load_file(path)\n",
    "    sorted_items = sorted(state_dict.items(), key=lambda x: natural_key(x[0]))\n",
    "\n",
    "    lines = [\n",
    "        f\"üîç Inspecting {path.name} ‚Äî {len(sorted_items)} tensors found:\",\n",
    "        \"\",  # adds a single blank line after title\n",
    "    ]\n",
    "\n",
    "    for name, tensor in sorted_items:\n",
    "        lines.append(f\"- {name}: shape={tuple(tensor.shape)}, dtype={tensor.dtype}\")\n",
    "        if tensor.ndim <= 2 and tensor.numel() < 100:\n",
    "            preview = tensor.flatten()[:max_preview].tolist()\n",
    "            lines.append(f\"   preview: {preview}\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def inspect_tensor(path: str | Path, tensor_name: str, max_preview: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Inspect a specific tensor by name in a .safetensors file.\n",
    "\n",
    "    Args:\n",
    "        path (str | Path): Path to the .safetensors file.\n",
    "        tensor_name (str): Exact name of the tensor to inspect.\n",
    "        max_preview (int): Number of values to preview.\n",
    "\n",
    "    Returns:\n",
    "        str: Information about the tensor or a message if not found.\n",
    "    \"\"\"\n",
    "    path = Path(path).expanduser()\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    state_dict = load_file(path)\n",
    "\n",
    "    if tensor_name not in state_dict:\n",
    "        return f\"‚ùå Tensor '{tensor_name}' not found in {path.name}\"\n",
    "\n",
    "    tensor = state_dict[tensor_name]\n",
    "    info = [\n",
    "        f\"‚úÖ Tensor '{tensor_name}' found in {path.name}:\",\n",
    "        f\"   shape: {tuple(tensor.shape)}\",\n",
    "        f\"   dtype: {tensor.dtype}\",\n",
    "    ]\n",
    "\n",
    "    if tensor.ndim <= 2 and tensor.numel() > 0:\n",
    "        preview = tensor.flatten()[:max_preview].tolist()\n",
    "        info.append(f\"   preview: {preview}\")\n",
    "\n",
    "    return \"\\n\".join(info)\n",
    "\n",
    "\n",
    "def inspect_tensor_with_preview(\n",
    "    path: str | Path, tensor_name: str, max_preview: int = 10\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Inspect a specific tensor by name in a .safetensors file.\n",
    "\n",
    "    Args:\n",
    "        path (str | Path): Path to the .safetensors file.\n",
    "        tensor_name (str): Exact name of the tensor to inspect.\n",
    "        max_preview (int): Number of values to preview per dimension.\n",
    "\n",
    "    Returns:\n",
    "        str: Detailed information about the tensor or a message if not found.\n",
    "    \"\"\"\n",
    "    path = Path(path).expanduser()\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    state_dict = load_file(path)\n",
    "\n",
    "    if tensor_name not in state_dict:\n",
    "        return f\"‚ùå Tensor '{tensor_name}' not found in {path.name}\"\n",
    "\n",
    "    tensor = state_dict[tensor_name]\n",
    "    info = [\n",
    "        f\"‚úÖ Tensor '{tensor_name}' found in {path.name}:\",\n",
    "        f\"   shape: {tuple(tensor.shape)}\",\n",
    "        f\"   dtype: {tensor.dtype}\",\n",
    "    ]\n",
    "\n",
    "    if tensor.numel() > 0 and (\n",
    "        torch.is_floating_point(tensor)\n",
    "        or tensor.dtype\n",
    "        in (torch.int8, torch.int16, torch.int32, torch.int64, torch.uint8)\n",
    "    ):\n",
    "        info.extend(\n",
    "            [\n",
    "                f\"   min: {tensor.min().item():.4e}\",\n",
    "                f\"   max: {tensor.max().item():.4e}\",\n",
    "                f\"   mean: {tensor.float().mean().item():.4e}\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        preview = head_tensor(tensor, max_preview).tolist()\n",
    "        info.append(f\"   preview (first {max_preview} per dimension):\")\n",
    "        info.append(f\"{preview}\")\n",
    "\n",
    "    return \"\\n\".join(info)\n",
    "\n",
    "\n",
    "def inspect_tensor_with_preview_pretty(\n",
    "    path: str | Path, tensor_name: str, max_preview: int = 10\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Inspect a specific tensor by name in a .safetensors file.\n",
    "\n",
    "    Args:\n",
    "        path (str | Path): Path to the .safetensors file.\n",
    "        tensor_name (str): Exact name of the tensor to inspect.\n",
    "        max_preview (int): Number of values to preview per dimension.\n",
    "\n",
    "    Returns:\n",
    "        str: Detailed information about the tensor or a message if not found.\n",
    "    \"\"\"\n",
    "    path = Path(path).expanduser()\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    state_dict = load_file(path)\n",
    "\n",
    "    if tensor_name not in state_dict:\n",
    "        return f\"‚ùå Tensor '{tensor_name}' not found in {path.name}\"\n",
    "\n",
    "    tensor = state_dict[tensor_name]\n",
    "    info = [\n",
    "        f\"‚úÖ Tensor '{tensor_name}' found in {path.name}:\",\n",
    "        f\"   shape: {tuple(tensor.shape)}\",\n",
    "        f\"   dtype: {tensor.dtype}\",\n",
    "    ]\n",
    "\n",
    "    if tensor.numel() > 0 and (\n",
    "        torch.is_floating_point(tensor)\n",
    "        or tensor.dtype\n",
    "        in (torch.int8, torch.int16, torch.int32, torch.int64, torch.uint8)\n",
    "    ):\n",
    "        info.extend(\n",
    "            [\n",
    "                f\"   min: {tensor.min().item():.4e}\",\n",
    "                f\"   max: {tensor.max().item():.4e}\",\n",
    "                f\"   mean: {tensor.float().mean().item():.4e}\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Preview with newline for each top-level slice\n",
    "        sliced = head_tensor(tensor, max_preview)\n",
    "        if sliced.ndim >= 2:\n",
    "            preview_rows = sliced[:max_preview]\n",
    "            preview_str = \"\\n\".join(str(row.tolist()) for row in preview_rows)\n",
    "        else:\n",
    "            preview_str = sliced[:max_preview].tolist().__str__()\n",
    "\n",
    "        info.append(f\"   preview (first {max_preview} rows/slices):\\n{preview_str}\")\n",
    "\n",
    "    return \"\\n\".join(info)\n",
    "\n",
    "\n",
    "def inspect_tensor_anomalies(path: str | Path, check_nan_inf: bool = True) -> str:\n",
    "    tensors = load_file(str(path))\n",
    "    lines = [f\"üîé Inspecting: {path} ({len(tensors)} tensors)\\n\"]\n",
    "\n",
    "    for name, tensor in tensors.items():\n",
    "        shape = tuple(tensor.shape)\n",
    "        dtype = str(tensor.dtype)\n",
    "        msg = f\"- {name}: shape={shape}, dtype={dtype}\"\n",
    "\n",
    "        try:\n",
    "            t = (\n",
    "                tensor.float()\n",
    "                if tensor.dtype in [torch.bfloat16, torch.float16]\n",
    "                else tensor\n",
    "            )\n",
    "\n",
    "            stats = []\n",
    "            if check_nan_inf:\n",
    "                if torch.isnan(t).any():\n",
    "                    stats.append(\"‚ùó NaN\")\n",
    "                if torch.isinf(t).any():\n",
    "                    stats.append(\"‚ùó Inf\")\n",
    "\n",
    "            if t.numel() > 0:\n",
    "                if t.dtype.is_floating_point:\n",
    "                    stats += [\n",
    "                        f\"min={t.min().item():.2e}\",\n",
    "                        f\"max={t.max().item():.2e}\",\n",
    "                        f\"mean={t.mean().item():.2e}\",\n",
    "                    ]\n",
    "                elif t.dtype == torch.int32:\n",
    "                    t_f = t.float()\n",
    "                    stats += [\n",
    "                        f\"min={t.min().item():.2e}\",\n",
    "                        f\"max={t.max().item():.2e}\",\n",
    "                        f\"mean={t_f.mean().item():.2e}\",\n",
    "                    ]\n",
    "\n",
    "            if \"qzeros\" in name and (t < 0).any():\n",
    "                stats.append(\"‚ö†Ô∏è Negative qzero\")\n",
    "\n",
    "            if stats:\n",
    "                msg += \" | \" + \", \".join(stats)\n",
    "\n",
    "        except Exception as e:\n",
    "            msg += f\" ‚ö†Ô∏è Error: {e}\"\n",
    "\n",
    "        lines.append(msg)\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf712f8",
   "metadata": {},
   "source": [
    "# Inspecting Full Qwen2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a14d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-72B\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"meta\",  # Loads architecture only, no weights\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "# Print layer names\n",
    "for name, _ in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cc70e6",
   "metadata": {},
   "source": [
    "# Inspecting DS R1 Qwen Model - FULL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f31124c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Config {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(config)  # Full architecture details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dae52d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "Qwen2ForCausalLM                                   [1, 2, 128, 128]          --\n",
       "‚îú‚îÄQwen2Model: 1-1                                  [1, 2, 128, 128]          --\n",
       "‚îÇ    ‚îî‚îÄEmbedding: 2-1                              [1, 128, 1536]            233,373,696\n",
       "‚îÇ    ‚îî‚îÄQwen2RotaryEmbedding: 2-2                   [1, 128, 128]             --\n",
       "‚îÇ    ‚îî‚îÄModuleList: 2-3                             --                        --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-1                 [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-2                 [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-3                 [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-4                 [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-5                 [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-6                 [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-7                 [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-8                 [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-9                 [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-10                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-11                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-12                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-13                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-14                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-15                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-16                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-17                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-18                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-19                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-20                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-21                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-22                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-23                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-24                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-25                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-26                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-27                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄQwen2DecoderLayer: 3-28                [1, 128, 1536]            46,797,824\n",
       "‚îÇ    ‚îî‚îÄQwen2RMSNorm: 2-4                           [1, 128, 1536]            1,536\n",
       "‚îú‚îÄLinear: 1-2                                      [1, 128, 151936]          233,373,696\n",
       "====================================================================================================\n",
       "Total params: 1,777,088,000\n",
       "Trainable params: 1,777,088,000\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.78\n",
       "====================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 907.41\n",
       "Params size (MB): 7108.35\n",
       "Estimated Total Size (MB): 8015.76\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "# Load models\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "summary(model, input_size=(1, 128), dtypes=[torch.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c809a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight: shape=(151936, 1536), dtype=torch.float32\n",
      "layers.0.self_attn.q_proj.weight: shape=(1536, 1536), dtype=torch.float32\n",
      "layers.0.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float32\n",
      "layers.0.self_attn.k_proj.weight: shape=(256, 1536), dtype=torch.float32\n",
      "layers.0.self_attn.k_proj.bias: shape=(256,), dtype=torch.float32\n",
      "layers.0.self_attn.v_proj.weight: shape=(256, 1536), dtype=torch.float32\n",
      "layers.0.self_attn.v_proj.bias: shape=(256,), dtype=torch.float32\n",
      "layers.0.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.float32\n",
      "layers.0.mlp.gate_proj.weight: shape=(8960, 1536), dtype=torch.float32\n",
      "layers.0.mlp.up_proj.weight: shape=(8960, 1536), dtype=torch.float32\n",
      "layers.0.mlp.down_proj.weight: shape=(1536, 8960), dtype=torch.float32\n",
      "layers.0.input_layernorm.weight: shape=(1536,), dtype=torch.float32\n",
      "layers.0.post_attention_layernorm.weight: shape=(1536,), dtype=torch.float32\n",
      "layers.1.self_attn.q_proj.weight: shape=(1536, 1536), dtype=torch.float32\n",
      "layers.1.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float32\n",
      "layers.1.self_attn.k_proj.weight: shape=(256, 1536), dtype=torch.float32\n",
      "layers.1.self_attn.k_proj.bias: shape=(256,), dtype=torch.float32\n",
      "layers.1.self_attn.v_proj.weight: shape=(256, 1536), dtype=torch.float32\n",
      "layers.1.self_attn.v_proj.bias: shape=(256,), dtype=torch.float32\n",
      "layers.1.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "def print_model_weights(model, max_lines=None):\n",
    "    \"\"\"\n",
    "    Print the model's weight tensors in the format:\n",
    "    - layer_name: shape=(...), dtype=...\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to inspect.\n",
    "        max_lines (int, optional): Limit the number of lines printed.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: shape={tuple(param.shape)}, dtype={param.dtype}\")\n",
    "        count += 1\n",
    "        if max_lines and count >= max_lines:\n",
    "            break\n",
    "\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "print_model_weights(model, max_lines=20)  # print only first 10 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "612f0b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 1536)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      ")\n",
      "model Qwen2Model(\n",
      "  (embed_tokens): Embedding(151936, 1536)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x Qwen2DecoderLayer(\n",
      "      (self_attn): Qwen2SdpaAttention(\n",
      "        (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "        (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "        (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "        (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "        (rotary_emb): Qwen2RotaryEmbedding()\n",
      "      )\n",
      "      (mlp): Qwen2MLP(\n",
      "        (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "        (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "        (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.embed_tokens Embedding(151936, 1536)\n",
      "model.layers ModuleList(\n",
      "  (0-27): 28 x Qwen2DecoderLayer(\n",
      "    (self_attn): Qwen2SdpaAttention(\n",
      "      (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "      (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "      (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "      (rotary_emb): Qwen2RotaryEmbedding()\n",
      "    )\n",
      "    (mlp): Qwen2MLP(\n",
      "      (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "      (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "      (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "    (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  )\n",
      ")\n",
      "model.layers.0 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.0.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.0.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.0.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.0.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.0.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.0.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.0.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.0.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.0.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.0.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.0.mlp.act_fn SiLU()\n",
      "model.layers.0.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.0.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.1 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.1.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.1.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.1.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.1.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.1.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.1.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.1.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.1.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.1.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.1.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.1.mlp.act_fn SiLU()\n",
      "model.layers.1.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.1.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.2 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.2.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.2.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.2.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.2.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.2.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.2.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.2.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.2.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.2.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.2.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.2.mlp.act_fn SiLU()\n",
      "model.layers.2.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.2.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.3 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.3.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.3.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.3.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.3.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.3.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.3.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.3.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.3.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.3.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.3.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.3.mlp.act_fn SiLU()\n",
      "model.layers.3.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.3.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.4 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.4.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.4.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.4.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.4.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.4.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.4.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.4.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.4.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.4.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.4.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.4.mlp.act_fn SiLU()\n",
      "model.layers.4.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.4.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.5 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.5.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.5.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.5.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.5.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.5.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.5.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.5.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.5.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.5.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.5.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.5.mlp.act_fn SiLU()\n",
      "model.layers.5.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.5.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.6 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.6.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.6.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.6.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.6.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.6.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.6.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.6.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.6.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.6.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.6.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.6.mlp.act_fn SiLU()\n",
      "model.layers.6.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.6.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.7 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.7.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.7.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.7.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.7.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.7.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.7.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.7.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.7.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.7.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.7.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.7.mlp.act_fn SiLU()\n",
      "model.layers.7.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.7.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.8 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.8.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.8.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.8.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.8.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.8.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.8.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.8.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.8.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.8.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.8.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.8.mlp.act_fn SiLU()\n",
      "model.layers.8.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.8.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.9 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.9.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.9.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.9.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.9.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.9.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.9.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.9.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.9.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.9.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.9.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.9.mlp.act_fn SiLU()\n",
      "model.layers.9.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.9.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.10 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.10.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.10.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.10.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.10.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.10.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.10.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.10.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.10.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.10.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.10.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.10.mlp.act_fn SiLU()\n",
      "model.layers.10.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.10.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.11 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.11.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.11.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.11.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.11.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.11.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.11.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.11.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.11.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.11.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.11.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.11.mlp.act_fn SiLU()\n",
      "model.layers.11.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.11.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.12 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.12.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.12.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.12.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.12.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.12.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.12.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.12.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.12.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.12.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.12.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.12.mlp.act_fn SiLU()\n",
      "model.layers.12.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.12.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.13 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.13.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.13.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.13.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.13.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.13.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.13.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.13.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.13.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.13.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.13.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.13.mlp.act_fn SiLU()\n",
      "model.layers.13.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.13.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.14 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.14.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.14.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.14.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.14.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.14.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.14.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.14.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.14.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.14.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.14.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.14.mlp.act_fn SiLU()\n",
      "model.layers.14.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.14.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.15 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.15.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.15.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.15.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.15.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.15.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.15.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.15.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.15.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.15.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.15.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.15.mlp.act_fn SiLU()\n",
      "model.layers.15.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.15.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.16 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.16.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.16.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.16.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.16.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.16.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.16.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.16.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.16.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.16.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.16.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.16.mlp.act_fn SiLU()\n",
      "model.layers.16.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.16.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.17 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.17.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.17.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.17.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.17.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.17.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.17.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.17.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.17.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.17.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.17.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.17.mlp.act_fn SiLU()\n",
      "model.layers.17.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.17.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.18 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.18.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.18.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.18.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.18.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.18.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.18.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.18.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.18.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.18.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.18.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.18.mlp.act_fn SiLU()\n",
      "model.layers.18.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.18.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.19 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.19.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.19.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.19.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.19.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.19.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.19.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.19.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.19.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.19.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.19.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.19.mlp.act_fn SiLU()\n",
      "model.layers.19.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.19.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.20 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.20.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.20.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.20.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.20.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.20.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.20.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.20.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.20.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.20.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.20.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.20.mlp.act_fn SiLU()\n",
      "model.layers.20.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.20.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.21 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.21.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.21.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.21.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.21.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.21.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.21.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.21.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.21.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.21.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.21.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.21.mlp.act_fn SiLU()\n",
      "model.layers.21.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.21.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.22 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.22.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.22.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.22.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.22.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.22.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.22.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.22.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.22.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.22.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.22.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.22.mlp.act_fn SiLU()\n",
      "model.layers.22.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.22.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.23 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.23.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.23.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.23.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.23.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.23.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.23.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.23.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.23.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.23.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.23.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.23.mlp.act_fn SiLU()\n",
      "model.layers.23.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.23.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.24 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.24.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.24.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.24.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.24.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.24.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.24.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.24.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.24.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.24.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.24.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.24.mlp.act_fn SiLU()\n",
      "model.layers.24.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.24.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.25 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.25.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.25.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.25.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.25.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.25.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.25.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.25.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.25.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.25.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.25.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.25.mlp.act_fn SiLU()\n",
      "model.layers.25.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.25.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.26 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.26.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.26.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.26.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.26.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.26.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.26.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.26.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.26.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.26.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.26.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.26.mlp.act_fn SiLU()\n",
      "model.layers.26.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.26.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.27 Qwen2DecoderLayer(\n",
      "  (self_attn): Qwen2SdpaAttention(\n",
      "    (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "    (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (mlp): Qwen2MLP(\n",
      "    (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "    (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      ")\n",
      "model.layers.27.self_attn Qwen2SdpaAttention(\n",
      "  (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "  (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  (rotary_emb): Qwen2RotaryEmbedding()\n",
      ")\n",
      "model.layers.27.self_attn.q_proj Linear(in_features=1536, out_features=1536, bias=True)\n",
      "model.layers.27.self_attn.k_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.27.self_attn.v_proj Linear(in_features=1536, out_features=256, bias=True)\n",
      "model.layers.27.self_attn.o_proj Linear(in_features=1536, out_features=1536, bias=False)\n",
      "model.layers.27.self_attn.rotary_emb Qwen2RotaryEmbedding()\n",
      "model.layers.27.mlp Qwen2MLP(\n",
      "  (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.27.mlp.gate_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.27.mlp.up_proj Linear(in_features=1536, out_features=8960, bias=False)\n",
      "model.layers.27.mlp.down_proj Linear(in_features=8960, out_features=1536, bias=False)\n",
      "model.layers.27.mlp.act_fn SiLU()\n",
      "model.layers.27.input_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.layers.27.post_attention_layernorm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.norm Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "model.rotary_emb Qwen2RotaryEmbedding()\n",
      "lm_head Linear(in_features=1536, out_features=151936, bias=False)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    # if \"layers\" in name and \"proj\" in name:\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d538ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen1.5-1.8B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "original_config = AutoConfig.from_pretrained(\"Qwen/Qwen1.5-1.8B\")\n",
    "print(original_config)  # Compare hidden_size, layers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9d445",
   "metadata": {},
   "source": [
    "## Inspect Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10a44c",
   "metadata": {},
   "source": [
    "### Peak Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f18e3646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Layer: model.layers.0.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0300,  0.0226,  0.0251],\n",
      "        [-0.0177, -0.0050,  0.0713],\n",
      "        [-0.0033, -0.0170,  0.0043]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.0.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0645,  0.0148, -0.1377],\n",
      "        [ 0.0254, -0.0625,  0.0957],\n",
      "        [ 0.0068, -0.0386, -0.0035]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.0.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0123, -0.0309,  0.0085],\n",
      "        [ 0.0025, -0.0030,  0.0259],\n",
      "        [ 0.0214,  0.0449, -0.0035]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.0.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0015,  0.0013, -0.0114],\n",
      "        [-0.0214,  0.0020,  0.0044],\n",
      "        [ 0.0430,  0.0229, -0.0354]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.0.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0266,  0.0369, -0.0342],\n",
      "        [-0.0210, -0.0135, -0.0405],\n",
      "        [ 0.0566, -0.0588,  0.0315]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.0.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0603, -0.0179,  0.0378],\n",
      "        [ 0.0104,  0.0085,  0.0270],\n",
      "        [-0.0035,  0.0195,  0.0361]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.0.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0064, -0.0107,  0.0212],\n",
      "        [ 0.0330, -0.0011,  0.0017],\n",
      "        [-0.0187,  0.0300,  0.0574]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.0.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.1.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0005,  0.0057, -0.0159],\n",
      "        [-0.0295,  0.0113, -0.0129],\n",
      "        [ 0.0532, -0.0128,  0.0243]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.1.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0084, -0.0461,  0.0123],\n",
      "        [ 0.0371,  0.0181,  0.0071],\n",
      "        [-0.0149,  0.0830,  0.0215]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.1.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0232, -0.0292, -0.0019],\n",
      "        [-0.0031,  0.0065, -0.0422],\n",
      "        [-0.0444, -0.0132, -0.0148]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.1.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0146,  0.0160,  0.0330],\n",
      "        [-0.0148, -0.0791,  0.0255],\n",
      "        [ 0.0060,  0.0364,  0.0513]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.1.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0162,  0.0233, -0.0415],\n",
      "        [ 0.0015,  0.0139, -0.0022],\n",
      "        [ 0.0991,  0.0510, -0.0162]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.1.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0283, -0.0063,  0.0708],\n",
      "        [ 0.0063,  0.0037,  0.0126],\n",
      "        [-0.0437, -0.0128, -0.0049]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.1.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0063,  0.0026, -0.0933],\n",
      "        [-0.0332, -0.0021, -0.0320],\n",
      "        [ 0.0269,  0.0066, -0.0050]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.1.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.2.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0072,  0.0032,  0.0640],\n",
      "        [-0.0139, -0.0393, -0.0135],\n",
      "        [-0.0107, -0.0649, -0.0908]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.2.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 1.7944e-02,  5.6458e-03,  3.2471e-02],\n",
      "        [-1.4954e-03,  3.5889e-02,  4.7302e-03],\n",
      "        [ 1.7212e-02,  3.5400e-02, -6.7711e-05]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.2.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0143, -0.0591, -0.0874],\n",
      "        [-0.0023,  0.0114,  0.0099],\n",
      "        [-0.0055, -0.0214, -0.0037]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.2.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0022, -0.0664, -0.0364],\n",
      "        [ 0.0201, -0.0128,  0.0381],\n",
      "        [-0.0036,  0.0569, -0.0403]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.2.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0278,  0.0457, -0.0388],\n",
      "        [ 0.0430, -0.0204,  0.0117],\n",
      "        [ 0.0540, -0.0310, -0.0576]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.2.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0554,  0.0233,  0.0515],\n",
      "        [ 0.0025, -0.0588, -0.0334],\n",
      "        [-0.0991,  0.0393, -0.0264]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.2.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0211,  0.0188,  0.0552],\n",
      "        [ 0.0250, -0.0569, -0.0177],\n",
      "        [ 0.0127,  0.0097,  0.0430]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.2.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.3.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0190, -0.0003,  0.0381],\n",
      "        [ 0.0022,  0.0099,  0.0649],\n",
      "        [-0.0041,  0.0549,  0.0654]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.3.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0247, -0.0074, -0.0118],\n",
      "        [ 0.0037,  0.0227,  0.0172],\n",
      "        [-0.0236, -0.0096, -0.0322]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.3.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0413,  0.0388, -0.0229],\n",
      "        [ 0.0742, -0.0649, -0.0265],\n",
      "        [ 0.0649,  0.0645,  0.0138]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.3.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0106, -0.0126,  0.0265],\n",
      "        [ 0.0236, -0.0261, -0.0649],\n",
      "        [ 0.0026,  0.0356, -0.0459]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.3.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0508,  0.0459, -0.0532],\n",
      "        [-0.0018, -0.0126,  0.1011],\n",
      "        [ 0.0262,  0.0184,  0.0522]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.3.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0437, -0.0442,  0.0100],\n",
      "        [ 0.0466,  0.0469,  0.0493],\n",
      "        [ 0.0840, -0.0459, -0.0388]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.3.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-3.7842e-02,  5.2979e-02, -5.4932e-02],\n",
      "        [-1.8188e-02, -3.2959e-02, -2.4292e-02],\n",
      "        [-5.3467e-02, -6.4453e-02,  2.7061e-05]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.3.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.4.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0369, -0.0036,  0.0339],\n",
      "        [-0.0212,  0.0226, -0.0137],\n",
      "        [ 0.0135, -0.0175, -0.0454]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.4.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0205, -0.0278,  0.0129],\n",
      "        [ 0.0339,  0.0194,  0.0160],\n",
      "        [ 0.0041,  0.0107,  0.0298]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.4.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0171, -0.0364,  0.0073],\n",
      "        [ 0.0027,  0.1328, -0.0376],\n",
      "        [ 0.0144,  0.0544, -0.0168]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.4.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0320, -0.0134,  0.0160],\n",
      "        [-0.0613,  0.0522, -0.0110],\n",
      "        [-0.0522,  0.1245, -0.0410]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.4.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0879, -0.0022, -0.0498],\n",
      "        [ 0.0056,  0.0518, -0.0234],\n",
      "        [-0.0579, -0.0325, -0.0354]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.4.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0234,  0.0615,  0.0189],\n",
      "        [ 0.0116, -0.0275,  0.0236],\n",
      "        [-0.0071,  0.0062, -0.0104]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.4.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0192,  0.0020,  0.0322],\n",
      "        [-0.0079,  0.0302,  0.0410],\n",
      "        [ 0.0009,  0.0231, -0.0054]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.4.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.5.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-7.1049e-05,  5.8289e-03,  2.5513e-02],\n",
      "        [-6.1340e-03, -3.6865e-02, -1.1658e-02],\n",
      "        [-3.1738e-03,  1.5259e-03,  3.8818e-02]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.5.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0040, -0.0233,  0.0422],\n",
      "        [ 0.0018,  0.0019,  0.0042],\n",
      "        [-0.0032, -0.0476,  0.0178]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.5.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0115,  0.0864, -0.0212],\n",
      "        [ 0.0084,  0.0164,  0.0630],\n",
      "        [-0.0151, -0.0542, -0.0432]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.5.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0334,  0.0038,  0.0211],\n",
      "        [-0.0160,  0.0364, -0.0371],\n",
      "        [-0.0027,  0.0981,  0.0840]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.5.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0327, -0.0123, -0.0688],\n",
      "        [-0.0403, -0.0476, -0.0654],\n",
      "        [ 0.0430,  0.0198, -0.0400]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.5.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0258, -0.0094, -0.0562],\n",
      "        [ 0.0518,  0.0347, -0.0245],\n",
      "        [ 0.0051, -0.0124,  0.0288]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.5.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0074, -0.0123,  0.0767],\n",
      "        [-0.0256,  0.0095, -0.0305],\n",
      "        [-0.0320,  0.0791,  0.0383]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.5.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.6.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0047, -0.0166,  0.0226],\n",
      "        [-0.0051,  0.0674,  0.0366],\n",
      "        [ 0.0149,  0.0068, -0.0208]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.6.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0109,  0.0035, -0.0364],\n",
      "        [ 0.0124, -0.0226,  0.0442],\n",
      "        [ 0.0110, -0.0110,  0.0081]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.6.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0120,  0.1226,  0.0571],\n",
      "        [ 0.0077, -0.0664,  0.0168],\n",
      "        [-0.0095,  0.0026,  0.0260]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.6.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0031, -0.0262,  0.0452],\n",
      "        [-0.0020,  0.0278, -0.0859],\n",
      "        [-0.0330, -0.0383,  0.0913]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.6.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0942,  0.0130,  0.0845],\n",
      "        [-0.0101, -0.0066,  0.0057],\n",
      "        [-0.0332, -0.0245, -0.0400]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.6.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0728, -0.0515,  0.0510],\n",
      "        [-0.0148, -0.0055,  0.0283],\n",
      "        [-0.0052, -0.0192,  0.0091]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.6.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0223,  0.0011, -0.0106],\n",
      "        [-0.0488, -0.0291,  0.0018],\n",
      "        [ 0.0608, -0.0349, -0.0376]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.6.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.7.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0054, -0.0256, -0.0029],\n",
      "        [ 0.0138, -0.0547,  0.0171],\n",
      "        [ 0.0074,  0.0154, -0.0109]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.7.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0256,  0.0243, -0.0089],\n",
      "        [ 0.0505,  0.0038, -0.0037],\n",
      "        [-0.0068, -0.0057, -0.0278]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.7.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0479,  0.0850,  0.0139],\n",
      "        [ 0.0732, -0.0034,  0.0122],\n",
      "        [ 0.0034, -0.0366,  0.0352]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.7.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0801, -0.0461, -0.0430],\n",
      "        [-0.1104,  0.0413,  0.0669],\n",
      "        [-0.0330,  0.0281, -0.0630]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.7.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0302, -0.0293,  0.0240],\n",
      "        [ 0.0123,  0.0334, -0.0559],\n",
      "        [-0.0280, -0.0659, -0.0142]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.7.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0099, -0.0023,  0.0067],\n",
      "        [ 0.0464, -0.0400,  0.0034],\n",
      "        [ 0.0007,  0.0099,  0.0020]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.7.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0148, -0.0344, -0.0103],\n",
      "        [ 0.0107, -0.0728,  0.0277],\n",
      "        [ 0.0058,  0.0029,  0.0254]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.7.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.8.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0640,  0.0820,  0.0747],\n",
      "        [-0.0408, -0.0047, -0.1084],\n",
      "        [ 0.0596, -0.0737, -0.0254]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.8.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0161, -0.0239,  0.0040],\n",
      "        [-0.0129, -0.0211, -0.0098],\n",
      "        [-0.0110,  0.0030,  0.0035]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.8.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0015, -0.0505,  0.0061],\n",
      "        [-0.0003,  0.0240, -0.0835],\n",
      "        [-0.0002,  0.0225, -0.0120]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.8.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0232, -0.0179,  0.0315],\n",
      "        [ 0.0131,  0.0042,  0.0776],\n",
      "        [-0.1118, -0.0140, -0.0464]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.8.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0564,  0.0227,  0.0625],\n",
      "        [-0.0388,  0.0206, -0.0320],\n",
      "        [-0.0036,  0.0184,  0.0014]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.8.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0371, -0.0835,  0.0605],\n",
      "        [ 0.0146,  0.0359, -0.0015],\n",
      "        [-0.0437, -0.0031,  0.0356]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.8.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0635, -0.0479,  0.0201],\n",
      "        [-0.0284,  0.0630, -0.0510],\n",
      "        [ 0.0130, -0.0825,  0.0381]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.8.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.9.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0039,  0.0864, -0.0078],\n",
      "        [-0.0280,  0.0547, -0.0104],\n",
      "        [-0.0031, -0.0059,  0.0515]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.9.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0248, -0.0210, -0.0045],\n",
      "        [-0.0344, -0.0061,  0.0028],\n",
      "        [ 0.0161, -0.0229,  0.0107]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.9.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-2.2583e-02, -5.3711e-02,  1.3428e-02],\n",
      "        [ 6.4697e-03, -4.6387e-03,  1.7762e-05],\n",
      "        [-6.4087e-03,  5.2002e-02, -1.7090e-02]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.9.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0304, -0.0381,  0.0226],\n",
      "        [ 0.0933,  0.0483, -0.0625],\n",
      "        [-0.0344,  0.0217, -0.0025]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.9.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0057, -0.0112,  0.0228],\n",
      "        [-0.0065,  0.0123, -0.0356],\n",
      "        [ 0.0294,  0.0261,  0.0161]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.9.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0291, -0.0537,  0.0425],\n",
      "        [ 0.0029,  0.0476, -0.0250],\n",
      "        [ 0.0059, -0.0347,  0.0025]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.9.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0221,  0.0181, -0.0449],\n",
      "        [-0.0008,  0.0199, -0.0294],\n",
      "        [-0.0359, -0.0137, -0.1021]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.9.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.10.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0049, -0.0034, -0.0193],\n",
      "        [ 0.0221, -0.0059, -0.0106],\n",
      "        [-0.0063,  0.0330, -0.0184]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.10.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0264, -0.0040,  0.0071],\n",
      "        [ 0.0074, -0.0123, -0.0117],\n",
      "        [-0.0150, -0.0149,  0.0153]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.10.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0087,  0.0840,  0.0271],\n",
      "        [ 0.0469, -0.0025, -0.0164],\n",
      "        [-0.0058, -0.0254, -0.0142]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.10.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0315, -0.0023,  0.0011],\n",
      "        [-0.0422,  0.0038, -0.0033],\n",
      "        [-0.0583, -0.0103, -0.0581]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.10.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0349,  0.0635,  0.0153],\n",
      "        [ 0.0547, -0.0554, -0.0635],\n",
      "        [ 0.0317, -0.0240, -0.0135]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.10.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0491, -0.0219, -0.0332],\n",
      "        [-0.0208,  0.0547, -0.0369],\n",
      "        [-0.0040,  0.0110,  0.0239]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.10.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0004, -0.0322, -0.0112],\n",
      "        [-0.0742,  0.0145,  0.0415],\n",
      "        [ 0.0237, -0.0165,  0.0282]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.10.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.11.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0286, -0.0160,  0.0161],\n",
      "        [ 0.0164,  0.0236, -0.0165],\n",
      "        [-0.0047,  0.0054,  0.0060]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.11.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0210, -0.0204,  0.0081],\n",
      "        [-0.0057, -0.0050, -0.0047],\n",
      "        [ 0.0022, -0.0166,  0.0045]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.11.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0068, -0.0037,  0.0104],\n",
      "        [ 0.0012, -0.0270, -0.0260],\n",
      "        [-0.0586,  0.0366, -0.0156]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.11.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0070, -0.0143, -0.0099],\n",
      "        [ 0.0136,  0.0308,  0.0811],\n",
      "        [ 0.0703, -0.0021,  0.0654]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.11.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0527, -0.0359,  0.0413],\n",
      "        [-0.0625, -0.0096,  0.0098],\n",
      "        [-0.0415, -0.0154,  0.0140]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.11.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0152,  0.0874,  0.0369],\n",
      "        [-0.0420, -0.0117, -0.0334],\n",
      "        [-0.0107, -0.0005,  0.0486]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.11.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0091, -0.0315, -0.0160],\n",
      "        [ 0.0635, -0.0015,  0.0425],\n",
      "        [-0.0398, -0.0447,  0.0079]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.11.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.12.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0112,  0.0123,  0.0003],\n",
      "        [ 0.0068,  0.0466, -0.0034],\n",
      "        [-0.0176,  0.0244, -0.0310]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.12.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0033,  0.0065,  0.0032],\n",
      "        [-0.0029, -0.0092,  0.0253],\n",
      "        [-0.0102,  0.0131,  0.0211]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.12.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0027, -0.0187, -0.0170],\n",
      "        [-0.0006,  0.0114, -0.0361],\n",
      "        [-0.0011, -0.0488,  0.0415]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.12.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0009,  0.0026,  0.0713],\n",
      "        [-0.0654,  0.0156, -0.0063],\n",
      "        [ 0.0090,  0.0019, -0.0493]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.12.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0913,  0.0150,  0.0459],\n",
      "        [ 0.0439, -0.0162,  0.0312],\n",
      "        [-0.0177,  0.0613,  0.0352]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.12.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0211,  0.0085, -0.0210],\n",
      "        [-0.0015,  0.0270,  0.0330],\n",
      "        [ 0.0491,  0.0046, -0.0270]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.12.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0417,  0.0374, -0.0269],\n",
      "        [-0.0153, -0.0048,  0.0532],\n",
      "        [-0.0439, -0.0016, -0.0209]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.12.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.13.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0255,  0.0225, -0.0024],\n",
      "        [ 0.0261,  0.0019,  0.0011],\n",
      "        [ 0.0152,  0.0032,  0.0635]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.13.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0074,  0.0092,  0.0060],\n",
      "        [ 0.0198, -0.0114, -0.0166],\n",
      "        [-0.0012, -0.0019, -0.0096]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.13.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0205, -0.0186,  0.0139],\n",
      "        [-0.0275, -0.0228, -0.0513],\n",
      "        [-0.0052,  0.0275, -0.0095]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.13.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0052,  0.0146, -0.0034],\n",
      "        [ 0.0226, -0.0708, -0.0381],\n",
      "        [ 0.0110,  0.0991,  0.0645]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.13.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0415, -0.1235,  0.0732],\n",
      "        [-0.0574,  0.0786, -0.0126],\n",
      "        [-0.0291, -0.0552,  0.0254]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.13.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0104,  0.0510,  0.0068],\n",
      "        [-0.0515, -0.0430,  0.0216],\n",
      "        [-0.0422, -0.0297, -0.0049]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.13.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0330,  0.0234, -0.0208],\n",
      "        [ 0.0149, -0.0062, -0.0947],\n",
      "        [-0.0013,  0.0064, -0.0093]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.13.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.14.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0083,  0.0228,  0.0102],\n",
      "        [-0.0028,  0.0277,  0.0017],\n",
      "        [ 0.0150,  0.0036, -0.0028]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.14.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0320, -0.0084, -0.0015],\n",
      "        [-0.0001,  0.0058, -0.0036],\n",
      "        [-0.0141,  0.0012, -0.0005]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.14.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0080, -0.0098, -0.0107],\n",
      "        [ 0.0120, -0.0082,  0.0195],\n",
      "        [-0.0559,  0.0315, -0.0312]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.14.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0276, -0.0019,  0.0322],\n",
      "        [ 0.0237,  0.0112,  0.0245],\n",
      "        [ 0.0840, -0.0053, -0.0437]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.14.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0938,  0.0437,  0.0603],\n",
      "        [-0.0430,  0.0405, -0.0260],\n",
      "        [-0.0270, -0.0391,  0.0815]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.14.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0117,  0.0212, -0.0052],\n",
      "        [-0.0598, -0.0310, -0.0254],\n",
      "        [ 0.0265,  0.0566,  0.0459]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.14.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0020,  0.0132,  0.0327],\n",
      "        [-0.0294, -0.0327,  0.0874],\n",
      "        [-0.0374, -0.0337,  0.0245]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.14.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.15.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0405,  0.0215, -0.0347],\n",
      "        [ 0.0142, -0.0154,  0.0107],\n",
      "        [ 0.0188,  0.0708, -0.0052]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.15.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0311,  0.0432,  0.0082],\n",
      "        [-0.0679, -0.0115,  0.0102],\n",
      "        [-0.0182, -0.0060, -0.0175]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.15.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0161, -0.0183, -0.0540],\n",
      "        [-0.0231, -0.0261,  0.0206],\n",
      "        [ 0.0356, -0.0610,  0.0199]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.15.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0113, -0.0019, -0.0209],\n",
      "        [ 0.0204,  0.0356,  0.0435],\n",
      "        [-0.0898,  0.0400,  0.0243]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.15.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0337,  0.0374,  0.0162],\n",
      "        [-0.0364,  0.0518,  0.0310],\n",
      "        [-0.0064,  0.0444,  0.0752]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.15.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0151,  0.0045, -0.0032],\n",
      "        [-0.0359, -0.0386,  0.0400],\n",
      "        [-0.0376, -0.1035, -0.0004]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.15.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0021, -0.0099, -0.0723],\n",
      "        [ 0.0132, -0.0503, -0.0903],\n",
      "        [ 0.0325,  0.0576, -0.0415]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.15.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.16.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0292,  0.0189, -0.0071],\n",
      "        [ 0.0219,  0.0109, -0.0099],\n",
      "        [ 0.0310, -0.0225,  0.0181]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.16.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 9.5215e-03,  2.4719e-03,  9.5215e-03],\n",
      "        [-4.8584e-02, -1.1536e-02,  1.1902e-02],\n",
      "        [-1.3489e-02, -1.4832e-02, -1.1802e-05]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.16.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0014,  0.0208,  0.0122],\n",
      "        [ 0.0234, -0.0114,  0.0069],\n",
      "        [ 0.0889, -0.0211,  0.0253]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.16.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0223, -0.0334, -0.0068],\n",
      "        [ 0.0137, -0.0179, -0.0991],\n",
      "        [-0.0192, -0.0133,  0.0464]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.16.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0869,  0.0581,  0.0669],\n",
      "        [-0.0165,  0.0309,  0.0396],\n",
      "        [-0.0014, -0.0054,  0.0237]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.16.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0361, -0.0549, -0.0420],\n",
      "        [-0.0229,  0.0601, -0.0386],\n",
      "        [ 0.0422, -0.1025, -0.0747]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.16.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-2.8320e-02, -2.2583e-02, -2.8320e-02],\n",
      "        [-8.3496e-02, -3.7354e-02, -1.8555e-02],\n",
      "        [ 6.4850e-05,  2.5513e-02,  3.5889e-02]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.16.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.17.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0630,  0.0415,  0.0193],\n",
      "        [-0.0300,  0.0189,  0.0417],\n",
      "        [ 0.0302,  0.0161, -0.0422]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.17.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0194, -0.0024,  0.0028],\n",
      "        [-0.0057,  0.0172,  0.0190],\n",
      "        [-0.0068, -0.0177, -0.0108]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.17.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0002, -0.0300, -0.0216],\n",
      "        [-0.0036, -0.1191, -0.0283],\n",
      "        [ 0.0092, -0.1108, -0.0432]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.17.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0142,  0.0271, -0.0085],\n",
      "        [ 0.0535, -0.0688, -0.0253],\n",
      "        [-0.0493,  0.0248, -0.1270]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.17.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0344, -0.0069,  0.0175],\n",
      "        [ 0.0055,  0.0432,  0.0532],\n",
      "        [ 0.0233,  0.0085,  0.0151]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.17.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0356,  0.0391,  0.0688],\n",
      "        [-0.0115, -0.0562,  0.0242],\n",
      "        [-0.0277, -0.0280,  0.0405]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.17.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0040,  0.0159, -0.0004],\n",
      "        [ 0.0214,  0.0032,  0.0260],\n",
      "        [ 0.0105, -0.0654, -0.0165]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.17.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.18.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0630, -0.0317, -0.0084],\n",
      "        [-0.0542,  0.0129, -0.0309],\n",
      "        [ 0.0179,  0.0051,  0.0170]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.18.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0233, -0.0005,  0.0089],\n",
      "        [ 0.0281,  0.0054,  0.0074],\n",
      "        [ 0.0059, -0.0145, -0.0007]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.18.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0640,  0.0020,  0.0522],\n",
      "        [ 0.0923, -0.0247, -0.0688],\n",
      "        [-0.0713, -0.0083, -0.0081]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.18.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0229,  0.0014,  0.0150],\n",
      "        [ 0.0420, -0.0723,  0.0366],\n",
      "        [ 0.0996, -0.1436, -0.1533]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.18.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0020,  0.0398, -0.0391],\n",
      "        [-0.0150,  0.0432,  0.0118],\n",
      "        [ 0.0128,  0.0493, -0.0165]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.18.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-3.1006e-02,  9.9609e-02,  1.7700e-02],\n",
      "        [-6.3782e-03,  1.7822e-02,  2.2949e-02],\n",
      "        [ 1.9165e-02, -6.6895e-02,  7.2479e-05]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.18.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0177, -0.0344,  0.0025],\n",
      "        [ 0.0718, -0.0435, -0.0275],\n",
      "        [-0.0286,  0.0104, -0.0227]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.18.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.19.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0540,  0.0171,  0.0203],\n",
      "        [ 0.0400,  0.0089, -0.0135],\n",
      "        [-0.0030,  0.0322, -0.0044]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.19.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0081,  0.0136, -0.0064],\n",
      "        [-0.0003,  0.0019,  0.0002],\n",
      "        [ 0.0243,  0.0012,  0.0113]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.19.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0022,  0.0493, -0.0659],\n",
      "        [-0.0112, -0.0266, -0.0171],\n",
      "        [ 0.2910,  0.0388,  0.0244]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.19.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0248,  0.0447, -0.0410],\n",
      "        [-0.0033,  0.0139, -0.0879],\n",
      "        [-0.1719,  0.0781,  0.0444]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.19.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0466, -0.0092,  0.0483],\n",
      "        [-0.0378, -0.0298,  0.0518],\n",
      "        [ 0.0159,  0.0020,  0.2109]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.19.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0491, -0.0211, -0.0085],\n",
      "        [-0.0525, -0.0369,  0.0698],\n",
      "        [ 0.0097, -0.0154, -0.2734]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.19.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0608, -0.0067, -0.0288],\n",
      "        [ 0.0050, -0.0226,  0.0454],\n",
      "        [-0.0201,  0.0457, -0.0864]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.19.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.20.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0386,  0.0356,  0.0095],\n",
      "        [ 0.0146,  0.0493,  0.0349],\n",
      "        [-0.0588, -0.0479,  0.0220]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.20.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 3.1586e-03,  5.8289e-03,  4.3335e-03],\n",
      "        [ 6.1035e-05,  1.0742e-02, -2.4414e-03],\n",
      "        [-7.6904e-03,  1.8555e-02,  4.3945e-03]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.20.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0820, -0.0229, -0.0306],\n",
      "        [ 0.0203,  0.0845,  0.0098],\n",
      "        [-0.0014, -0.0640,  0.0133]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.20.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-1.8799e-02,  4.2969e-02, -4.9072e-02],\n",
      "        [-9.7168e-02,  7.1716e-03,  1.2695e-01],\n",
      "        [-1.0204e-04, -1.2695e-01, -1.8677e-02]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.20.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0327, -0.0177, -0.0527],\n",
      "        [ 0.0732, -0.0067, -0.0308],\n",
      "        [-0.0031,  0.0776, -0.0530]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.20.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0190,  0.0020,  0.0618],\n",
      "        [-0.0752,  0.0051,  0.0161],\n",
      "        [-0.0194,  0.0320, -0.0742]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.20.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0125,  0.0013,  0.0058],\n",
      "        [-0.0752,  0.0148,  0.0147],\n",
      "        [-0.0255,  0.0104,  0.0498]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.20.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.21.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0469, -0.0393,  0.0120],\n",
      "        [ 0.0311,  0.0435,  0.0037],\n",
      "        [ 0.0137,  0.0079, -0.0003]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.21.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0018, -0.0083, -0.0154],\n",
      "        [-0.0131, -0.0459,  0.0009],\n",
      "        [-0.0117,  0.0277, -0.0107]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.21.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.1157,  0.0281,  0.0135],\n",
      "        [-0.0035, -0.0356, -0.0435],\n",
      "        [ 0.0273,  0.0143, -0.0175]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.21.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0008,  0.0339, -0.0254],\n",
      "        [ 0.0312,  0.1201,  0.0820],\n",
      "        [ 0.0136,  0.0317, -0.0457]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.21.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0022,  0.0232,  0.0393],\n",
      "        [-0.0486, -0.0342,  0.0261],\n",
      "        [-0.0259, -0.0002,  0.0242]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.21.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0244, -0.0181,  0.0938],\n",
      "        [ 0.0322, -0.0067, -0.0295],\n",
      "        [ 0.0105, -0.0544,  0.0067]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.21.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0330, -0.0029, -0.0366],\n",
      "        [-0.0054,  0.0405, -0.0535],\n",
      "        [ 0.0347, -0.0245,  0.0053]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.21.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.22.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0059,  0.0261, -0.0225],\n",
      "        [ 0.0161,  0.0183, -0.0160],\n",
      "        [ 0.0251, -0.0118, -0.0369]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.22.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0038, -0.0025, -0.0020],\n",
      "        [-0.0060,  0.0288,  0.0077],\n",
      "        [-0.0067,  0.0093,  0.0110]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.22.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0297, -0.0420, -0.0072],\n",
      "        [-0.0110, -0.0115,  0.0236],\n",
      "        [-0.0249,  0.1953, -0.0220]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.22.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0459, -0.0051,  0.0102],\n",
      "        [-0.0154, -0.0405,  0.0942],\n",
      "        [ 0.0261,  0.0483, -0.0640]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.22.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0928, -0.0527, -0.0010],\n",
      "        [ 0.0087,  0.0117, -0.0408],\n",
      "        [ 0.0033,  0.0277, -0.0166]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.22.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0684,  0.0120, -0.0205],\n",
      "        [-0.0339, -0.0610, -0.0128],\n",
      "        [ 0.0003,  0.0210, -0.0081]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.22.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0173,  0.0221,  0.0014],\n",
      "        [-0.0383, -0.0165, -0.0415],\n",
      "        [-0.0303,  0.0210,  0.0022]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.22.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.23.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0237,  0.0242, -0.0134],\n",
      "        [ 0.0134, -0.0225, -0.0129],\n",
      "        [-0.0019,  0.0079,  0.0347]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.23.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0065, -0.0024,  0.0150],\n",
      "        [-0.0074, -0.0188,  0.0137],\n",
      "        [ 0.0051, -0.0039, -0.0040]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.23.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.1328,  0.0150, -0.0060],\n",
      "        [ 0.0159,  0.0120, -0.0015],\n",
      "        [-0.1396, -0.0005,  0.0009]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.23.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0265, -0.0320, -0.0118],\n",
      "        [ 0.0752, -0.0381,  0.0601],\n",
      "        [-0.0071, -0.0176,  0.0039]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.23.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0413, -0.0183,  0.0479],\n",
      "        [ 0.0074,  0.0356,  0.0718],\n",
      "        [-0.0396,  0.0291,  0.0040]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.23.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0354,  0.0615,  0.0237],\n",
      "        [ 0.0505, -0.0549, -0.0251],\n",
      "        [-0.0449, -0.0157, -0.0737]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.23.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0417, -0.0330,  0.0101],\n",
      "        [ 0.0525, -0.0430,  0.0107],\n",
      "        [ 0.0474,  0.0091, -0.0275]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.23.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.24.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[0.0024, 0.0109, 0.0066],\n",
      "        [0.0034, 0.0079, 0.0187],\n",
      "        [0.0107, 0.0067, 0.0284]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.24.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0063, -0.0447, -0.0042],\n",
      "        [-0.0079,  0.0242,  0.0010],\n",
      "        [-0.0027,  0.0255,  0.0232]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.24.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0267,  0.0747,  0.0239],\n",
      "        [ 0.0201,  0.0496, -0.0256],\n",
      "        [-0.0001, -0.0016, -0.0059]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.24.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0175, -0.0094,  0.0354],\n",
      "        [ 0.0698, -0.0025,  0.0530],\n",
      "        [-0.0496, -0.0295, -0.0879]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.24.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0019, -0.0231,  0.0664],\n",
      "        [ 0.0347,  0.0649,  0.0347],\n",
      "        [-0.0262, -0.0292,  0.0270]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.24.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0017, -0.0244, -0.0170],\n",
      "        [ 0.0613, -0.0320, -0.0645],\n",
      "        [ 0.0820,  0.0747,  0.0339]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.24.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0291,  0.0227, -0.0146],\n",
      "        [ 0.0693,  0.0166, -0.0098],\n",
      "        [ 0.0069, -0.0028, -0.0041]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.24.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.25.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0042, -0.0001,  0.0151],\n",
      "        [ 0.0069, -0.0466, -0.0339],\n",
      "        [-0.0117,  0.0493, -0.0250]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.25.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0048, -0.0010, -0.0095],\n",
      "        [-0.0029,  0.0292,  0.0062],\n",
      "        [-0.0043,  0.0034, -0.0238]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.25.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.1455, -0.1172,  0.0216],\n",
      "        [-0.0234, -0.0320, -0.0120],\n",
      "        [-0.1089,  0.0002,  0.0645]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.25.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0327,  0.0476,  0.0308],\n",
      "        [-0.0630, -0.0510,  0.0238],\n",
      "        [ 0.0112,  0.0537, -0.0713]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.25.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0083,  0.0192,  0.0091],\n",
      "        [ 0.0752,  0.0051, -0.0031],\n",
      "        [ 0.0659,  0.0593, -0.0261]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.25.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0236, -0.0194, -0.0315],\n",
      "        [-0.0366,  0.0308,  0.0479],\n",
      "        [ 0.0981,  0.0049, -0.0349]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.25.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0522,  0.0457, -0.0588],\n",
      "        [-0.0125, -0.0220, -0.0500],\n",
      "        [-0.0359, -0.0337,  0.0189]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.25.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.26.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0178, -0.0400, -0.0179],\n",
      "        [ 0.0150,  0.0199,  0.0091],\n",
      "        [ 0.0062, -0.0269,  0.0087]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.26.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0116, -0.0172,  0.0052],\n",
      "        [ 0.0025, -0.0034,  0.0072],\n",
      "        [ 0.0060,  0.0160, -0.0177]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.26.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0728,  0.0009, -0.0123],\n",
      "        [ 0.0347, -0.0170, -0.0009],\n",
      "        [ 0.1494, -0.0049,  0.0189]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.26.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0238,  0.0075,  0.0415],\n",
      "        [ 0.1021, -0.0535,  0.0325],\n",
      "        [ 0.0078,  0.0469, -0.0156]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.26.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0089,  0.0112, -0.0435],\n",
      "        [ 0.0791,  0.0089,  0.0486],\n",
      "        [-0.0026,  0.0620,  0.0079]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.26.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0461, -0.0195,  0.0806],\n",
      "        [-0.0544,  0.0115,  0.0122],\n",
      "        [ 0.0271,  0.0160, -0.0332]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.26.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0703, -0.0933,  0.0145],\n",
      "        [-0.0032, -0.0398,  0.0576],\n",
      "        [ 0.0210, -0.0145, -0.0309]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.26.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.27.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0032,  0.0564, -0.0186],\n",
      "        [ 0.0011,  0.0236, -0.0047],\n",
      "        [-0.0090,  0.0094,  0.0018]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.27.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0022, -0.0342,  0.0020],\n",
      "        [-0.0115, -0.0083, -0.0125],\n",
      "        [ 0.0086, -0.0041, -0.0225]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.27.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 8.1055e-02, -7.7637e-02,  1.2665e-03],\n",
      "        [-9.2773e-02, -1.3733e-02, -2.2217e-02],\n",
      "        [ 2.0874e-02, -8.3008e-03,  1.3888e-05]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.27.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0150,  0.0214, -0.0442],\n",
      "        [-0.0398, -0.0359,  0.0044],\n",
      "        [-0.0620,  0.0620,  0.0757]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.27.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.1118,  0.0111, -0.0679],\n",
      "        [-0.0400, -0.0168,  0.0040],\n",
      "        [ 0.0640,  0.0067, -0.0479]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.27.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0260,  0.0830,  0.0854],\n",
      "        [-0.0090, -0.0069, -0.0060],\n",
      "        [ 0.0376, -0.0270,  0.0786]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.27.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0447,  0.0231,  0.0383],\n",
      "        [ 0.0300, -0.0322, -0.0359],\n",
      "        [-0.0244, -0.0444, -0.0605]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.27.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: lm_head ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([151936, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0776,  0.0293, -0.0143],\n",
      "        [ 0.0518, -0.0444,  0.0284],\n",
      "        [ 0.0269, -0.0486, -0.0099]])\n",
      "\n",
      "=== Detailed First Layer Inspection ===\n",
      "\n",
      "Parameter: model.layers.0.self_attn.q_proj.weight\n",
      "Shape: torch.Size([1536, 1536])\n",
      "First few values:\n",
      "tensor([[-0.0300,  0.0226,  0.0251,  0.0065,  0.0058],\n",
      "        [-0.0177, -0.0050,  0.0713,  0.0150,  0.0315]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch.nn as nn\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "def inspect_layer(name, module):\n",
    "    \"\"\"Helper function to inspect a single layer\"\"\"\n",
    "    print(f\"\\n--- Layer: {name} ---\")\n",
    "    print(f\"Type: {type(module)}\")\n",
    "\n",
    "    # Check if it's a standard linear layer\n",
    "    if isinstance(module, nn.Linear):\n",
    "        print(f\"  Weight shape: {module.weight.shape} (out_features, in_features)\")\n",
    "        if module.bias is not None:\n",
    "            print(f\"  Bias shape: {module.bias.shape}\")\n",
    "        print(f\"  First weight values (3x3):\\n{module.weight.data[:3, :3]}\")\n",
    "\n",
    "    # Check for common patterns in transformer layers\n",
    "    if \"q_proj\" in name:\n",
    "        print(\"  [This appears to be a query projection]\")\n",
    "    elif \"k_proj\" in name:\n",
    "        print(\"  [This appears to be a key projection]\")\n",
    "    elif \"v_proj\" in name:\n",
    "        print(\"  [This appears to be a value projection]\")\n",
    "    elif \"o_proj\" in name:\n",
    "        print(\"  [This appears to be an output projection]\")\n",
    "    elif \"gate_proj\" in name or \"up_proj\" in name or \"down_proj\" in name:\n",
    "        print(\"  [This appears to be a feed-forward layer component]\")\n",
    "\n",
    "\n",
    "# Iterate through all layers\n",
    "for name, module in model.named_modules():\n",
    "    # Skip very high-level modules to reduce output\n",
    "    if len(name.split(\".\")) > 6:  # Adjust this number as needed\n",
    "        continue\n",
    "\n",
    "    # Only inspect certain types of layers\n",
    "    if isinstance(module, nn.Linear) or \"proj\" in name or \"attention\" in name:\n",
    "        inspect_layer(name, module)\n",
    "\n",
    "# Additional inspection of the first layer's weights\n",
    "print(\"\\n=== Detailed First Layer Inspection ===\")\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layers.0\" in name and \"weight\" in name:\n",
    "        print(f\"\\nParameter: {name}\")\n",
    "        print(f\"Shape: {param.shape}\")\n",
    "        print(\n",
    "            f\"First few values:\\n{param.data[:2, :5] if len(param.shape) > 1 else param.data[:5]}\"\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af059f",
   "metadata": {},
   "source": [
    "### Weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeddbfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>type</th>\n",
       "      <th>shape</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>zero_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model.layers.0.self_attn.q_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(1536, 1536)</td>\n",
       "      <td>-0.953125</td>\n",
       "      <td>0.960938</td>\n",
       "      <td>1.750172e-05</td>\n",
       "      <td>0.053037</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model.layers.0.self_attn.k_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(256, 1536)</td>\n",
       "      <td>-0.384766</td>\n",
       "      <td>0.394531</td>\n",
       "      <td>-8.440000e-05</td>\n",
       "      <td>0.061757</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model.layers.0.self_attn.v_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(256, 1536)</td>\n",
       "      <td>-0.159180</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>-1.660858e-05</td>\n",
       "      <td>0.030817</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model.layers.0.self_attn.o_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(1536, 1536)</td>\n",
       "      <td>-0.597656</td>\n",
       "      <td>0.605469</td>\n",
       "      <td>3.382745e-06</td>\n",
       "      <td>0.046337</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model.layers.0.mlp.gate_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(8960, 1536)</td>\n",
       "      <td>-0.753906</td>\n",
       "      <td>0.660156</td>\n",
       "      <td>-4.833728e-07</td>\n",
       "      <td>0.038473</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model.layers.0.mlp.up_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(8960, 1536)</td>\n",
       "      <td>-0.457031</td>\n",
       "      <td>0.419922</td>\n",
       "      <td>-5.622278e-06</td>\n",
       "      <td>0.034311</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model.layers.0.mlp.down_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(1536, 8960)</td>\n",
       "      <td>-0.550781</td>\n",
       "      <td>0.519531</td>\n",
       "      <td>1.262973e-05</td>\n",
       "      <td>0.036591</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model.layers.1.self_attn.q_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(1536, 1536)</td>\n",
       "      <td>-0.386719</td>\n",
       "      <td>0.394531</td>\n",
       "      <td>-1.325980e-05</td>\n",
       "      <td>0.040712</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model.layers.1.self_attn.k_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(256, 1536)</td>\n",
       "      <td>-0.341797</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>-3.723954e-05</td>\n",
       "      <td>0.053641</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>model.layers.1.self_attn.v_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(256, 1536)</td>\n",
       "      <td>-0.217773</td>\n",
       "      <td>0.189453</td>\n",
       "      <td>3.042569e-05</td>\n",
       "      <td>0.031537</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>model.layers.1.self_attn.o_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(1536, 1536)</td>\n",
       "      <td>-0.746094</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>-4.657109e-05</td>\n",
       "      <td>0.048069</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>model.layers.1.mlp.gate_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(8960, 1536)</td>\n",
       "      <td>-0.531250</td>\n",
       "      <td>0.402344</td>\n",
       "      <td>-1.391528e-04</td>\n",
       "      <td>0.041104</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>model.layers.1.mlp.up_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(8960, 1536)</td>\n",
       "      <td>-0.542969</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>-5.409225e-06</td>\n",
       "      <td>0.037177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>model.layers.1.mlp.down_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(1536, 8960)</td>\n",
       "      <td>-0.625000</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>-1.388098e-05</td>\n",
       "      <td>0.037183</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>model.layers.2.self_attn.q_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(1536, 1536)</td>\n",
       "      <td>-0.589844</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>5.410079e-05</td>\n",
       "      <td>0.040616</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>model.layers.2.self_attn.k_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(256, 1536)</td>\n",
       "      <td>-0.400391</td>\n",
       "      <td>0.341797</td>\n",
       "      <td>-5.674783e-05</td>\n",
       "      <td>0.047785</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>model.layers.2.self_attn.v_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(256, 1536)</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>0.191406</td>\n",
       "      <td>2.652831e-05</td>\n",
       "      <td>0.031185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>model.layers.2.self_attn.o_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(1536, 1536)</td>\n",
       "      <td>-0.570312</td>\n",
       "      <td>0.503906</td>\n",
       "      <td>-1.735032e-05</td>\n",
       "      <td>0.049860</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>model.layers.2.mlp.gate_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(8960, 1536)</td>\n",
       "      <td>-0.539062</td>\n",
       "      <td>0.398438</td>\n",
       "      <td>-3.038841e-04</td>\n",
       "      <td>0.042733</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>model.layers.2.mlp.up_proj</td>\n",
       "      <td>Linear</td>\n",
       "      <td>(8960, 1536)</td>\n",
       "      <td>-0.490234</td>\n",
       "      <td>0.558594</td>\n",
       "      <td>1.314533e-05</td>\n",
       "      <td>0.035866</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              layer    type         shape       min       max  \\\n",
       "0   model.layers.0.self_attn.q_proj  Linear  (1536, 1536) -0.953125  0.960938   \n",
       "1   model.layers.0.self_attn.k_proj  Linear   (256, 1536) -0.384766  0.394531   \n",
       "2   model.layers.0.self_attn.v_proj  Linear   (256, 1536) -0.159180  0.197266   \n",
       "3   model.layers.0.self_attn.o_proj  Linear  (1536, 1536) -0.597656  0.605469   \n",
       "4      model.layers.0.mlp.gate_proj  Linear  (8960, 1536) -0.753906  0.660156   \n",
       "5        model.layers.0.mlp.up_proj  Linear  (8960, 1536) -0.457031  0.419922   \n",
       "6      model.layers.0.mlp.down_proj  Linear  (1536, 8960) -0.550781  0.519531   \n",
       "7   model.layers.1.self_attn.q_proj  Linear  (1536, 1536) -0.386719  0.394531   \n",
       "8   model.layers.1.self_attn.k_proj  Linear   (256, 1536) -0.341797  0.406250   \n",
       "9   model.layers.1.self_attn.v_proj  Linear   (256, 1536) -0.217773  0.189453   \n",
       "10  model.layers.1.self_attn.o_proj  Linear  (1536, 1536) -0.746094  0.539062   \n",
       "11     model.layers.1.mlp.gate_proj  Linear  (8960, 1536) -0.531250  0.402344   \n",
       "12       model.layers.1.mlp.up_proj  Linear  (8960, 1536) -0.542969  0.453125   \n",
       "13     model.layers.1.mlp.down_proj  Linear  (1536, 8960) -0.625000  0.640625   \n",
       "14  model.layers.2.self_attn.q_proj  Linear  (1536, 1536) -0.589844  0.535156   \n",
       "15  model.layers.2.self_attn.k_proj  Linear   (256, 1536) -0.400391  0.341797   \n",
       "16  model.layers.2.self_attn.v_proj  Linear   (256, 1536) -0.179688  0.191406   \n",
       "17  model.layers.2.self_attn.o_proj  Linear  (1536, 1536) -0.570312  0.503906   \n",
       "18     model.layers.2.mlp.gate_proj  Linear  (8960, 1536) -0.539062  0.398438   \n",
       "19       model.layers.2.mlp.up_proj  Linear  (8960, 1536) -0.490234  0.558594   \n",
       "\n",
       "            mean       std  zero_ratio  \n",
       "0   1.750172e-05  0.053037         0.0  \n",
       "1  -8.440000e-05  0.061757         0.0  \n",
       "2  -1.660858e-05  0.030817         0.0  \n",
       "3   3.382745e-06  0.046337         0.0  \n",
       "4  -4.833728e-07  0.038473         0.0  \n",
       "5  -5.622278e-06  0.034311         0.0  \n",
       "6   1.262973e-05  0.036591         0.0  \n",
       "7  -1.325980e-05  0.040712         0.0  \n",
       "8  -3.723954e-05  0.053641         0.0  \n",
       "9   3.042569e-05  0.031537         0.0  \n",
       "10 -4.657109e-05  0.048069         0.0  \n",
       "11 -1.391528e-04  0.041104         0.0  \n",
       "12 -5.409225e-06  0.037177         0.0  \n",
       "13 -1.388098e-05  0.037183         0.0  \n",
       "14  5.410079e-05  0.040616         0.0  \n",
       "15 -5.674783e-05  0.047785         0.0  \n",
       "16  2.652831e-05  0.031185         0.0  \n",
       "17 -1.735032e-05  0.049860         0.0  \n",
       "18 -3.038841e-04  0.042733         0.0  \n",
       "19  1.314533e-05  0.035866         0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "def collect_layer_weight_stats(model):\n",
    "    \"\"\"\n",
    "    Collect weight statistics (min, max, mean, std, sparsity) for each Linear layer in the model.\n",
    "    \"\"\"\n",
    "    stats = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            weight = module.weight.data\n",
    "\n",
    "            stats.append(\n",
    "                {\n",
    "                    \"layer\": name,\n",
    "                    \"type\": \"Linear\",\n",
    "                    \"shape\": tuple(weight.shape),\n",
    "                    \"min\": weight.min().item(),\n",
    "                    \"max\": weight.max().item(),\n",
    "                    \"mean\": weight.mean().item(),\n",
    "                    \"std\": weight.std().item(),\n",
    "                    \"zero_ratio\": (weight == 0).float().mean().item(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "\n",
    "# Run the analysis\n",
    "df_stats = collect_layer_weight_stats(model)\n",
    "\n",
    "# Show top 20 layers with highest sparsity\n",
    "df_top_sparse = df_stats.sort_values(\"zero_ratio\", ascending=False).head(20)\n",
    "\n",
    "display(df_top_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12c96831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average zero ratio across groups: 91.43%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load original model\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Choose a specific layer to inspect\n",
    "layer = model.model.layers[0].mlp.gate_proj\n",
    "weight = layer.weight.data  # shape: (8960, 1536)\n",
    "\n",
    "# Simulate group-wise scaling\n",
    "group_size = 128\n",
    "num_groups = weight.shape[0] // group_size\n",
    "qweight_sim = []\n",
    "zero_ratios = []\n",
    "\n",
    "for g in range(num_groups):\n",
    "    start = g * group_size\n",
    "    end = (g + 1) * group_size\n",
    "    w_slice = weight[start:end, :]\n",
    "\n",
    "    # Use 99th percentile as simulated scale\n",
    "    scale = torch.quantile(w_slice.abs(), 0.99)\n",
    "    scale = max(scale.item(), 1e-5)  # Avoid divide-by-zero\n",
    "\n",
    "    # Simulate quantization\n",
    "    scaled = w_slice / scale\n",
    "    rounded = torch.round(scaled)\n",
    "\n",
    "    # Simulate clamping to 4-bit range [0‚Äì15]\n",
    "    q = torch.clamp(rounded, 0, 15)\n",
    "\n",
    "    # Track zero ratio\n",
    "    zero_ratio = (q == 0).float().mean().item()\n",
    "    zero_ratios.append(zero_ratio)\n",
    "\n",
    "print(f\"Average zero ratio across groups: {sum(zero_ratios)/len(zero_ratios):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb39d48",
   "metadata": {},
   "source": [
    "## Summary View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3789c8d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f7fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b3f9e29",
   "metadata": {},
   "source": [
    "# LLAMA3 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6249a5ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B.\n401 Client Error. (Request ID: Root=1-683353fb-51039aed3df2f93708f1d9e0;d16a0d44-2fe3-471d-beec-565097c2ccd1)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:406\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGatedRepoError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    402\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     resolved_file = \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:860\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    859\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m860\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m    862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:967\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m    966\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1482\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1480\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[32m   1481\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1483\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1484\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1374\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1294\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[39m\n\u001b[32m   1293\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1294\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1303\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:278\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:302\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    301\u001b[39m response = get_session().request(method=method, url=url, **params)\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:423\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    420\u001b[39m     message = (\n\u001b[32m    421\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    422\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_message == \u001b[33m\"\u001b[39m\u001b[33mAccess to this resource is disabled.\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mGatedRepoError\u001b[39m: 401 Client Error. (Request ID: Root=1-683353fb-51039aed3df2f93708f1d9e0;d16a0d44-2fe3-471d-beec-565097c2ccd1)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoConfig\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m original_config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta-llama/Meta-Llama-3-8B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(original_config)  \u001b[38;5;66;03m# Compare hidden_size, layers, etc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1017\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1014\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1015\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1017\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1018\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1019\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/transformers/configuration_utils.py:574\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    573\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/transformers/configuration_utils.py:633\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    629\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    632\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    648\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:421\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[32m    420\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    422\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure to have access to it at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    423\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    424\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    427\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a local folder and is not a valid model identifier \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    428\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlisted on \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    429\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    430\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`token=<your_token>`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    431\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B.\n401 Client Error. (Request ID: Root=1-683353fb-51039aed3df2f93708f1d9e0;d16a0d44-2fe3-471d-beec-565097c2ccd1)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "original_config = AutoConfig.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "print(original_config)  # Compare hidden_size, layers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen1.5-1.8B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "original_config = AutoConfig.from_pretrained(\"meta-llama/Meta-Llama-3-70B\")\n",
    "print(original_config)  # Compare hidden_size, layers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75561121",
   "metadata": {},
   "source": [
    "# DeepSeek V3 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c899877c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Loading deepseek-ai/deepseek-v3 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdeepseek-ai/deepseek-v3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(config)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Infer layer names (DeepSeek follows LLaMA-style)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1020\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1018\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1019\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m trust_remote_code = \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_remote_code\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m   1025\u001b[39m     class_ref = config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:678\u001b[39m, in \u001b[36mresolve_trust_remote_code\u001b[39m\u001b[34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001b[39m\n\u001b[32m    675\u001b[39m         _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_local_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    679\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires you to execute the configuration file in that\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    680\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m repo on your local machine. Make sure you have read the code there to avoid malicious use, then\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m set the option `trust_remote_code=True` to remove this error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m     )\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trust_remote_code\n",
      "\u001b[31mValueError\u001b[39m: Loading deepseek-ai/deepseek-v3 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/deepseek-v3\",\n",
    "    torch_dtype=\"meta\",  # No memory/download used\n",
    "    trust_remote_code=True,  # Required for DeepSeek\n",
    "    local_files_only=False,  # Allow fetching config (tiny file)\n",
    ")\n",
    "\n",
    "# Print all layer names + shapes (fake tensors)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb6cf052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config files: ['config.json', 'inference/configs/config_16B.json', 'inference/configs/config_236B.json', 'inference/configs/config_671B.json', 'model.safetensors.index.json', 'tokenizer.json', 'tokenizer_config.json']\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import list_repo_files\n",
    "\n",
    "files = list_repo_files(\"deepseek-ai/deepseek-v3\")\n",
    "print(\"Config files:\", [f for f in files if f.endswith(\".json\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e40f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "# Load models\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "summary(model, input_size=(1, 128), dtypes=[torch.int64])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3fe33",
   "metadata": {},
   "source": [
    "# Inspect Layer Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "61ec2550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files: [PosixPath('/home/xzhang/models/deepseek-r1-distill-qwen-1.5b-awq-scrooge-4bit-g128/quantized_layers/model.layers.0.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-r1-distill-qwen-1.5b-awq-scrooge-4bit-g128/quantized_layers/model.layers.0.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-r1-distill-qwen-1.5b-awq-scrooge-4bit-g128/quantized_layers/model.layers.0.self_attn.v_proj.pt')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from project_config import DEEPSEEK_R1_DISTILL_QUANT_MODEL_OUTPUT_DIR as output_dir\n",
    "\n",
    "layers_dir = output_dir / \"quantized_layers\"\n",
    "print(\"Available files:\", list(layers_dir.glob(\"*\")))\n",
    "\n",
    "\n",
    "# print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47044212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qweight': tensor([[  -1048576,          0,       -256,  ...,          0,     -65536,\n",
       "             1048576],\n",
       "         [     -8176,       4097,     -65280,  ...,       -256,          0,\n",
       "              -65264],\n",
       "         [     -4096,      -3840,   -1048576,  ...,        -16,         16,\n",
       "              -65279],\n",
       "         ...,\n",
       "         [       -27,       -208,     -32255,  ...,     -15359,  -15593215,\n",
       "                -253],\n",
       "         [        -1,        -14,        -47,  ...,         -1,        -16,\n",
       "                  -3],\n",
       "         [   -908766,         -2,      -1242,  ...,        -32,        -46,\n",
       "          1125134880]], dtype=torch.int32),\n",
       " 'qzeros': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32),\n",
       " 'scales': tensor([[0.0556, 0.0556, 0.0556,  ..., 0.0556, 0.0556, 0.0556],\n",
       "         [0.0186, 0.0186, 0.0186,  ..., 0.0186, 0.0186, 0.0186],\n",
       "         [0.0582, 0.0582, 0.0582,  ..., 0.0582, 0.0582, 0.0582],\n",
       "         ...,\n",
       "         [0.0204, 0.0204, 0.0204,  ..., 0.0204, 0.0204, 0.0204],\n",
       "         [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\n",
       "         [0.0149, 0.0149, 0.0149,  ..., 0.0149, 0.0149, 0.0149]],\n",
       "        dtype=torch.float16),\n",
       " 'bias': tensor([-0.0083,  0.0083, -0.0083,  0.0083,  0.0083, -0.0083, -0.0083, -0.0083,\n",
       "          0.0083,  0.0083,  0.0083, -0.0083,  0.0083,  0.0083, -0.0041,  0.0083,\n",
       "          0.0082,  0.0083,  0.0083,  0.0083,  0.0083,  0.0083,  0.0083, -0.0083,\n",
       "          0.0083, -0.0013, -0.0083, -0.0083, -0.0083, -0.0083, -0.0083,  0.0083,\n",
       "          0.0083, -0.0083, -0.0083,  0.0083, -0.0083,  0.0083, -0.0083,  0.0083,\n",
       "          0.0083, -0.0083,  0.0083,  0.0014, -0.0083, -0.0083,  0.0083, -0.0083,\n",
       "          0.0072,  0.0083,  0.0083,  0.0083, -0.0083,  0.0083, -0.0083, -0.0083,\n",
       "          0.0083,  0.0083,  0.0032,  0.0009, -0.0083,  0.0083,  0.0083, -0.0083,\n",
       "         -0.0083,  0.0083, -0.0083,  0.0015,  0.0083, -0.0083,  0.0083, -0.0083,\n",
       "         -0.0083, -0.0083,  0.0083,  0.0083, -0.0083,  0.0081,  0.0083, -0.0022,\n",
       "         -0.0083, -0.0083, -0.0083, -0.0083,  0.0083, -0.0083,  0.0083,  0.0083,\n",
       "         -0.0083,  0.0083,  0.0083,  0.0083,  0.0083, -0.0083, -0.0083, -0.0026,\n",
       "          0.0083, -0.0009,  0.0083, -0.0083, -0.0075, -0.0083, -0.0083, -0.0031,\n",
       "          0.0083,  0.0083,  0.0083, -0.0083,  0.0083, -0.0083, -0.0083, -0.0083,\n",
       "          0.0083,  0.0083,  0.0083,  0.0083,  0.0083,  0.0083, -0.0083,  0.0069,\n",
       "          0.0083, -0.0083, -0.0035,  0.0083,  0.0083,  0.0083, -0.0083, -0.0083,\n",
       "         -0.0083, -0.0083, -0.0083, -0.0083,  0.0077,  0.0083,  0.0083,  0.0083,\n",
       "          0.0083, -0.0083, -0.0083, -0.0083, -0.0083,  0.0083,  0.0061, -0.0083,\n",
       "         -0.0083,  0.0083,  0.0083, -0.0083,  0.0045,  0.0083,  0.0083,  0.0083,\n",
       "          0.0013,  0.0083, -0.0083,  0.0083, -0.0083,  0.0083,  0.0008, -0.0083,\n",
       "          0.0083,  0.0083, -0.0083, -0.0083, -0.0083,  0.0083,  0.0083, -0.0083,\n",
       "         -0.0083, -0.0075, -0.0083, -0.0083, -0.0083, -0.0083, -0.0083,  0.0083,\n",
       "          0.0083,  0.0083,  0.0083,  0.0083,  0.0083, -0.0083,  0.0083, -0.0038,\n",
       "          0.0083, -0.0083,  0.0083, -0.0083, -0.0083,  0.0009, -0.0083,  0.0061,\n",
       "         -0.0083,  0.0062, -0.0083,  0.0083,  0.0083,  0.0083,  0.0083,  0.0083,\n",
       "         -0.0083,  0.0083,  0.0083,  0.0083, -0.0083, -0.0083, -0.0083, -0.0083,\n",
       "         -0.0083, -0.0083,  0.0016,  0.0083, -0.0083, -0.0083,  0.0083, -0.0083,\n",
       "          0.0083, -0.0083,  0.0083, -0.0083, -0.0083,  0.0013, -0.0083, -0.0083,\n",
       "         -0.0083, -0.0083,  0.0083, -0.0083,  0.0083,  0.0083,  0.0083, -0.0083,\n",
       "         -0.0083,  0.0083, -0.0083,  0.0083, -0.0083,  0.0083,  0.0038, -0.0083,\n",
       "         -0.0083,  0.0083,  0.0083,  0.0031, -0.0083, -0.0083, -0.0083, -0.0083,\n",
       "         -0.0083, -0.0083,  0.0083,  0.0083, -0.0083, -0.0083, -0.0083,  0.0083],\n",
       "        dtype=torch.float16, requires_grad=True)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_file = \"model.layers.0.self_attn.v_proj.pt\"  # Change as needed\n",
    "target_file = layers_dir / layer_file\n",
    "data = torch.load(target_file)\n",
    "\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d5cba",
   "metadata": {},
   "source": [
    "## All Files - Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7592aead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üì¶ File: model.layers.0.self_attn.k_proj.pt\n",
      "--------------------------------------------------------------------------------\n",
      "qweight: shape=(1536, 32), dtype=torch.int32\n",
      "qzeros: shape=(12, 32), dtype=torch.int32\n",
      "scales: shape=(12, 256), dtype=torch.float16\n",
      "bias: shape=(256,), dtype=torch.float16\n",
      "================================================================================\n",
      "================================================================================\n",
      "üì¶ File: model.layers.0.self_attn.q_proj.pt\n",
      "--------------------------------------------------------------------------------\n",
      "qweight: shape=(1536, 192), dtype=torch.int32\n",
      "qzeros: shape=(12, 192), dtype=torch.int32\n",
      "scales: shape=(12, 1536), dtype=torch.float16\n",
      "bias: shape=(1536,), dtype=torch.float16\n",
      "================================================================================\n",
      "================================================================================\n",
      "üì¶ File: model.layers.0.self_attn.v_proj.pt\n",
      "--------------------------------------------------------------------------------\n",
      "qweight: shape=(1536, 32), dtype=torch.int32\n",
      "qzeros: shape=(12, 32), dtype=torch.int32\n",
      "scales: shape=(12, 256), dtype=torch.float16\n",
      "bias: shape=(256,), dtype=torch.float16\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from project_config import DEEPSEEK_R1_DISTILL_QUANT_MODEL_OUTPUT_DIR\n",
    "\n",
    "base_dir = DEEPSEEK_R1_DISTILL_QUANT_MODEL_OUTPUT_DIR / \"quantized_layers\"\n",
    "\n",
    "# Iterate over all .pt files under quantized_layers\n",
    "for pt_file in sorted(base_dir.glob(\"**/*.pt\")):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üì¶ File: {pt_file.relative_to(base_dir)}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    try:\n",
    "        data = torch.load(pt_file, map_location=\"cpu\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load: {e}\")\n",
    "        continue\n",
    "\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        print(f\"Tensor shape: {data.shape}\")\n",
    "        print(f\"Tensor dtype: {data.dtype}\")\n",
    "\n",
    "    elif isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"{key}: shape={tuple(value.shape)}, dtype={value.dtype}\")\n",
    "            else:\n",
    "                print(f\"{key}: non-tensor value (type={type(value).__name__})\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Unknown object type: {type(data)}\")\n",
    "\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a9401e",
   "metadata": {},
   "source": [
    "## Qweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1973cf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Inspecting 11 layer files in: /home/xzhang/models/deepseek-awq-scrooge/quantized_layers\n",
      "\n",
      "model.layers.0.mlp.down_proj.pt          | shape: (8960, 192)          | zeros: 0.80% | unique[:10]: [-2147483632, -2147483595, -2147478528, -2147477696, -2147471184, -2147463135, -2147351983, -2147342333, -2147286528, -2147270592] ‚úÖ\n",
      "\n",
      "model.layers.0.mlp.gate_proj.pt          | shape: (1536, 1120)         | zeros: 1.48% | unique[:10]: [-2147483632, -2147483136, -2147482605, -2147477706, -2147469259, -2147458810, -2147454684, -2147411712, -2147409151, -2147405529] ‚úÖ\n",
      "\n",
      "model.layers.0.mlp.up_proj.pt            | shape: (1536, 1120)         | zeros: 3.10% | unique[:10]: [-2143018463, -2130636768, -1877794043, -1861152494, -1861082576, -1860168944, -1859911150, -1859055340, -1844444879, -1843068415] ‚úÖ\n",
      "\n",
      "model.layers.0.self_attn.k_proj.pt       | shape: (1536, 32)           | zeros: 4.22% | unique[:10]: [-2147465929, -2147286224, -2146239273, -2145711647, -2140118910, -2132942554, -2130898540, -2130403142, -2130104181, -2113228400] ‚úÖ\n",
      "\n",
      "model.layers.0.self_attn.q_proj.pt       | shape: (1536, 192)          | zeros: 8.61% | unique[:10]: [-2147483638, -2147483616, -2147475454, -2147474905, -2147458956, -2145254901, -2144821040, -2143288576, -2142631052, -2140638624] ‚úÖ\n",
      "\n",
      "model.layers.0.self_attn.v_proj.pt       | shape: (1536, 32)           | zeros: 1.54% | unique[:10]: [-2145246039, -2141144991, -2111315297, -2105514998, -2097055951, -2090624476, -2080208636, -2076988576, -2063154648, -1997127085] ‚úÖ\n",
      "\n",
      "model.layers.1.mlp.gate_proj.pt          | shape: (1536, 1120)         | zeros: 0.32% | unique[:10]: [-2147470287, -2147449294, -2147446010, -2147340250, -2147319532, -2147269004, -2147076541, -2147065584, -2146360749, -2146102000] ‚úÖ\n",
      "\n",
      "model.layers.1.mlp.up_proj.pt            | shape: (1536, 1120)         | zeros: 0.10% | unique[:10]: [-2147462047, -2147450878, -2147417328, -2147413983, -2147352301, -2147284907, -2147273193, -2147264412, -2147253760, -2147250111] ‚úÖ\n",
      "\n",
      "model.layers.1.self_attn.k_proj.pt       | shape: (1536, 32)           | zeros: 2.51% | unique[:10]: [-2140121472, -2110644213, -2092609503, -2087714755, -2071252114, -2063400947, -2058302595, -2044706780, -1995226985, -1988075360] ‚úÖ\n",
      "\n",
      "model.layers.1.self_attn.q_proj.pt       | shape: (1536, 192)          | zeros: 2.47% | unique[:10]: [-2147483648, -2147465206, -2147460304, -2147250150, -2147200591, -2147161344, -2146542159, -2146366608, -2145845235, -2145613964] ‚úÖ\n",
      "\n",
      "model.layers.1.self_attn.v_proj.pt       | shape: (1536, 32)           | zeros: 1.66% | unique[:10]: [-2147073772, -2140076332, -2131398546, -2130644744, -2097119226, -2097046711, -2087503280, -2014684833, -1935668839, -1878848901] ‚úÖ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def inspect_qweights_in_dir(layer_dir: Path, zero_threshold: float = 0.9):\n",
    "    \"\"\"\n",
    "    Inspect qweight statistics for all saved quantized layer files in a directory.\n",
    "\n",
    "    Args:\n",
    "        layer_dir (Path): Directory containing *.pt quantized layer files.\n",
    "        zero_threshold (float): Warn if percentage of zeros in qweight exceeds this.\n",
    "    \"\"\"\n",
    "    layer_dir = Path(layer_dir).expanduser()\n",
    "    if not layer_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {layer_dir}\")\n",
    "\n",
    "    pt_files = sorted(layer_dir.glob(\"*.pt\"))\n",
    "    if not pt_files:\n",
    "        print(\"‚ùå No .pt layer files found in directory.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nüîç Inspecting {len(pt_files)} layer files in: {layer_dir}\\n\")\n",
    "\n",
    "    for f in pt_files:\n",
    "        try:\n",
    "            state_dict = torch.load(f, map_location=\"cpu\")\n",
    "            qweight = state_dict.get(\"qweight\", None)\n",
    "\n",
    "            if qweight is None:\n",
    "                print(f\"‚ö†Ô∏è  {f.name}: Missing `qweight` key.\")\n",
    "                continue\n",
    "\n",
    "            zeros = (qweight == 0).sum().item()\n",
    "            total = qweight.numel()\n",
    "            zero_pct = zeros / total\n",
    "\n",
    "            unique_vals = torch.unique(qweight)\n",
    "            preview_vals = unique_vals.tolist()[:10]\n",
    "\n",
    "            flag = \"‚ö†Ô∏è HIGH ZERO RATIO!\" if zero_pct > zero_threshold else \"‚úÖ\"\n",
    "\n",
    "            # ‚úÖ FIX: Convert tuple to string before formatting\n",
    "            shape_str = str(tuple(qweight.shape))\n",
    "\n",
    "            print(\n",
    "                f\"{f.name:<40} | shape: {shape_str:<20} | \"\n",
    "                f\"zeros: {zero_pct:.2%} | unique[:10]: {preview_vals} {flag}\"\n",
    "            )\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {f.name}: Error loading or parsing file ‚Äî {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage ‚Äî adjust this path if needed\n",
    "    quantized_dir = Path(\"~/models/deepseek-awq-scrooge/quantized_layers\")\n",
    "    inspect_qweights_in_dir(quantized_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba852846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contents of model.layers.0.self_attn.q_proj.pt:\n",
      "  qweight    ‚Üí shape: (1536, 192), dtype: torch.int32\n",
      "  qzeros     ‚Üí shape: (12, 192), dtype: torch.int32\n",
      "  scales     ‚Üí shape: (12, 1536), dtype: torch.float16\n",
      "  bias       ‚Üí shape: (1536,), dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the file\n",
    "base_dir = Path(\"/home/xzhang/models/deepseek-awq-scrooge/quantized_layers\")\n",
    "layer_file = \"model.layers.0.self_attn.q_proj.pt\"\n",
    "target_file = base_dir / layer_file\n",
    "\n",
    "# Load the file\n",
    "data = torch.load(target_file, map_location=\"cpu\")\n",
    "\n",
    "print(f\"\\nContents of {layer_file}:\")\n",
    "\n",
    "if isinstance(data, dict):\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"  {key:<10} ‚Üí shape: {tuple(value.shape)}, dtype: {value.dtype}\")\n",
    "        else:\n",
    "            print(f\"  {key:<10} ‚Üí type: {type(value).__name__}\")\n",
    "else:\n",
    "    print(\"File content is not a dict.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ea438b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xzhang/models/deepseek-r1-distill-qwen-1.5b-awq-scrooge-4bit-g128/quantized_layers/model.layers.0.self_attn.q_proj.pt\n",
      "\n",
      "Analyzing model.layers.0.self_attn.q_proj.pt\n",
      "--------------------------------------------------\n",
      "Group size: 128\n",
      "Scales range: 0.0072 - 0.1002\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 128, 1536]' is invalid for input of size 18432",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m file = base_dir / layer_file\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(file)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[43manalyze_quantization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36manalyze_quantization\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScales range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscales.min().item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscales.max().item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Properly expand dimensions for broadcasting\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m scales = \u001b[43mscales\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscales\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [groups, group_size, out_features]\u001b[39;00m\n\u001b[32m     31\u001b[39m scales = scales.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# [groups, out_features, group_size]\u001b[39;00m\n\u001b[32m     32\u001b[39m scales = scales.reshape(-\u001b[32m1\u001b[39m, scales.shape[-\u001b[32m1\u001b[39m])  \u001b[38;5;66;03m# [groups*out_features, group_size]\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[-1, 128, 1536]' is invalid for input of size 18432"
     ]
    }
   ],
   "source": [
    "# check_quantization_efficiency.py\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from project_config import DEEPSEEK_R1_DISTILL_QUANT_MODEL_OUTPUT_DIR\n",
    "\n",
    "\n",
    "def analyze_quantization(file_path):\n",
    "    data = torch.load(file_path)\n",
    "\n",
    "    if not isinstance(data, dict):\n",
    "        print(\"Error: Expected quantized layer dictionary\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nAnalyzing {file_path.name}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Extract parameters\n",
    "    qweight = data[\"qweight\"]  # shape: [in_features, out_features//8]\n",
    "    scales = data[\"scales\"]  # shape: [in_features//group_size, out_features]\n",
    "    qzeros = data[\"qzeros\"]  # shape: [in_features//group_size, out_features//8]\n",
    "\n",
    "    group_size = qweight.shape[0] // scales.shape[0]\n",
    "    print(f\"Group size: {group_size}\")\n",
    "    print(f\"Scales range: {scales.min().item():.4f} - {scales.max().item():.4f}\")\n",
    "\n",
    "    # Properly expand dimensions for broadcasting\n",
    "    scales = scales.view(\n",
    "        -1, group_size, scales.shape[-1]\n",
    "    )  # [groups, group_size, out_features]\n",
    "    scales = scales.transpose(1, 2)  # [groups, out_features, group_size]\n",
    "    scales = scales.reshape(-1, scales.shape[-1])  # [groups*out_features, group_size]\n",
    "\n",
    "    qzeros = qzeros.view(-1, 1, qzeros.shape[-1])  # [groups, 1, out_features//8]\n",
    "    qzeros = qzeros.expand(-1, group_size, -1)  # [groups, group_size, out_features//8]\n",
    "    qzeros = qzeros.reshape(-1, qzeros.shape[-1])  # [in_features, out_features//8]\n",
    "\n",
    "    # Dequantize sample weights\n",
    "    sample_qweight = qweight[:group_size]  # First group only for demo\n",
    "    sample_qzeros = qzeros[:group_size]\n",
    "    sample_scales = scales[:group_size]\n",
    "\n",
    "    dequant_weight = (sample_qweight - sample_qzeros) * sample_scales\n",
    "    print(\n",
    "        f\"Sample dequantized range: {dequant_weight.min().item():.4f} - {dequant_weight.max().item():.4f}\"\n",
    "    )\n",
    "\n",
    "    # Check 4-bit utilization\n",
    "    quantized_values = qweight.unique(sorted=True)\n",
    "    print(f\"Unique 4-bit values: {len(quantized_values)}/16\")\n",
    "    print(\n",
    "        f\"Value range: {quantized_values.min().item()} to {quantized_values.max().item()}\"\n",
    "    )\n",
    "\n",
    "    # Plot first group's weights\n",
    "\n",
    "    plt.hist(qweight[:group_size].cpu().flatten().numpy(), bins=16)\n",
    "    plt.title(\"4-bit Weight Values (First Group)\")\n",
    "    plt.xlabel(\"Quantized Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = DEEPSEEK_R1_DISTILL_QUANT_MODEL_OUTPUT_DIR / \"quantized_layers\"\n",
    "    layer_file = \"model.layers.0.self_attn.q_proj.pt\"\n",
    "    file = base_dir / layer_file\n",
    "\n",
    "    print(file)\n",
    "    analyze_quantization(base_dir / layer_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f65a7f",
   "metadata": {},
   "source": [
    "## QZeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044a9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys: dict_keys(['qweight', 'qzeros', 'scales', 'bias'])\n",
      "qzeros shape: torch.Size([12, 192])\n",
      "qzeros dtype: torch.int32\n",
      "qzeros stats ‚Äî min: 0 max: 285216768 mean: 2660141.0\n",
      "qzeros zero ratio: 96.35%\n",
      "qzeros preview:\n",
      " tensor([[        0,         0,         0,         0,         0,         0,\n",
      "                 0,         0,         0,         0],\n",
      "        [        0,         0,         0,         0,         0,         0,\n",
      "                 0,         0,         0,         0],\n",
      "        [        0,         0,         0,         0,         0,         0,\n",
      "                 0,         0,         0,         0],\n",
      "        [        0,         0,         0,         0,         0,         0,\n",
      "                 0, 268439552,         0,         0],\n",
      "        [        0,         0,         0,         0,         0,         0,\n",
      "                 0,         0,         0,         0],\n",
      "        [        0,         0,         0,         0,         0,         0,\n",
      "                 0,         0,         0,         0],\n",
      "        [        0,         0,         0,         0,         0,         0,\n",
      "                 0,         0,         0,         0],\n",
      "        [        0,         0,         0,         0,         0,         0,\n",
      "                 0,         0,         0,         0],\n",
      "        [        0,         0,         0,         0,         0,         0,\n",
      "                 0,         0,         0,         0],\n",
      "        [        0,         0,         0,         0,         0,         0,\n",
      "                 0,         0,         0,         0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from project_config import DEEPSEEK_R1_DISTILL_QUANT_MODEL_OUTPUT_DIR\n",
    "\n",
    "base_dir = DEEPSEEK_R1_DISTILL_QUANT_MODEL_OUTPUT_DIR / \"quantized_layers\"\n",
    "layer_file = \"model.layers.0.self_attn.q_proj.pt\"\n",
    "file = base_dir / layer_file\n",
    "data = torch.load(file)\n",
    "\n",
    "# Inspect available keys\n",
    "print(\"Available keys:\", data.keys())\n",
    "\n",
    "qzeros = data[\"qzeros\"]\n",
    "print(\"qzeros shape:\", qzeros.shape)\n",
    "print(\"qzeros dtype:\", qzeros.dtype)\n",
    "print(\n",
    "    \"qzeros stats ‚Äî min:\",\n",
    "    qzeros.min().item(),\n",
    "    \"max:\",\n",
    "    qzeros.max().item(),\n",
    "    \"mean:\",\n",
    "    qzeros.float().mean().item(),\n",
    ")\n",
    "\n",
    "# Percent age zero values\n",
    "zero_ratio = (qzeros == 0).float().mean().item()\n",
    "print(f\"qzeros zero ratio: {zero_ratio:.2%}\")\n",
    "\n",
    "# Preview\n",
    "print(\"qzeros preview:\\n\", qzeros[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_quantization_efficiency.py\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def analyze_quantization(file_path):\n",
    "    data = torch.load(file_path)\n",
    "\n",
    "    if not isinstance(data, dict):\n",
    "        print(\"Error: Expected quantized layer dictionary (qweight, scales, qzeros)\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nAnalyzing {file_path.name}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Extract quantization parameters\n",
    "    qweight = data[\"qweight\"]\n",
    "    scales = data[\"scales\"]\n",
    "    qzeros = data[\"qzeros\"]\n",
    "    group_size = scales.shape[0] * (qweight.shape[0] // scales.shape[0])\n",
    "\n",
    "    print(f\"Group size: {group_size}\")\n",
    "    print(f\"Scales range: {scales.min().item():.4f} - {scales.max().item():.4f}\")\n",
    "\n",
    "    # Simulate dequantization\n",
    "    dequant_weight = (qweight - qzeros) * scales\n",
    "    print(\n",
    "        f\"Dequantized weight range: {dequant_weight.min().item():.4f} - {dequant_weight.max().item():.4f}\"\n",
    "    )\n",
    "\n",
    "    # Check 4-bit utilization\n",
    "    quantized_values = qweight.unique(sorted=True)\n",
    "    print(f\"Unique 4-bit values used: {len(quantized_values)}/16 possible\")\n",
    "    print(\n",
    "        f\"Value range: {quantized_values.min().item()} to {quantized_values.max().item()}\"\n",
    "    )\n",
    "\n",
    "    # Plot value distribution\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.hist(qweight.cpu().flatten().numpy(), bins=50)\n",
    "    plt.title(f\"4-bit Weight Distribution\\n{file_path.name}\")\n",
    "    plt.xlabel(\"Quantized Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = Path(\"/home/xzhang/models/deepseek-awq-scrooge/quantized_layers\")\n",
    "    layer_file = \"model.layers.0.self_attn.q_proj.pt\"  # Change as needed\n",
    "    analyze_quantization(base_dir / layer_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de1421",
   "metadata": {},
   "source": [
    "## Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6bd78abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from project_config import DEEPSEEK_R1_DISTILL_QUANT_MODEL_OUTPUT_DIR\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "base_dir = DEEPSEEK_R1_DISTILL_QUANT_MODEL_OUTPUT_DIR / \"quantized_layers\"\n",
    "layer_file = \"model.layers.0.self_attn.q_proj.pt\"\n",
    "file = base_dir / layer_file\n",
    "state_dict = torch.load(file)\n",
    "\n",
    "scales = state_dict[\"scales\"]\n",
    "\n",
    "\n",
    "def plot_scale_diagnostics(\n",
    "    scales: torch.Tensor, title=\"Scale Distribution\", save_path=None\n",
    "):\n",
    "    scales_np = scales.cpu().numpy().flatten()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(scales_np, bins=50, color=\"blue\", alpha=0.7)\n",
    "    plt.title(\"Histogram\")\n",
    "    plt.xlabel(\"Scale Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(scales_np, marker=\"o\", linestyle=\"-\", alpha=0.7)\n",
    "    plt.title(\"Per-Group Scale Values\")\n",
    "    plt.xlabel(\"Group Index\")\n",
    "    plt.ylabel(\"Scale\")\n",
    "\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "        print(f\"‚úÖ Saved plot to: {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_scale_diagnostics(scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128964e",
   "metadata": {},
   "source": [
    "# Inspect Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40d0023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file filtered successfully. Output saved to /home/xzhang/dev/deepseek_local_runner/documents/filtered_rcs.log\n"
     ]
    }
   ],
   "source": [
    "def filter_log_file(input_file, output_file, keyword):\n",
    "    try:\n",
    "        with open(input_file, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        filtered_lines = [line for line in lines if keyword not in line]\n",
    "\n",
    "        with open(output_file, \"w\") as new_file:\n",
    "            new_file.writelines(filtered_lines)\n",
    "\n",
    "        print(f\"Log file filtered successfully. Output saved to {output_file}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Input file {input_file} not found.\")\n",
    "\n",
    "\n",
    "log_file = \"/home/xzhang/dev/deepseek_local_runner/documents/full_log_20250529_183204_resource.log\"\n",
    "filtered_log_file = \"/home/xzhang/dev/deepseek_local_runner/documents/filtered_rcs.log\"\n",
    "\n",
    "# Usage\n",
    "input_file = log_file\n",
    "output_file = filtered_log_file\n",
    "keyword = \"[AutoMonitor]\"\n",
    "\n",
    "filter_log_file(input_file, output_file, keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c9ef2",
   "metadata": {},
   "source": [
    "# Inspect Saftetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676229ce",
   "metadata": {},
   "source": [
    "## All Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f61c892b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0f5f0da0534a4ba11417b7a8918636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n    <div style=\"\\n        height: 400px;\\n        overflow: auto;\\n        background-color: bla‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "from project_config import DEEPSEEK_R1_DISTILL_QUANT_MODEL_DIR\n",
    "\n",
    "\n",
    "def inspect_safetensors_pretty(path: str | Path, max_preview: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    Load and format .safetensors inspection results as a list of printable strings.\n",
    "\n",
    "    Args:\n",
    "        path (str | Path): Path to the .safetensors file.\n",
    "        max_preview (int): Number of preview elements for small tensors.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Cleaned list of lines describing tensor structure and sample values.\n",
    "    \"\"\"\n",
    "    path = Path(path).expanduser()\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    state_dict = load_file(path)\n",
    "    lines = []\n",
    "    lines.append(f\"üîç Inspecting {path.name} ‚Äî {len(state_dict)} tensors found:\")\n",
    "\n",
    "    for name, tensor in state_dict.items():\n",
    "        line = f\"‚Ä¢ {name}: shape={tuple(tensor.shape)}, dtype={tensor.dtype}\"\n",
    "        if tensor.ndim <= 2 and tensor.numel() < 100:\n",
    "            preview = tensor.flatten()[:max_preview].tolist()\n",
    "            line += f\" | preview: {preview}\"\n",
    "        lines.append(line)\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "tensor_file = DEEPSEEK_R1_DISTILL_QUANT_MODEL_DIR / \"model.safetensors\"\n",
    "tensor_info = inspect_safetensors(tensor_file)\n",
    "\n",
    "show_scrollable(tensor_info, height=\"400px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7994cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tensor 'model.layers.0.self_attn.o_proj.weight' found in model.safetensors:\n",
      "   shape: (1536, 1536)\n",
      "   dtype: torch.bfloat16\n",
      "   preview: [-0.00151824951171875, 0.0012969970703125, -0.01141357421875, -0.046875, -0.0123291015625, -0.01519775390625, 0.01806640625, 0.0002899169921875, 0.021484375, -0.0076904296875]\n"
     ]
    }
   ],
   "source": [
    "tensor_file = DEEPSEEK_R1_DISTILL_QUANT_MODEL_DIR / \"model.safetensors\"\n",
    "\n",
    "summary = inspect_tensor(tensor_file, \"model.layers.0.self_attn.o_proj.weight\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d275800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Inspecting model.safetensors ‚Äî 588 tensors found:\n",
      "\n",
      "- model.layers.0.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.0.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.0.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.0.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.0.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.0.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.0.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.0.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.0.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.0.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.0.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.0.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.0.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.0.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.0.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.0.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.0.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.0.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.0.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.0.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.0.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.1.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.1.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.1.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.1.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.1.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.1.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.1.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.1.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.1.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.1.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.1.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.1.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.1.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.1.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.1.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.1.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.1.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.1.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.1.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.1.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.1.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.2.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.2.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.2.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.2.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.2.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.2.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.2.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.2.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.2.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.2.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.2.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.2.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.2.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.2.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.2.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.2.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.2.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.2.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.2.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.2.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.2.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.3.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.3.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.3.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.3.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.3.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.3.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.3.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.3.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.3.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.3.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.3.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.3.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.3.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.3.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.3.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.3.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.3.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.3.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.3.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.3.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.3.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.4.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.4.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.4.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.4.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.4.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.4.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.4.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.4.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.4.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.4.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.4.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.4.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.4.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.4.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.4.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.4.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.4.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.4.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.4.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.4.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.4.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.5.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.5.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.5.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.5.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.5.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.5.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.5.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.5.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.5.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.5.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.5.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.5.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.5.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.5.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.5.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.5.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.5.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.5.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.5.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.5.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.5.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.6.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.6.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.6.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.6.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.6.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.6.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.6.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.6.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.6.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.6.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.6.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.6.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.6.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.6.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.6.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.6.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.6.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.6.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.6.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.6.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.6.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.7.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.7.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.7.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.7.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.7.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.7.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.7.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.7.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.7.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.7.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.7.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.7.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.7.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.7.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.7.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.7.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.7.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.7.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.7.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.7.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.7.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.8.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.8.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.8.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.8.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.8.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.8.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.8.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.8.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.8.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.8.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.8.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.8.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.8.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.8.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.8.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.8.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.8.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.8.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.8.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.8.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.8.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.9.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.9.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.9.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.9.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.9.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.9.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.9.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.9.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.9.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.9.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.9.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.9.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.9.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.9.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.9.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.9.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.9.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.9.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.9.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.9.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.9.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.10.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.10.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.10.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.10.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.10.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.10.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.10.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.10.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.10.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.10.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.10.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.10.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.10.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.10.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.10.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.10.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.10.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.10.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.10.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.10.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.10.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.11.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.11.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.11.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.11.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.11.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.11.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.11.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.11.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.11.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.11.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.11.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.11.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.11.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.11.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.11.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.11.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.11.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.11.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.11.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.11.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.11.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.12.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.12.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.12.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.12.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.12.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.12.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.12.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.12.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.12.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.12.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.12.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.12.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.12.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.12.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.12.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.12.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.12.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.12.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.12.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.12.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.12.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.13.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.13.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.13.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.13.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.13.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.13.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.13.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.13.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.13.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.13.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.13.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.13.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.13.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.13.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.13.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.13.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.13.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.13.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.13.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.13.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.13.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.14.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.14.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.14.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.14.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.14.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.14.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.14.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.14.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.14.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.14.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.14.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.14.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.14.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.14.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.14.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.14.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.14.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.14.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.14.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.14.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.14.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.15.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.15.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.15.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.15.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.15.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.15.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.15.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.15.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.15.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.15.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.15.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.15.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.15.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.15.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.15.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.15.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.15.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.15.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.15.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.15.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.15.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.16.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.16.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.16.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.16.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.16.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.16.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.16.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.16.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.16.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.16.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.16.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.16.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.16.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.16.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.16.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.16.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.16.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.16.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.16.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.16.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.16.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.17.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.17.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.17.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.17.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.17.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.17.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.17.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.17.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.17.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.17.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.17.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.17.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.17.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.17.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.17.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.17.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.17.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.17.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.17.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.17.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.17.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.18.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.18.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.18.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.18.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.18.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.18.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.18.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.18.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.18.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.18.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.18.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.18.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.18.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.18.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.18.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.18.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.18.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.18.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.18.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.18.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.18.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.19.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.19.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.19.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.19.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.19.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.19.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.19.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.19.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.19.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.19.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.19.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.19.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.19.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.19.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.19.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.19.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.19.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.19.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.19.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.19.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.19.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.20.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.20.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.20.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.20.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.20.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.20.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.20.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.20.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.20.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.20.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.20.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.20.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.20.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.20.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.20.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.20.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.20.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.20.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.20.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.20.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.20.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.21.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.21.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.21.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.21.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.21.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.21.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.21.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.21.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.21.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.21.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.21.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.21.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.21.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.21.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.21.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.21.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.21.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.21.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.21.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.21.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.21.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.22.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.22.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.22.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.22.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.22.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.22.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.22.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.22.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.22.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.22.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.22.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.22.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.22.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.22.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.22.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.22.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.22.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.22.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.22.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.22.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.22.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.23.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.23.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.23.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.23.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.23.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.23.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.23.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.23.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.23.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.23.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.23.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.23.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.23.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.23.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.23.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.23.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.23.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.23.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.23.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.23.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.23.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.24.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.24.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.24.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.24.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.24.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.24.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.24.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.24.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.24.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.24.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.24.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.24.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.24.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.24.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.24.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.24.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.24.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.24.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.24.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.24.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.24.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.25.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.25.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.25.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.25.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.25.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.25.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.25.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.25.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.25.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.25.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.25.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.25.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.25.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.25.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.25.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.25.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.25.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.25.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.25.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.25.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.25.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.26.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.26.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.26.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.26.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.26.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.26.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.26.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.26.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.26.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.26.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.26.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.26.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.26.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.26.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.26.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.26.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.26.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.26.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.26.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.26.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.26.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.27.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "- model.layers.27.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "- model.layers.27.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "- model.layers.27.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.27.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.27.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.27.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "- model.layers.27.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "- model.layers.27.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "- model.layers.27.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.27.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.27.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.27.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "- model.layers.27.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "- model.layers.27.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "- model.layers.27.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "- model.layers.27.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "- model.layers.27.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "- model.layers.27.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "- model.layers.27.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "- model.layers.27.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def natural_key(text):\n",
    "    # Breaks text into chunks of digits and non-digits: \"layers.10\" ‚Üí [\"layers.\", 10]\n",
    "    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", text)]\n",
    "\n",
    "\n",
    "def inspect_safetensors(path: str | Path, max_preview: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Print a readable, naturally sorted summary of a .safetensors file.\n",
    "\n",
    "    Args:\n",
    "        path (str | Path): Path to the .safetensors file.\n",
    "        max_preview (int): Max number of values to preview for small tensors.\n",
    "    \"\"\"\n",
    "    path = Path(path).expanduser()\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    state_dict = load_file(path)\n",
    "    sorted_items = sorted(state_dict.items(), key=lambda x: natural_key(x[0]))\n",
    "\n",
    "    print(f\"\\nüîç Inspecting {path.name} ‚Äî {len(sorted_items)} tensors found:\\n\")\n",
    "\n",
    "    for name, tensor in sorted_items:\n",
    "        print(f\"- {name}: shape={tuple(tensor.shape)}, dtype={tensor.dtype}\")\n",
    "        if tensor.ndim <= 2 and tensor.numel() < 100:\n",
    "            print(f\"   preview: {tensor.flatten()[:max_preview].tolist()}\")\n",
    "\n",
    "\n",
    "tensor_file = Path(\n",
    "    \"~/models/deepseek-r1-distill-qwen-1.5b-awq-scrooge-4bit-g128/quantized_model/model.safetensors\"\n",
    ").expanduser()\n",
    "\n",
    "inspect_safetensors(tensor_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ad4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Missing critical weights: {'model.embed_tokens.weight', 'lm_head.weight'}\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "required_keys = {\"model.embed_tokens.weight\", \"lm_head.weight\"}  # Example keys\n",
    "tensor_file = Path(\n",
    "    \"~/models/deepseek-awq-scrooge/quantized_model/model.safetensors\"\n",
    ").expanduser()\n",
    "with safe_open(tensor_file, framework=\"pt\") as f:\n",
    "    missing = required_keys - set(f.keys())\n",
    "    if missing:\n",
    "        print(f\"WARNING: Missing critical weights: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3fcb4865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Total tensors: 588\n",
      "model.layers.0.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.0.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.0.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.0.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.0.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.0.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.0.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.0.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.0.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.0.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.0.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.0.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.0.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.0.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.0.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.0.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.0.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.0.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.0.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.0.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.0.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.1.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.1.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.1.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.1.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.1.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.1.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.1.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.1.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.1.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.1.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.1.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.1.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.1.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.1.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.1.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.1.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.1.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.1.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.1.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.1.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.1.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.10.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.10.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.10.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.10.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.10.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.10.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.10.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.10.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.10.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.10.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.10.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.10.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.10.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.10.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.10.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.10.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.10.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.10.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.10.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.10.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.10.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.11.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.11.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.11.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.11.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.11.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.11.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.11.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.11.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.11.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.11.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.11.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.11.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.11.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.11.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.11.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.11.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.11.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.11.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.11.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.11.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.11.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.12.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.12.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.12.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.12.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.12.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.12.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.12.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.12.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.12.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.12.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.12.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.12.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.12.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.12.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.12.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.12.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.12.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.12.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.12.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.12.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.12.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.13.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.13.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.13.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.13.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.13.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.13.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.13.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.13.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.13.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.13.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.13.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.13.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.13.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.13.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.13.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.13.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.13.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.13.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.13.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.13.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.13.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.14.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.14.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.14.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.14.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.14.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.14.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.14.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.14.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.14.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.14.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.14.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.14.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.14.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.14.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.14.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.14.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.14.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.14.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.14.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.14.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.14.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.15.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.15.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.15.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.15.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.15.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.15.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.15.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.15.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.15.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.15.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.15.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.15.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.15.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.15.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.15.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.15.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.15.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.15.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.15.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.15.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.15.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.16.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.16.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.16.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.16.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.16.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.16.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.16.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.16.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.16.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.16.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.16.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.16.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.16.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.16.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.16.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.16.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.16.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.16.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.16.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.16.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.16.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.17.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.17.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.17.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.17.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.17.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.17.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.17.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.17.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.17.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.17.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.17.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.17.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.17.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.17.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.17.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.17.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.17.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.17.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.17.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.17.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.17.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.18.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.18.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.18.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.18.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.18.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.18.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.18.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.18.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.18.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.18.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.18.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.18.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.18.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.18.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.18.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.18.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.18.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.18.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.18.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.18.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.18.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.19.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.19.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.19.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.19.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.19.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.19.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.19.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.19.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.19.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.19.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.19.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.19.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.19.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.19.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.19.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.19.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.19.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.19.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.19.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.19.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.19.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.2.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.2.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.2.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.2.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.2.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.2.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.2.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.2.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.2.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.2.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.2.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.2.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.2.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.2.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.2.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.2.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.2.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.2.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.2.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.2.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.2.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.20.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.20.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.20.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.20.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.20.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.20.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.20.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.20.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.20.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.20.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.20.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.20.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.20.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.20.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.20.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.20.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.20.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.20.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.20.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.20.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.20.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.21.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.21.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.21.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.21.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.21.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.21.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.21.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.21.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.21.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.21.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.21.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.21.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.21.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.21.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.21.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.21.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.21.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.21.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.21.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.21.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.21.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.22.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.22.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.22.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.22.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.22.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.22.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.22.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.22.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.22.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.22.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.22.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.22.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.22.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.22.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.22.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.22.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.22.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.22.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.22.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.22.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.22.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.23.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.23.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.23.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.23.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.23.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.23.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.23.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.23.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.23.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.23.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.23.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.23.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.23.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.23.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.23.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.23.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.23.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.23.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.23.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.23.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.23.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.24.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.24.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.24.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.24.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.24.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.24.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.24.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.24.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.24.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.24.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.24.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.24.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.24.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.24.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.24.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.24.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.24.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.24.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.24.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.24.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.24.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.25.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.25.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.25.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.25.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.25.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.25.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.25.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.25.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.25.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.25.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.25.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.25.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.25.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.25.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.25.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.25.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.25.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.25.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.25.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.25.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.25.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.26.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.26.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.26.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.26.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.26.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.26.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.26.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.26.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.26.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.26.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.26.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.26.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.26.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.26.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.26.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.26.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.26.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.26.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.26.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.26.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.26.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.27.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.27.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.27.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.27.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.27.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.27.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.27.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.27.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.27.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.27.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.27.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.27.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.27.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.27.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.27.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.27.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.27.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.27.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.27.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.27.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.27.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.3.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.3.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.3.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.3.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.3.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.3.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.3.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.3.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.3.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.3.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.3.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.3.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.3.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.3.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.3.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.3.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.3.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.3.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.3.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.3.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.3.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.4.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.4.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.4.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.4.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.4.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.4.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.4.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.4.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.4.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.4.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.4.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.4.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.4.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.4.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.4.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.4.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.4.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.4.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.4.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.4.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.4.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.5.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.5.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.5.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.5.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.5.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.5.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.5.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.5.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.5.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.5.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.5.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.5.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.5.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.5.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.5.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.5.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.5.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.5.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.5.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.5.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.5.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.6.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.6.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.6.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.6.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.6.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.6.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.6.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.6.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.6.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.6.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.6.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.6.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.6.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.6.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.6.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.6.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.6.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.6.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.6.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.6.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.6.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.7.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.7.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.7.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.7.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.7.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.7.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.7.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.7.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.7.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.7.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.7.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.7.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.7.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.7.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.7.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.7.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.7.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.7.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.7.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.7.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.7.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.8.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.8.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.8.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.8.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.8.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.8.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.8.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.8.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.8.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.8.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.8.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.8.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.8.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.8.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.8.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.8.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.8.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.8.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.8.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.8.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.8.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.9.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32\n",
      "model.layers.9.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32\n",
      "model.layers.9.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16\n",
      "model.layers.9.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.9.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.9.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.9.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32\n",
      "model.layers.9.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32\n",
      "model.layers.9.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16\n",
      "model.layers.9.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.9.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.9.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.9.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16\n",
      "model.layers.9.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16\n",
      "model.layers.9.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32\n",
      "model.layers.9.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32\n",
      "model.layers.9.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16\n",
      "model.layers.9.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16\n",
      "model.layers.9.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32\n",
      "model.layers.9.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32\n",
      "model.layers.9.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensor_file = Path(\n",
    "    \"~/models/deepseek-r1-distill-qwen-1.5b-awq-scrooge-4bit-g128/quantized_model/model.safetensors\"\n",
    ").expanduser()\n",
    "\n",
    "with safe_open(tensor_file, framework=\"pt\") as f:\n",
    "    print(f\"\\nüîç Total tensors: {len(f.keys())}\")\n",
    "    for key in f.keys():\n",
    "        tensor = f.get_tensor(key)\n",
    "        print(f\"{key}: shape={tuple(tensor.shape)}, dtype={tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1eb8c997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Loaded config from: /home/xzhang/models/deepseek-r1-distill-qwen-1.5b-awq-scrooge-4bit-g128/quantized_model/config.json\n",
      "architectures: ['Qwen2ForCausalLM'] (type: <class 'list'>)\n",
      "attention_dropout: 0.0 (type: <class 'float'>)\n",
      "bos_token_id: 151643 (type: <class 'int'>)\n",
      "eos_token_id: 151643 (type: <class 'int'>)\n",
      "hidden_act: silu (type: <class 'str'>)\n",
      "hidden_size: 1536 (type: <class 'int'>)\n",
      "initializer_range: 0.02 (type: <class 'float'>)\n",
      "intermediate_size: 8960 (type: <class 'int'>)\n",
      "max_position_embeddings: 131072 (type: <class 'int'>)\n",
      "max_window_layers: 21 (type: <class 'int'>)\n",
      "model_type: qwen2 (type: <class 'str'>)\n",
      "num_attention_heads: 12 (type: <class 'int'>)\n",
      "num_hidden_layers: 28 (type: <class 'int'>)\n",
      "num_key_value_heads: 2 (type: <class 'int'>)\n",
      "quantization_config.q_group_size: 128 (type: <class 'int'>)\n",
      "quantization_config.version: gemm (type: <class 'str'>)\n",
      "quantization_config.w_bit: 4 (type: <class 'int'>)\n",
      "quantization_config.zero_point: True (type: <class 'bool'>)\n",
      "rms_norm_eps: 1e-06 (type: <class 'float'>)\n",
      "rope_scaling: None (type: <class 'NoneType'>)\n",
      "rope_theta: 10000 (type: <class 'int'>)\n",
      "sliding_window: None (type: <class 'NoneType'>)\n",
      "tie_word_embeddings: False (type: <class 'bool'>)\n",
      "torch_dtype: bfloat16 (type: <class 'str'>)\n",
      "transformers_version: 4.46.0 (type: <class 'str'>)\n",
      "use_cache: False (type: <class 'bool'>)\n",
      "use_mrope: False (type: <class 'bool'>)\n",
      "use_sliding_window: False (type: <class 'bool'>)\n",
      "vocab_size: 151936 (type: <class 'int'>)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def inspect_config_types(config_path: str | Path) -> None:\n",
    "    config_path = Path(config_path).expanduser()\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    def print_types(d, prefix=\"\"):\n",
    "        for k, v in d.items():\n",
    "            full_key = f\"{prefix}.{k}\" if prefix else k\n",
    "            if isinstance(v, dict):\n",
    "                print_types(v, prefix=full_key)\n",
    "            else:\n",
    "                print(f\"{full_key}: {v} (type: {type(v)})\")\n",
    "\n",
    "    print(f\"\\nüìÅ Loaded config from: {config_path}\")\n",
    "    print_types(config)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "inspect_config_types(\n",
    "    \"~/models/deepseek-r1-distill-qwen-1.5b-awq-scrooge-4bit-g128/quantized_model/config.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b81ce",
   "metadata": {},
   "source": [
    "## All Layers for Irregularities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb69d438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"\n",
       "        height: 700px;\n",
       "        overflow: auto;\n",
       "        background-color: black;\n",
       "        color: white;\n",
       "        border: 1px solid #444;\n",
       "        padding: 10px;\n",
       "        font-family: monospace;\n",
       "        white-space: pre;  /* <‚Äî THIS PRESERVES INDENTATION */\n",
       "    \">\n",
       "        üîé Inspecting: /home/xzhang/models/deepseek-r1-distill-qwen-1.5b-awq-scrooge-4bit-g128/quantized_model/model.safetensors (675 tensors)\n",
       "\n",
       "- lm_head.weight: shape=(151936, 1536), dtype=torch.bfloat16 | min=-3.28e-01, max=3.20e-01, mean=4.50e-04\n",
       "- model.embed_tokens.weight: shape=(151936, 1536), dtype=torch.bfloat16 | min=-3.30e-01, max=3.05e-01, mean=3.81e-04\n",
       "- model.layers.0.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-1.15e-01, max=1.07e+00, mean=2.15e-01\n",
       "- model.layers.0.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.05e+09, mean=-1.32e+05\n",
       "- model.layers.0.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.0.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.09e-03, max=6.99e-02, mean=4.00e-02\n",
       "- model.layers.0.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=4.98e+05\n",
       "- model.layers.0.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.0.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.58e-03, max=6.96e-02, mean=4.14e-02\n",
       "- model.layers.0.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.14e+09, max=1.98e+09, mean=-2.70e+03\n",
       "- model.layers.0.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.0.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.53e-02, max=6.26e-02, mean=5.42e-02\n",
       "- model.layers.0.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-8.24e-03, max=4.53e-01, mean=2.63e-01\n",
       "- model.layers.0.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-3.98e+02, max=4.02e+02, mean=7.77e+00\n",
       "- model.layers.0.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.12e+09, mean=-1.71e+06\n",
       "- model.layers.0.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.0.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=3.44e-03, max=1.10e-01, mean=5.58e-02\n",
       "- model.layers.0.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-5.98e-01, max=6.05e-01, mean=3.38e-06\n",
       "- model.layers.0.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.10e+02, max=5.35e+01, mean=5.96e-02\n",
       "- model.layers.0.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=-4.73e+04\n",
       "- model.layers.0.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.0.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=7.19e-03, max=1.00e-01, mean=4.32e-02\n",
       "- model.layers.0.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-8.30e-03, max=8.30e-03, mean=4.34e-05\n",
       "- model.layers.0.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=-4.37e+05\n",
       "- model.layers.0.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.0.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=7.15e-04, max=5.82e-02, mean=2.08e-02\n",
       "- model.layers.1.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-7.70e-01, max=9.26e-01, mean=1.61e-01\n",
       "- model.layers.1.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.07e+09, mean=1.17e+04\n",
       "- model.layers.1.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.1.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.22e-03, max=7.17e-02, mean=4.38e-02\n",
       "- model.layers.1.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.05e+09, mean=1.76e+06\n",
       "- model.layers.1.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.1.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.03e-02, max=7.45e-02, mean=3.49e-02\n",
       "- model.layers.1.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=-1.86e+05\n",
       "- model.layers.1.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.1.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.22e-03, max=4.57e-02, mean=2.32e-02\n",
       "- model.layers.1.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=1.46e-04, max=7.03e+00, mean=7.15e-01\n",
       "- model.layers.1.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-3.19e+01, max=8.10e+01, mean=6.11e-01\n",
       "- model.layers.1.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.14e+09, max=2.12e+09, mean=-6.27e+05\n",
       "- model.layers.1.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.1.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=5.68e-03, max=1.02e-01, mean=4.98e-02\n",
       "- model.layers.1.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-7.46e-01, max=5.39e-01, mean=-4.66e-05\n",
       "- model.layers.1.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.14e+01, max=1.05e+01, mean=4.35e-02\n",
       "- model.layers.1.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.15e+09, mean=-2.49e+05\n",
       "- model.layers.1.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.1.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=2.42e-03, max=7.78e-02, mean=3.07e-02\n",
       "- model.layers.1.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-8.48e-03, max=8.48e-03, mean=-6.93e-05\n",
       "- model.layers.1.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.12e+09, mean=-6.91e+05\n",
       "- model.layers.1.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.1.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=6.14e-04, max=5.43e-02, mean=2.07e-02\n",
       "- model.layers.10.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-6.23e-02, max=1.68e+00, mean=6.04e-01\n",
       "- model.layers.10.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.00e+09, mean=1.98e+04\n",
       "- model.layers.10.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.10.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.34e-03, max=7.22e-02, mean=4.26e-02\n",
       "- model.layers.10.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=1.98e+09, mean=8.32e+05\n",
       "- model.layers.10.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.10.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.77e-02, max=6.49e-02, mean=4.06e-02\n",
       "- model.layers.10.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=-1.95e+05\n",
       "- model.layers.10.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.10.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.52e-03, max=3.92e-02, mean=2.43e-02\n",
       "- model.layers.10.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=3.07e-04, max=1.02e+00, mean=5.90e-01\n",
       "- model.layers.10.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-5.84e+00, max=6.31e+00, mean=7.96e-03\n",
       "- model.layers.10.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=9.12e+05\n",
       "- model.layers.10.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.10.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.76e-03, max=7.64e-02, mean=4.63e-02\n",
       "- model.layers.10.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-1.20e+00, max=1.12e+00, mean=-1.78e-05\n",
       "- model.layers.10.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-4.31e+00, max=1.50e+01, mean=1.01e-01\n",
       "- model.layers.10.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=-1.87e+05\n",
       "- model.layers.10.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.10.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=3.72e-03, max=7.45e-02, mean=4.22e-02\n",
       "- model.layers.10.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-1.60e-02, max=1.60e-02, mean=-1.82e-04\n",
       "- model.layers.10.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=-1.05e+06\n",
       "- model.layers.10.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.10.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=2.19e-03, max=7.46e-02, mean=2.30e-02\n",
       "- model.layers.11.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-9.72e-02, max=1.49e+00, mean=5.88e-01\n",
       "- model.layers.11.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=-3.18e+04\n",
       "- model.layers.11.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.11.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.22e-03, max=7.17e-02, mean=3.97e-02\n",
       "- model.layers.11.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.09e+09, mean=5.59e+05\n",
       "- model.layers.11.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.11.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.01e-02, max=5.74e-02, mean=2.65e-02\n",
       "- model.layers.11.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.07e+09, mean=-2.61e+05\n",
       "- model.layers.11.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.11.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.46e-03, max=3.20e-02, mean=2.19e-02\n",
       "- model.layers.11.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=4.89e-05, max=1.23e+00, mean=5.86e-01\n",
       "- model.layers.11.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-6.34e+00, max=5.19e+00, mean=3.42e-02\n",
       "- model.layers.11.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=-1.68e+05\n",
       "- model.layers.11.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.11.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.72e-03, max=8.39e-02, mean=5.09e-02\n",
       "- model.layers.11.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-8.20e-01, max=1.34e+00, mean=4.89e-05\n",
       "- model.layers.11.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.66e+01, max=5.59e+00, mean=-1.26e-01\n",
       "- model.layers.11.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.15e+09, mean=-6.76e+05\n",
       "- model.layers.11.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.11.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=1.23e-03, max=7.45e-02, mean=2.76e-02\n",
       "- model.layers.11.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-1.28e-02, max=1.28e-02, mean=-1.00e-03\n",
       "- model.layers.11.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.14e+09, max=2.08e+09, mean=-5.17e+05\n",
       "- model.layers.11.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.11.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=4.37e-03, max=7.07e-02, mean=3.80e-02\n",
       "- model.layers.12.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-5.52e-02, max=1.49e+00, mean=6.39e-01\n",
       "- model.layers.12.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=-1.45e+05\n",
       "- model.layers.12.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.12.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.34e-03, max=7.27e-02, mean=4.42e-02\n",
       "- model.layers.12.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.12e+09, mean=4.72e+05\n",
       "- model.layers.12.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.12.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.83e-03, max=3.32e-02, mean=1.89e-02\n",
       "- model.layers.12.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.06e+09, mean=-2.01e+05\n",
       "- model.layers.12.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.12.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.52e-03, max=6.92e-02, mean=2.95e-02\n",
       "- model.layers.12.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-1.08e-04, max=1.45e+00, mean=5.95e-01\n",
       "- model.layers.12.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-3.44e+00, max=4.81e+00, mean=2.12e-02\n",
       "- model.layers.12.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=-9.21e+05\n",
       "- model.layers.12.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.12.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.45e-03, max=7.45e-02, mean=3.69e-02\n",
       "- model.layers.12.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-8.01e-01, max=7.34e-01, mean=2.25e-06\n",
       "- model.layers.12.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.65e+01, max=1.52e+01, mean=-6.98e-02\n",
       "- model.layers.12.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.05e+09, mean=-1.22e+06\n",
       "- model.layers.12.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.12.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=6.56e-03, max=7.40e-02, mean=4.12e-02\n",
       "- model.layers.12.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-1.42e-02, max=1.42e-02, mean=4.26e-05\n",
       "- model.layers.12.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.06e+09, mean=-3.36e+05\n",
       "- model.layers.12.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.12.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=3.17e-03, max=6.65e-02, mean=2.56e-02\n",
       "- model.layers.13.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-6.69e-02, max=1.25e+00, mean=5.65e-01\n",
       "- model.layers.13.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.05e+09, mean=-3.35e+04\n",
       "- model.layers.13.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.13.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.46e-03, max=7.22e-02, mean=3.87e-02\n",
       "- model.layers.13.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.00e+09, mean=7.79e+05\n",
       "- model.layers.13.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.13.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.73e-02, max=4.77e-02, mean=2.68e-02\n",
       "- model.layers.13.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=-3.47e+05\n",
       "- model.layers.13.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.13.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.58e-03, max=4.68e-02, mean=2.15e-02\n",
       "- model.layers.13.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=2.07e-04, max=1.20e+00, mean=5.79e-01\n",
       "- model.layers.13.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-2.75e+00, max=5.69e+00, mean=6.64e-02\n",
       "- model.layers.13.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=-1.65e+06\n",
       "- model.layers.13.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.13.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.82e-03, max=8.25e-02, mean=3.13e-02\n",
       "- model.layers.13.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-8.20e-01, max=7.58e-01, mean=-4.32e-05\n",
       "- model.layers.13.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-2.72e+01, max=1.79e+01, mean=-6.86e-02\n",
       "- model.layers.13.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=-8.27e+05\n",
       "- model.layers.13.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.13.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=2.98e-03, max=7.36e-02, mean=3.93e-02\n",
       "- model.layers.13.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-2.11e-02, max=2.11e-02, mean=8.05e-04\n",
       "- model.layers.13.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-1.90e+09, max=2.14e+09, mean=1.12e+06\n",
       "- model.layers.13.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.13.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=2.65e-03, max=7.22e-02, mean=4.24e-02\n",
       "- model.layers.14.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-4.84e-01, max=5.84e+00, mean=7.28e-01\n",
       "- model.layers.14.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=1.59e+04\n",
       "- model.layers.14.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.14.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.40e-03, max=7.27e-02, mean=4.52e-02\n",
       "- model.layers.14.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=5.68e+05\n",
       "- model.layers.14.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.14.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.52e-03, max=6.88e-02, mean=2.57e-02\n",
       "- model.layers.14.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=-2.47e+05\n",
       "- model.layers.14.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.14.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.58e-03, max=6.92e-02, mean=2.78e-02\n",
       "- model.layers.14.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-8.29e-06, max=1.33e+00, mean=6.26e-01\n",
       "- model.layers.14.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-3.92e+00, max=3.36e+00, mean=-7.36e-02\n",
       "- model.layers.14.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.14e+09, max=2.12e+09, mean=1.28e+05\n",
       "- model.layers.14.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.14.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.88e-03, max=6.82e-02, mean=3.84e-02\n",
       "- model.layers.14.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-1.73e+00, max=1.28e+00, mean=1.16e-05\n",
       "- model.layers.14.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.34e+01, max=1.27e+01, mean=3.46e-02\n",
       "- model.layers.14.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=-4.15e+05\n",
       "- model.layers.14.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.14.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=4.00e-03, max=7.36e-02, mean=3.97e-02\n",
       "- model.layers.14.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-1.71e-02, max=1.71e-02, mean=1.64e-05\n",
       "- model.layers.14.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.09e+09, mean=-1.74e+06\n",
       "- model.layers.14.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.14.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.54e-03, max=6.65e-02, mean=3.35e-02\n",
       "- model.layers.15.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-8.01e-02, max=1.84e+00, mean=6.74e-01\n",
       "- model.layers.15.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.04e+09, mean=-1.56e+05\n",
       "- model.layers.15.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.15.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.40e-03, max=7.31e-02, mean=3.66e-02\n",
       "- model.layers.15.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.09e+09, mean=2.07e+05\n",
       "- model.layers.15.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.15.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.70e-03, max=6.29e-02, mean=2.98e-02\n",
       "- model.layers.15.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.05e+09, mean=-3.73e+05\n",
       "- model.layers.15.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.15.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.64e-03, max=6.96e-02, mean=2.41e-02\n",
       "- model.layers.15.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-1.75e-04, max=1.46e+00, mean=6.12e-01\n",
       "- model.layers.15.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-8.31e+00, max=6.34e+00, mean=-9.92e-03\n",
       "- model.layers.15.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.11e+09, max=2.14e+09, mean=-7.19e+05\n",
       "- model.layers.15.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.15.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=4.78e-03, max=7.31e-02, mean=4.16e-02\n",
       "- model.layers.15.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-1.20e+00, max=1.32e+00, mean=6.75e-06\n",
       "- model.layers.15.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-4.42e+01, max=4.00e+01, mean=-1.16e-01\n",
       "- model.layers.15.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=-1.21e+06\n",
       "- model.layers.15.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.15.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=2.14e-03, max=6.85e-02, mean=2.58e-02\n",
       "- model.layers.15.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-1.48e-02, max=1.48e-02, mean=-1.51e-03\n",
       "- model.layers.15.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.05e+09, mean=-6.63e+05\n",
       "- model.layers.15.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.15.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=8.92e-03, max=6.81e-02, mean=3.05e-02\n",
       "- model.layers.16.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-1.11e-01, max=1.70e+00, mean=7.75e-01\n",
       "- model.layers.16.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.03e+09, mean=-1.39e+05\n",
       "- model.layers.16.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.16.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.40e-03, max=7.27e-02, mean=4.17e-02\n",
       "- model.layers.16.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.05e+09, mean=5.99e+05\n",
       "- model.layers.16.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.16.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.70e-03, max=4.75e-02, mean=2.60e-02\n",
       "- model.layers.16.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.12e+09, mean=-3.66e+05\n",
       "- model.layers.16.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.16.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.64e-03, max=6.21e-02, mean=2.03e-02\n",
       "- model.layers.16.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=2.06e-05, max=1.35e+00, mean=6.26e-01\n",
       "- model.layers.16.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-5.53e+00, max=2.33e+00, mean=-2.28e-02\n",
       "- model.layers.16.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.06e+09, mean=-1.22e+06\n",
       "- model.layers.16.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.16.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.07e-03, max=6.23e-02, mean=2.69e-02\n",
       "- model.layers.16.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-8.83e-01, max=1.05e+00, mean=6.90e-07\n",
       "- model.layers.16.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.48e+01, max=9.56e+00, mean=-4.02e-02\n",
       "- model.layers.16.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=-2.78e+05\n",
       "- model.layers.16.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.16.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=4.06e-03, max=6.70e-02, mean=3.83e-02\n",
       "- model.layers.16.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-1.64e-02, max=1.64e-02, mean=4.12e-04\n",
       "- model.layers.16.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-1.61e+09, max=1.63e+09, mean=4.03e+05\n",
       "- model.layers.16.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.16.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.35e-02, max=7.97e-02, mean=5.03e-02\n",
       "- model.layers.17.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-6.79e-02, max=1.59e+00, mean=7.17e-01\n",
       "- model.layers.17.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.03e+09, mean=-1.56e+05\n",
       "- model.layers.17.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.17.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.34e-03, max=7.22e-02, mean=3.88e-02\n",
       "- model.layers.17.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.09e+09, mean=1.24e+05\n",
       "- model.layers.17.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.17.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.70e-03, max=5.50e-02, mean=2.48e-02\n",
       "- model.layers.17.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.09e+09, mean=-4.30e+05\n",
       "- model.layers.17.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.17.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.70e-03, max=4.04e-02, mean=1.93e-02\n",
       "- model.layers.17.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=6.58e-05, max=1.41e+00, mean=6.30e-01\n",
       "- model.layers.17.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-4.44e+00, max=4.97e+00, mean=6.25e-02\n",
       "- model.layers.17.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.06e+09, mean=-5.02e+05\n",
       "- model.layers.17.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.17.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=3.47e-03, max=6.38e-02, mean=3.41e-02\n",
       "- model.layers.17.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-7.62e-01, max=8.55e-01, mean=5.73e-05\n",
       "- model.layers.17.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.72e+01, max=1.01e+01, mean=-5.22e-02\n",
       "- model.layers.17.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=-6.58e+05\n",
       "- model.layers.17.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.17.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=5.54e-03, max=7.03e-02, mean=4.08e-02\n",
       "- model.layers.17.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-1.37e-02, max=1.37e-02, mean=7.32e-04\n",
       "- model.layers.17.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.14e+09, max=2.08e+09, mean=-1.20e+06\n",
       "- model.layers.17.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.17.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.85e-03, max=6.51e-02, mean=2.94e-02\n",
       "- model.layers.18.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=7.18e-02, max=2.09e+00, mean=6.34e-01\n",
       "- model.layers.18.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.05e+09, mean=-3.25e+05\n",
       "- model.layers.18.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.18.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.40e-03, max=7.27e-02, mean=4.21e-02\n",
       "- model.layers.18.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=-3.42e+04\n",
       "- model.layers.18.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.18.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.64e-03, max=2.48e-02, mean=1.35e-02\n",
       "- model.layers.18.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=-2.79e+05\n",
       "- model.layers.18.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.18.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.77e-03, max=4.06e-02, mean=2.13e-02\n",
       "- model.layers.18.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-2.59e-04, max=1.38e+00, mean=6.25e-01\n",
       "- model.layers.18.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-7.00e+00, max=3.95e+00, mean=-8.65e-02\n",
       "- model.layers.18.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.09e+09, mean=-7.46e+05\n",
       "- model.layers.18.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.18.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.29e-03, max=6.73e-02, mean=2.57e-02\n",
       "- model.layers.18.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-7.42e-01, max=6.99e-01, mean=5.85e-05\n",
       "- model.layers.18.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.78e+01, max=1.23e+01, mean=1.85e-03\n",
       "- model.layers.18.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.12e+09, mean=-2.15e+05\n",
       "- model.layers.18.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.18.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=5.58e-03, max=6.83e-02, mean=3.68e-02\n",
       "- model.layers.18.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-2.60e-02, max=2.60e-02, mean=9.49e-04\n",
       "- model.layers.18.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=1.90e+05\n",
       "- model.layers.18.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.18.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.63e-03, max=8.11e-02, mean=5.28e-02\n",
       "- model.layers.19.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-7.37e-02, max=3.42e+00, mean=6.97e-01\n",
       "- model.layers.19.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.06e+09, mean=-7.05e+04\n",
       "- model.layers.19.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.19.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.70e-03, max=7.54e-02, mean=4.20e-02\n",
       "- model.layers.19.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=2.70e+04\n",
       "- model.layers.19.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.19.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.46e-03, max=3.92e-02, mean=1.82e-02\n",
       "- model.layers.19.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=-5.19e+05\n",
       "- model.layers.19.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.19.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.00e-02, max=7.18e-02, mean=2.24e-02\n",
       "- model.layers.19.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-9.49e-05, max=1.28e+00, mean=6.63e-01\n",
       "- model.layers.19.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-6.72e+00, max=2.59e+00, mean=-4.83e-02\n",
       "- model.layers.19.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.07e+09, mean=-6.55e+05\n",
       "- model.layers.19.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.19.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.10e-03, max=5.47e-02, mean=2.49e-02\n",
       "- model.layers.19.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-9.53e-01, max=9.18e-01, mean=-1.16e-05\n",
       "- model.layers.19.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.75e+01, max=9.31e+00, mean=-1.01e-01\n",
       "- model.layers.19.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.05e+09, mean=-4.80e+05\n",
       "- model.layers.19.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.19.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=5.83e-03, max=6.27e-02, mean=3.19e-02\n",
       "- model.layers.19.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-2.58e-02, max=2.58e-02, mean=-3.75e-04\n",
       "- model.layers.19.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.09e+09, mean=-2.11e+06\n",
       "- model.layers.19.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.19.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.90e-03, max=7.70e-02, mean=3.63e-02\n",
       "- model.layers.2.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-1.07e+00, max=1.20e+00, mean=3.98e-01\n",
       "- model.layers.2.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.07e+09, mean=-1.35e+05\n",
       "- model.layers.2.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.2.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.28e-03, max=7.13e-02, mean=4.26e-02\n",
       "- model.layers.2.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.11e+09, mean=1.49e+06\n",
       "- model.layers.2.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.2.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.06e-02, max=7.67e-02, mean=2.65e-02\n",
       "- model.layers.2.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=-3.79e+05\n",
       "- model.layers.2.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.2.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=8.91e-03, max=6.52e-02, mean=2.81e-02\n",
       "- model.layers.2.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=4.03e-05, max=3.73e+00, mean=6.71e-01\n",
       "- model.layers.2.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-5.03e+00, max=7.69e+00, mean=-3.77e-03\n",
       "- model.layers.2.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-1.87e+09, max=1.90e+09, mean=-8.76e+05\n",
       "- model.layers.2.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.2.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.60e-02, max=9.22e-02, mean=5.67e-02\n",
       "- model.layers.2.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-5.70e-01, max=5.04e-01, mean=-1.74e-05\n",
       "- model.layers.2.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.66e+01, max=1.19e+01, mean=-5.48e-02\n",
       "- model.layers.2.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=-9.95e+05\n",
       "- model.layers.2.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.2.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=2.33e-03, max=6.24e-02, mean=1.87e-02\n",
       "- model.layers.2.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-9.46e-03, max=9.46e-03, mean=4.46e-04\n",
       "- model.layers.2.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=-1.06e+06\n",
       "- model.layers.2.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.2.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.23e-03, max=5.67e-02, mean=2.24e-02\n",
       "- model.layers.20.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-7.62e-02, max=1.21e+00, mean=7.26e-01\n",
       "- model.layers.20.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=-6.06e+04\n",
       "- model.layers.20.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.20.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.70e-03, max=7.45e-02, mean=4.21e-02\n",
       "- model.layers.20.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.12e+09, mean=4.27e+05\n",
       "- model.layers.20.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.20.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.64e-03, max=4.77e-02, mean=1.91e-02\n",
       "- model.layers.20.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.11e+09, mean=-6.22e+05\n",
       "- model.layers.20.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.20.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.95e-03, max=4.11e-02, mean=1.71e-02\n",
       "- model.layers.20.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=3.12e-05, max=1.25e+00, mean=6.91e-01\n",
       "- model.layers.20.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-4.53e+00, max=6.69e+00, mean=-2.61e-04\n",
       "- model.layers.20.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.12e+09, max=2.11e+09, mean=-1.37e+06\n",
       "- model.layers.20.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.20.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.13e-03, max=5.53e-02, mean=2.32e-02\n",
       "- model.layers.20.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-1.28e+00, max=1.04e+00, mean=3.93e-05\n",
       "- model.layers.20.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.06e+01, max=1.22e+01, mean=-4.93e-02\n",
       "- model.layers.20.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.11e+09, mean=1.17e+05\n",
       "- model.layers.20.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.20.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=8.09e-03, max=6.85e-02, mean=4.04e-02\n",
       "- model.layers.20.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-3.42e-02, max=3.42e-02, mean=2.12e-03\n",
       "- model.layers.20.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=-1.59e+06\n",
       "- model.layers.20.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.20.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=6.13e-03, max=9.92e-02, mean=4.62e-02\n",
       "- model.layers.21.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-8.50e-02, max=1.19e+00, mean=6.73e-01\n",
       "- model.layers.21.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=-1.42e+05\n",
       "- model.layers.21.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.21.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.77e-03, max=7.50e-02, mean=3.91e-02\n",
       "- model.layers.21.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=2.56e+05\n",
       "- model.layers.21.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.21.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.70e-03, max=4.02e-02, mean=1.92e-02\n",
       "- model.layers.21.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=-5.90e+05\n",
       "- model.layers.21.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.21.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.95e-03, max=3.34e-02, mean=1.65e-02\n",
       "- model.layers.21.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=1.17e-04, max=1.55e+00, mean=7.82e-01\n",
       "- model.layers.21.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-6.12e+00, max=3.00e+00, mean=-3.00e-02\n",
       "- model.layers.21.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=2.32e+05\n",
       "- model.layers.21.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.21.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.77e-03, max=3.39e-02, mean=1.90e-02\n",
       "- model.layers.21.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-6.52e-01, max=5.35e-01, mean=-4.79e-05\n",
       "- model.layers.21.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-7.41e+00, max=1.53e+01, mean=1.74e-01\n",
       "- model.layers.21.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=2.92e+05\n",
       "- model.layers.21.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.21.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=2.43e-03, max=6.27e-02, mean=3.48e-02\n",
       "- model.layers.21.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-2.28e-02, max=2.28e-02, mean=-7.71e-05\n",
       "- model.layers.21.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=1.98e+09, mean=-6.69e+04\n",
       "- model.layers.21.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.21.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=8.29e-03, max=9.00e-02, mean=5.61e-02\n",
       "- model.layers.22.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-5.23e-01, max=1.39e+00, mean=6.28e-01\n",
       "- model.layers.22.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.03e+09, mean=-7.20e+04\n",
       "- model.layers.22.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.22.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=1.33e-02, max=7.50e-02, mean=4.66e-02\n",
       "- model.layers.22.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.12e+09, mean=1.93e+04\n",
       "- model.layers.22.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.22.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.89e-03, max=4.06e-02, mean=1.89e-02\n",
       "- model.layers.22.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=-6.79e+05\n",
       "- model.layers.22.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.22.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.95e-03, max=3.34e-02, mean=1.78e-02\n",
       "- model.layers.22.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-2.48e-04, max=1.97e+00, mean=8.30e-01\n",
       "- model.layers.22.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-5.94e+00, max=6.56e+00, mean=-6.51e-02\n",
       "- model.layers.22.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=-9.20e+05\n",
       "- model.layers.22.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.22.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=4.17e-03, max=4.46e-02, mean=1.66e-02\n",
       "- model.layers.22.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-7.70e-01, max=6.72e-01, mean=3.86e-05\n",
       "- model.layers.22.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.96e+01, max=1.26e+01, mean=-8.57e-02\n",
       "- model.layers.22.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.15e+09, mean=-9.92e+05\n",
       "- model.layers.22.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.22.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=1.13e-03, max=6.13e-02, mean=2.83e-02\n",
       "- model.layers.22.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-3.25e-02, max=3.25e-02, mean=-1.23e-03\n",
       "- model.layers.22.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.11e+09, mean=-6.06e+05\n",
       "- model.layers.22.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.22.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=2.48e-03, max=9.04e-02, mean=5.72e-02\n",
       "- model.layers.23.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-7.96e-02, max=1.84e+00, mean=6.67e-01\n",
       "- model.layers.23.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.04e+09, mean=-4.37e+04\n",
       "- model.layers.23.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.23.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.83e-03, max=7.18e-02, mean=3.81e-02\n",
       "- model.layers.23.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.09e+09, mean=-2.04e+05\n",
       "- model.layers.23.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.23.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.89e-03, max=2.54e-02, mean=1.44e-02\n",
       "- model.layers.23.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.06e+09, mean=-1.90e+05\n",
       "- model.layers.23.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.23.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.01e-02, max=6.49e-02, mean=3.42e-02\n",
       "- model.layers.23.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=9.58e-05, max=2.25e+00, mean=9.07e-01\n",
       "- model.layers.23.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-3.02e+00, max=5.59e+00, mean=-4.31e-02\n",
       "- model.layers.23.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-1.61e+09, max=1.91e+09, mean=-1.28e+06\n",
       "- model.layers.23.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.23.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.20e-02, max=5.61e-02, mean=3.86e-02\n",
       "- model.layers.23.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-6.13e-01, max=6.48e-01, mean=5.12e-05\n",
       "- model.layers.23.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.37e+01, max=1.19e+01, mean=-8.93e-02\n",
       "- model.layers.23.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=-4.54e+05\n",
       "- model.layers.23.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.23.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=1.48e-03, max=5.60e-02, mean=2.94e-02\n",
       "- model.layers.23.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-3.32e-02, max=3.32e-02, mean=1.99e-03\n",
       "- model.layers.23.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.09e+09, mean=-5.96e+05\n",
       "- model.layers.23.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.23.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=2.72e-03, max=8.75e-02, mean=4.44e-02\n",
       "- model.layers.24.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-4.93e-02, max=1.46e+00, mean=6.77e-01\n",
       "- model.layers.24.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.07e+09, mean=-4.92e+04\n",
       "- model.layers.24.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.24.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.95e-03, max=7.68e-02, mean=4.25e-02\n",
       "- model.layers.24.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=3.28e+05\n",
       "- model.layers.24.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.24.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.83e-03, max=5.57e-02, mean=2.32e-02\n",
       "- model.layers.24.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.09e+09, mean=-1.84e+05\n",
       "- model.layers.24.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.24.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.01e-02, max=4.92e-02, mean=2.46e-02\n",
       "- model.layers.24.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-1.09e-04, max=1.84e+00, mean=9.51e-01\n",
       "- model.layers.24.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-5.41e+00, max=3.92e+00, mean=-3.63e-02\n",
       "- model.layers.24.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.14e+09, max=2.15e+09, mean=-1.19e+06\n",
       "- model.layers.24.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.24.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=6.34e-04, max=6.10e-02, mean=2.57e-02\n",
       "- model.layers.24.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-6.37e-01, max=6.13e-01, mean=4.04e-05\n",
       "- model.layers.24.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.51e+01, max=1.05e+01, mean=-3.01e-02\n",
       "- model.layers.24.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.05e+09, mean=2.49e+05\n",
       "- model.layers.24.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.24.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=1.07e-02, max=6.43e-02, mean=3.51e-02\n",
       "- model.layers.24.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-4.05e-02, max=4.05e-02, mean=-6.65e-04\n",
       "- model.layers.24.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.14e+09, max=2.00e+09, mean=-9.18e+05\n",
       "- model.layers.24.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.24.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=9.22e-03, max=9.78e-02, mean=4.58e-02\n",
       "- model.layers.25.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-6.49e-02, max=2.39e+00, mean=6.30e-01\n",
       "- model.layers.25.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.07e+09, mean=5.08e+02\n",
       "- model.layers.25.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.25.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=1.01e-02, max=7.78e-02, mean=4.29e-02\n",
       "- model.layers.25.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.05e+09, mean=6.24e+05\n",
       "- model.layers.25.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.25.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.70e-03, max=5.50e-02, mean=2.29e-02\n",
       "- model.layers.25.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=-3.19e+05\n",
       "- model.layers.25.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.25.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.02e-02, max=5.01e-02, mean=2.68e-02\n",
       "- model.layers.25.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-1.32e-04, max=1.88e+00, mean=9.92e-01\n",
       "- model.layers.25.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-7.84e+00, max=3.47e+00, mean=5.26e-03\n",
       "- model.layers.25.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=3.46e+04\n",
       "- model.layers.25.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.25.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=5.87e-04, max=4.47e-02, mean=1.66e-02\n",
       "- model.layers.25.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-9.65e-01, max=1.02e+00, mean=3.68e-05\n",
       "- model.layers.25.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.18e+01, max=1.36e+01, mean=-2.22e-02\n",
       "- model.layers.25.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=-2.20e+05\n",
       "- model.layers.25.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.25.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=1.28e-03, max=5.72e-02, mean=2.82e-02\n",
       "- model.layers.25.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-3.78e-02, max=3.78e-02, mean=-1.04e-03\n",
       "- model.layers.25.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.09e+09, mean=5.11e+05\n",
       "- model.layers.25.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.25.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=5.78e-03, max=8.47e-02, mean=5.26e-02\n",
       "- model.layers.26.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-1.06e-01, max=2.73e+00, mean=6.19e-01\n",
       "- model.layers.26.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.05e+09, mean=-9.51e+04\n",
       "- model.layers.26.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.26.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.89e-03, max=7.73e-02, mean=4.33e-02\n",
       "- model.layers.26.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=2.28e+05\n",
       "- model.layers.26.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.26.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.58e-03, max=4.71e-02, mean=2.21e-02\n",
       "- model.layers.26.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=-3.61e+05\n",
       "- model.layers.26.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.26.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.01e-02, max=4.24e-02, mean=1.96e-02\n",
       "- model.layers.26.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-1.82e-03, max=2.22e+00, mean=1.03e+00\n",
       "- model.layers.26.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-2.77e+00, max=4.72e+00, mean=6.78e-02\n",
       "- model.layers.26.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.12e+09, mean=2.45e+05\n",
       "- model.layers.26.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.26.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=4.59e-03, max=4.93e-02, mean=2.30e-02\n",
       "- model.layers.26.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-8.32e-01, max=8.28e-01, mean=-3.54e-06\n",
       "- model.layers.26.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.06e+01, max=1.15e+01, mean=-4.65e-02\n",
       "- model.layers.26.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.15e+09, mean=-1.25e+06\n",
       "- model.layers.26.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.26.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=2.16e-03, max=4.77e-02, mean=1.61e-02\n",
       "- model.layers.26.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-5.00e-02, max=5.00e-02, mean=-3.58e-03\n",
       "- model.layers.26.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-1.07e+09, max=1.09e+09, mean=-2.69e+05\n",
       "- model.layers.26.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.26.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=6.17e-02, max=1.14e-01, mean=8.94e-02\n",
       "- model.layers.27.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-4.13e-02, max=3.52e+00, mean=7.37e-01\n",
       "- model.layers.27.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.00e+09, mean=1.14e+05\n",
       "- model.layers.27.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.27.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.40e-03, max=7.22e-02, mean=4.59e-02\n",
       "- model.layers.27.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.05e+09, mean=1.01e+06\n",
       "- model.layers.27.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.27.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.77e-03, max=4.77e-02, mean=2.39e-02\n",
       "- model.layers.27.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.08e+09, mean=-7.98e+05\n",
       "- model.layers.27.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.27.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.95e-03, max=3.36e-02, mean=1.46e-02\n",
       "- model.layers.27.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-8.58e-05, max=4.84e+00, mean=1.33e+00\n",
       "- model.layers.27.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-2.89e+00, max=6.31e+00, mean=8.60e-02\n",
       "- model.layers.27.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.12e+09, mean=-1.53e+06\n",
       "- model.layers.27.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.27.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=9.14e-04, max=5.17e-02, mean=2.00e-02\n",
       "- model.layers.27.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-1.23e+00, max=1.27e+00, mean=-1.16e-05\n",
       "- model.layers.27.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-8.62e+00, max=8.88e+00, mean=2.61e-02\n",
       "- model.layers.27.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=-1.21e+06\n",
       "- model.layers.27.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.27.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=2.92e-03, max=5.52e-02, mean=2.86e-02\n",
       "- model.layers.27.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-5.00e-02, max=5.00e-02, mean=-1.36e-03\n",
       "- model.layers.27.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=1.98e+09, mean=1.85e+05\n",
       "- model.layers.27.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.27.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.23e-02, max=1.07e-01, mean=6.88e-02\n",
       "- model.layers.3.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-3.69e-01, max=1.20e+00, mean=3.53e-01\n",
       "- model.layers.3.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.00e+09, mean=-5.63e+04\n",
       "- model.layers.3.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.3.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=8.91e-03, max=6.94e-02, mean=3.96e-02\n",
       "- model.layers.3.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.06e+09, mean=1.13e+06\n",
       "- model.layers.3.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.3.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.11e-02, max=7.20e-02, mean=3.50e-02\n",
       "- model.layers.3.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=-3.91e+05\n",
       "- model.layers.3.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.3.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=8.85e-03, max=6.48e-02, mean=2.93e-02\n",
       "- model.layers.3.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=9.73e-05, max=3.02e+00, mean=6.72e-01\n",
       "- model.layers.3.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-6.50e+00, max=5.69e+00, mean=3.91e-02\n",
       "- model.layers.3.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=-8.41e+05\n",
       "- model.layers.3.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.3.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=2.39e-03, max=9.04e-02, mean=4.85e-02\n",
       "- model.layers.3.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-1.03e+00, max=8.83e-01, mean=2.85e-05\n",
       "- model.layers.3.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.00e+01, max=2.05e+01, mean=1.20e-01\n",
       "- model.layers.3.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=-6.56e+03\n",
       "- model.layers.3.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.3.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=1.82e-03, max=6.72e-02, mean=3.88e-02\n",
       "- model.layers.3.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-1.12e-02, max=1.12e-02, mean=-1.78e-03\n",
       "- model.layers.3.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.11e+09, mean=1.07e+05\n",
       "- model.layers.3.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.3.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=2.06e-03, max=5.87e-02, mean=2.86e-02\n",
       "- model.layers.4.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-4.59e-01, max=1.38e+00, mean=4.35e-01\n",
       "- model.layers.4.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.03e+09, mean=-9.06e+04\n",
       "- model.layers.4.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.4.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=8.85e-03, max=6.89e-02, mean=3.95e-02\n",
       "- model.layers.4.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.09e+09, mean=1.24e+05\n",
       "- model.layers.4.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.4.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.08e-02, max=7.00e-02, mean=3.22e-02\n",
       "- model.layers.4.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.11e+09, mean=-2.12e+05\n",
       "- model.layers.4.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.4.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=8.97e-03, max=6.48e-02, mean=3.28e-02\n",
       "- model.layers.4.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=1.65e-05, max=1.23e+00, mean=5.22e-01\n",
       "- model.layers.4.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-3.12e+00, max=5.62e+00, mean=3.99e-02\n",
       "- model.layers.4.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=-1.35e+06\n",
       "- model.layers.4.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.4.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.71e-03, max=8.39e-02, mean=3.62e-02\n",
       "- model.layers.4.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-6.76e-01, max=6.64e-01, mean=-1.16e-05\n",
       "- model.layers.4.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.64e+01, max=1.94e+01, mean=3.15e-02\n",
       "- model.layers.4.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=-8.32e+05\n",
       "- model.layers.4.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.4.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=2.63e-03, max=7.68e-02, mean=3.80e-02\n",
       "- model.layers.4.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-1.90e-02, max=1.90e-02, mean=-5.72e-04\n",
       "- model.layers.4.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.14e+09, max=2.12e+09, mean=-5.95e+05\n",
       "- model.layers.4.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.4.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=5.04e-03, max=7.36e-02, mean=4.59e-02\n",
       "- model.layers.5.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-2.38e-02, max=1.44e+00, mean=4.16e-01\n",
       "- model.layers.5.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=-8.19e+04\n",
       "- model.layers.5.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.5.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.09e-03, max=7.03e-02, mean=4.15e-02\n",
       "- model.layers.5.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=2.33e+06\n",
       "- model.layers.5.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.5.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.06e-02, max=5.95e-02, mean=2.56e-02\n",
       "- model.layers.5.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.09e+09, mean=-2.92e+05\n",
       "- model.layers.5.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.5.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.28e-03, max=6.74e-02, mean=3.09e-02\n",
       "- model.layers.5.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=5.87e-05, max=3.73e+00, mean=9.04e-01\n",
       "- model.layers.5.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-4.75e+00, max=7.53e+00, mean=1.51e-02\n",
       "- model.layers.5.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=-1.34e+06\n",
       "- model.layers.5.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.5.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.38e-03, max=7.67e-02, mean=3.06e-02\n",
       "- model.layers.5.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-8.20e-01, max=6.33e-01, mean=1.76e-05\n",
       "- model.layers.5.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.62e+01, max=1.28e+01, mean=-9.99e-02\n",
       "- model.layers.5.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=1.14e+05\n",
       "- model.layers.5.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.5.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=7.32e-03, max=7.36e-02, mean=3.71e-02\n",
       "- model.layers.5.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-1.97e-02, max=1.97e-02, mean=-1.97e-04\n",
       "- model.layers.5.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=1.64e+09, mean=8.26e+04\n",
       "- model.layers.5.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.5.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=2.15e-02, max=7.78e-02, mean=4.84e-02\n",
       "- model.layers.6.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-1.61e-02, max=1.63e+00, mean=3.98e-01\n",
       "- model.layers.6.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.07e+09, mean=-5.54e+04\n",
       "- model.layers.6.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.6.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.22e-03, max=7.08e-02, mean=3.73e-02\n",
       "- model.layers.6.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=1.98e+09, mean=6.29e+05\n",
       "- model.layers.6.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.6.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.82e-02, max=7.50e-02, mean=4.78e-02\n",
       "- model.layers.6.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=1.93e+09, mean=7.39e+04\n",
       "- model.layers.6.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.6.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.66e-02, max=6.79e-02, mean=4.81e-02\n",
       "- model.layers.6.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-4.17e-05, max=1.13e+00, mean=5.36e-01\n",
       "- model.layers.6.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-3.94e+00, max=2.09e+00, mean=-4.21e-02\n",
       "- model.layers.6.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.07e+09, max=1.91e+09, mean=-8.33e+05\n",
       "- model.layers.6.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.6.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=4.08e-03, max=7.93e-02, mean=5.20e-02\n",
       "- model.layers.6.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-7.46e-01, max=7.54e-01, mean=3.71e-05\n",
       "- model.layers.6.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.17e+01, max=1.31e+01, mean=1.40e-01\n",
       "- model.layers.6.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=-1.08e+06\n",
       "- model.layers.6.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.6.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=4.38e-03, max=5.98e-02, mean=2.91e-02\n",
       "- model.layers.6.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-1.43e-02, max=1.43e-02, mean=7.78e-04\n",
       "- model.layers.6.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=-1.14e+06\n",
       "- model.layers.6.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.6.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.72e-03, max=6.65e-02, mean=3.23e-02\n",
       "- model.layers.7.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-1.95e-02, max=1.42e+00, mean=4.07e-01\n",
       "- model.layers.7.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.04e+09, mean=-1.94e+05\n",
       "- model.layers.7.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.7.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.40e-03, max=7.22e-02, mean=4.18e-02\n",
       "- model.layers.7.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=5.17e+05\n",
       "- model.layers.7.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.7.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.01e-02, max=7.40e-02, mean=3.41e-02\n",
       "- model.layers.7.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-1.88e+09, max=1.91e+09, mean=8.94e+04\n",
       "- model.layers.7.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.7.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=2.42e-02, max=6.88e-02, mean=4.08e-02\n",
       "- model.layers.7.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-1.26e-04, max=1.09e+00, mean=5.46e-01\n",
       "- model.layers.7.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-2.72e+01, max=4.60e+01, mean=2.53e-01\n",
       "- model.layers.7.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.14e+09, max=1.98e+09, mean=-1.31e+06\n",
       "- model.layers.7.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.7.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=4.95e-03, max=8.15e-02, mean=4.04e-02\n",
       "- model.layers.7.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-6.48e-01, max=1.04e+00, mean=2.93e-05\n",
       "- model.layers.7.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.48e+01, max=9.62e+00, mean=-7.08e-02\n",
       "- model.layers.7.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=-1.16e+06\n",
       "- model.layers.7.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.7.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=2.44e-03, max=6.62e-02, mean=3.40e-02\n",
       "- model.layers.7.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-1.18e-02, max=1.18e-02, mean=-4.95e-05\n",
       "- model.layers.7.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.06e+09, mean=-4.26e+04\n",
       "- model.layers.7.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.7.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=3.88e-03, max=5.69e-02, mean=2.29e-02\n",
       "- model.layers.8.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-2.83e-02, max=1.76e+00, mean=5.15e-01\n",
       "- model.layers.8.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.11e+09, mean=1.40e+04\n",
       "- model.layers.8.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.8.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.34e-03, max=7.22e-02, mean=3.89e-02\n",
       "- model.layers.8.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.06e+09, mean=4.14e+05\n",
       "- model.layers.8.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.8.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=1.02e-02, max=7.32e-02, mean=3.91e-02\n",
       "- model.layers.8.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.11e+09, mean=-2.50e+05\n",
       "- model.layers.8.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.8.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.40e-03, max=6.83e-02, mean=3.82e-02\n",
       "- model.layers.8.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-1.91e-04, max=1.05e+00, mean=5.82e-01\n",
       "- model.layers.8.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-5.00e+00, max=1.88e+00, mean=-2.44e-02\n",
       "- model.layers.8.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.14e+09, mean=-2.94e+05\n",
       "- model.layers.8.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.8.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=1.37e-03, max=6.85e-02, mean=3.30e-02\n",
       "- model.layers.8.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-6.95e-01, max=8.36e-01, mean=-3.88e-05\n",
       "- model.layers.8.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.46e+01, max=1.39e+01, mean=6.31e-02\n",
       "- model.layers.8.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.13e+09, mean=-3.05e+05\n",
       "- model.layers.8.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.8.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=4.96e-03, max=7.59e-02, mean=3.62e-02\n",
       "- model.layers.8.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-1.17e-02, max=1.17e-02, mean=-6.76e-04\n",
       "- model.layers.8.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.14e+09, max=2.01e+09, mean=-8.01e+04\n",
       "- model.layers.8.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.8.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=9.30e-03, max=5.35e-02, mean=2.68e-02\n",
       "- model.layers.9.input_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-7.03e-02, max=1.57e+00, mean=5.27e-01\n",
       "- model.layers.9.mlp.down_proj.qweight: shape=(8960, 192), dtype=torch.int32 | min=-2.15e+09, max=2.05e+09, mean=-1.52e+05\n",
       "- model.layers.9.mlp.down_proj.qzeros: shape=(70, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.9.mlp.down_proj.scales: shape=(70, 1536), dtype=torch.float16 | min=9.34e-03, max=7.17e-02, mean=3.85e-02\n",
       "- model.layers.9.mlp.gate_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.10e+09, mean=4.87e+05\n",
       "- model.layers.9.mlp.gate_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.9.mlp.gate_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.95e-03, max=4.90e-02, mean=2.62e-02\n",
       "- model.layers.9.mlp.up_proj.qweight: shape=(1536, 1120), dtype=torch.int32 | min=-2.15e+09, max=2.04e+09, mean=-1.63e+05\n",
       "- model.layers.9.mlp.up_proj.qzeros: shape=(12, 1120), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.9.mlp.up_proj.scales: shape=(12, 8960), dtype=torch.float16 | min=9.52e-03, max=6.96e-02, mean=4.23e-02\n",
       "- model.layers.9.post_attention_layernorm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-3.62e-05, max=9.92e-01, mean=5.87e-01\n",
       "- model.layers.9.self_attn.k_proj.bias: shape=(256,), dtype=torch.float16 | min=-5.19e+00, max=4.84e+00, mean=-2.90e-02\n",
       "- model.layers.9.self_attn.k_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.15e+09, max=2.07e+09, mean=1.23e+06\n",
       "- model.layers.9.self_attn.k_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.9.self_attn.k_proj.scales: shape=(12, 256), dtype=torch.float16 | min=3.28e-03, max=6.73e-02, mean=3.47e-02\n",
       "- model.layers.9.self_attn.o_proj.weight: shape=(1536, 1536), dtype=torch.bfloat16 | min=-1.34e+00, max=1.81e+00, mean=-1.73e-05\n",
       "- model.layers.9.self_attn.q_proj.bias: shape=(1536,), dtype=torch.float16 | min=-1.78e+01, max=2.15e+01, mean=1.01e-02\n",
       "- model.layers.9.self_attn.q_proj.qweight: shape=(1536, 192), dtype=torch.int32 | min=-2.15e+09, max=2.15e+09, mean=-7.26e+05\n",
       "- model.layers.9.self_attn.q_proj.qzeros: shape=(12, 192), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.9.self_attn.q_proj.scales: shape=(12, 1536), dtype=torch.float16 | min=1.60e-03, max=7.92e-02, mean=3.41e-02\n",
       "- model.layers.9.self_attn.v_proj.bias: shape=(256,), dtype=torch.float16 | min=-1.33e-02, max=1.33e-02, mean=-5.86e-04\n",
       "- model.layers.9.self_attn.v_proj.qweight: shape=(1536, 32), dtype=torch.int32 | min=-2.14e+09, max=2.11e+09, mean=-8.78e+05\n",
       "- model.layers.9.self_attn.v_proj.qzeros: shape=(12, 32), dtype=torch.int32 | min=0.00e+00, max=0.00e+00, mean=0.00e+00\n",
       "- model.layers.9.self_attn.v_proj.scales: shape=(12, 256), dtype=torch.float16 | min=2.57e-03, max=7.08e-02, mean=2.49e-02\n",
       "- model.norm.weight: shape=(1536,), dtype=torch.bfloat16 | min=-4.37e-02, max=7.41e+00, mean=2.31e+00\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from safetensors.torch import load_file\n",
    "from IPython.display import display, HTML\n",
    "from pprint import pformat\n",
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "# üß™ Example Usage\n",
    "tensor_file = DEEPSEEK_R1_DISTILL_QUANT_MODEL_DIR / \"model.safetensors\"\n",
    "\n",
    "tensor_info = inspect_tensor_anomalies(tensor_file)\n",
    "show_scrollable(tensor_info, height=\"700px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47498949",
   "metadata": {},
   "source": [
    "## Check o_proj layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f4b5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "\n",
    "tensor_file = Path(\n",
    "    \"~/models/deepseek-r1-distill-qwen-1.5b-awq-scrooge-4bit-g128/quantized_model/model.safetensors\"\n",
    ").expanduser()\n",
    "tensor_dict = load_file(tensor_file)\n",
    "\n",
    "layers = [f\"model.layers.{i}.self_attn.o_proj.weight\" for i in range(28)]\n",
    "\n",
    "for name in layers:\n",
    "    if name not in tensor_dict:\n",
    "        print(f\"‚ùå Missing: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde1608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to your model.safetensors file\n",
    "tensor_file = Path(\n",
    "    \"~/models/deepseek-r1-distill-qwen-1.5b-awq-scrooge-4bit-g128/quantized_model/model.safetensors\"\n",
    ").expanduser()\n",
    "\n",
    "# Load the tensor dictionary\n",
    "tensor_dict = load_file(tensor_file)\n",
    "\n",
    "# Build expected full-precision MLP layer weights\n",
    "expected_fp_weights = [\n",
    "    f\"model.layers.{i}.{proj}.weight\"\n",
    "    for i in range(28)\n",
    "    for proj in [\"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\"]\n",
    "]\n",
    "\n",
    "# Print missing ones\n",
    "missing = [name for name in expected_fp_weights if name not in tensor_dict]\n",
    "print(\n",
    "    \"‚úÖ All expected MLP weights are present.\" if not missing else \"‚ùå Missing weights:\"\n",
    ")\n",
    "for name in missing:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9b21d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top-level keys:\n",
      "- architectures\n",
      "- attention_dropout\n",
      "- bos_token_id\n",
      "- eos_token_id\n",
      "- hidden_act\n",
      "- hidden_size\n",
      "- initializer_range\n",
      "- intermediate_size\n",
      "- max_position_embeddings\n",
      "- max_window_layers\n",
      "- model_type\n",
      "- num_attention_heads\n",
      "- num_hidden_layers\n",
      "- num_key_value_heads\n",
      "- quantization_config\n",
      "- rms_norm_eps\n",
      "- rope_scaling\n",
      "- rope_theta\n",
      "- sliding_window\n",
      "- tie_word_embeddings\n",
      "- torch_dtype\n",
      "- transformers_version\n",
      "- use_cache\n",
      "- use_mrope\n",
      "- use_sliding_window\n",
      "- vocab_size\n",
      "\n",
      "üîç quantization_config block:\n",
      "{'q_group_size': 128, 'version': 'gemm', 'w_bit': 4, 'zero_point': True}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "config_path = Path(\n",
    "    \"/home/xzhang/models/deepseek-r1-distill-qwen-1.5b-awq-scrooge-4bit-g128/quantized_model/config.json\"\n",
    ")\n",
    "with config_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"\\nüîç Top-level keys:\")\n",
    "for k in data:\n",
    "    print(\"-\", k)\n",
    "\n",
    "print(\"\\nüîç quantization_config block:\")\n",
    "print(data.get(\"quantization_config\", \"‚ùå Not found\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc4f5e",
   "metadata": {},
   "source": [
    "# Inspect Clips & Scales.py Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d54b4832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'list'>\n",
      "Content: []\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "tensor_file = DEEPSEEK_R1_DISTILL_QUANT_MODEL_DIR / \"clips.pt\"\n",
    "# tensor_file = DEEPSEEK_R1_DISTILL_QUANT_MODEL_DIR / \"scales.pt\"\n",
    "\n",
    "obj = torch.load(tensor_file)\n",
    "print(\"Type:\", type(obj))\n",
    "print(\"Content:\", obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e43be",
   "metadata": {},
   "source": [
    "# Inspect Hansen Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4757041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1751706f27ef41b081dea0e80a125beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/xzhang/models/casperhansen-deepseek-r1-distill-qwen-1.5b-awq'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "model_name = \"casperhansen/deepseek-r1-distill-qwen-1.5b-awq\"\n",
    "local_dir = Path(\"~/models/casperhansen-deepseek-r1-distill-qwen-1.5b-awq\").expanduser()\n",
    "\n",
    "snapshot_download(repo_id=model_name, local_dir=str(local_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b7df16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece59cc6a65b4b1494f73729c2698be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n    <div style=\"\\n        height: 700px;\\n        overflow: auto;\\n        background-color: bla‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = Path(\"~/models/casperhansen-deepseek-r1-distill-qwen-1.5b-awq\").expanduser()\n",
    "tensor_file = model_dir / \"model.safetensors\"\n",
    "\n",
    "\n",
    "# Display in scrollable box\n",
    "tensor_info = inspect_tensor_anomalies(tensor_file)\n",
    "show_scrollable(tensor_info, height=\"700px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e17d988",
   "metadata": {},
   "source": [
    "## Inspect All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53072e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q weights\n",
      "‚úÖ Tensor 'model.layers.12.mlp.gate_proj.qweight' found in model.safetensors:\n",
      "   shape: (1536, 1120)\n",
      "   dtype: torch.int32\n",
      "   min: -2.1475e+09\n",
      "   max: 2.1475e+09\n",
      "   mean: -5.1006e+07\n",
      "   preview (first 5 per dimension):\n",
      "[[-1533306763, 2087025544, 692627494, -1720351045, -1970759351], [2020056282, -1734774665, -1802487723, 1991667324, -1012423563], [1936371131, -1196127596, 982148987, 1404598931, -697714536], [-1717998971, -1955162004, 1958445400, 1665793879, -1134077114], [1952147082, -711421368, 1384789107, 1985490330, 1517852465]]\n",
      "\n",
      "scales\n",
      "‚úÖ Tensor 'model.layers.12.mlp.gate_proj.scales' found in model.safetensors:\n",
      "   shape: (12, 8960)\n",
      "   dtype: torch.float16\n",
      "   min: 4.3602e-03\n",
      "   max: 9.9487e-02\n",
      "   mean: 1.5744e-02\n",
      "   preview (first 5 per dimension):\n",
      "[[0.0227813720703125, 0.01666259765625, 0.01497650146484375, 0.016632080078125, 0.01477813720703125], [0.0286407470703125, 0.0124969482421875, 0.0119781494140625, 0.0100250244140625, 0.0153656005859375], [0.0159149169921875, 0.01236724853515625, 0.014129638671875, 0.014678955078125, 0.01471710205078125], [0.016082763671875, 0.015655517578125, 0.0162811279296875, 0.016510009765625, 0.01357269287109375], [0.021881103515625, 0.016143798828125, 0.01428985595703125, 0.014190673828125, 0.01328277587890625]]\n",
      "\n",
      "zero points\n",
      "‚úÖ Tensor 'model.layers.12.mlp.gate_proj.qzeros' found in model.safetensors:\n",
      "   shape: (12, 1120)\n",
      "   dtype: torch.int32\n",
      "   min: -2.1228e+09\n",
      "   max: 2.1051e+09\n",
      "   mean: -3.4410e+08\n",
      "   preview (first 5 per dimension):\n",
      "[[-2006419575, -1986558105, 1987610487, -2023196553, 2022213752], [-1467455862, -1970763673, 1988585353, 2005497753, 2005362325], [-2006419575, 1703381142, -1735878774, -1734834296, -2037873034], [-2021087336, -1735948665, 2021156712, -2006485368, 2022148232], [-1970706295, 2006485351, -2002286680, -1988519799, -1733781385]]\n"
     ]
    }
   ],
   "source": [
    "model_dir = Path(\"~/models/casperhansen-deepseek-r1-distill-qwen-1.5b-awq\").expanduser()\n",
    "tensor_file = model_dir / \"model.safetensors\"\n",
    "\n",
    "\n",
    "print(\"q weights\")\n",
    "print(\n",
    "    inspect_tensor_with_preview(\n",
    "        tensor_file, \"model.layers.12.mlp.gate_proj.qweight\", max_preview=5\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nscales\")\n",
    "print(\n",
    "    inspect_tensor_with_preview(\n",
    "        tensor_file, \"model.layers.12.mlp.gate_proj.scales\", max_preview=5\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nzero points\")\n",
    "print(\n",
    "    inspect_tensor_with_preview(\n",
    "        tensor_file, \"model.layers.12.mlp.gate_proj.qzeros\", max_preview=5\n",
    "    )\n",
    ")\n",
    "\n",
    "zero_ratio = (qzeros == 0).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92acf892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q weights\n",
      "‚úÖ Tensor 'model.layers.12.mlp.gate_proj.qweight' found in model.safetensors:\n",
      "   shape: (1536, 1120)\n",
      "   dtype: torch.int32\n",
      "   min: -2.1475e+09\n",
      "   max: 2.1475e+09\n",
      "   mean: -5.1006e+07\n",
      "   preview (first 5 per dimension):\n",
      "[[-1533306763, 2087025544, 692627494, -1720351045, -1970759351], [2020056282, -1734774665, -1802487723, 1991667324, -1012423563], [1936371131, -1196127596, 982148987, 1404598931, -697714536], [-1717998971, -1955162004, 1958445400, 1665793879, -1134077114], [1952147082, -711421368, 1384789107, 1985490330, 1517852465]]\n",
      "\n",
      "scales\n",
      "‚úÖ Tensor 'model.layers.12.mlp.gate_proj.scales' found in model.safetensors:\n",
      "   shape: (12, 8960)\n",
      "   dtype: torch.float16\n",
      "   min: 4.3602e-03\n",
      "   max: 9.9487e-02\n",
      "   mean: 1.5744e-02\n",
      "   preview (first 5 per dimension):\n",
      "[[0.0227813720703125, 0.01666259765625, 0.01497650146484375, 0.016632080078125, 0.01477813720703125], [0.0286407470703125, 0.0124969482421875, 0.0119781494140625, 0.0100250244140625, 0.0153656005859375], [0.0159149169921875, 0.01236724853515625, 0.014129638671875, 0.014678955078125, 0.01471710205078125], [0.016082763671875, 0.015655517578125, 0.0162811279296875, 0.016510009765625, 0.01357269287109375], [0.021881103515625, 0.016143798828125, 0.01428985595703125, 0.014190673828125, 0.01328277587890625]]\n",
      "\n",
      "zero points\n",
      "‚úÖ Tensor 'model.layers.12.mlp.gate_proj.qzeros' found in model.safetensors:\n",
      "   shape: (12, 1120)\n",
      "   dtype: torch.int32\n",
      "   min: -2.1228e+09\n",
      "   max: 2.1051e+09\n",
      "   mean: -3.4410e+08\n",
      "   preview (first 5 per dimension):\n",
      "[[-2006419575, -1986558105, 1987610487, -2023196553, 2022213752], [-1467455862, -1970763673, 1988585353, 2005497753, 2005362325], [-2006419575, 1703381142, -1735878774, -1734834296, -2037873034], [-2021087336, -1735948665, 2021156712, -2006485368, 2022148232], [-1970706295, 2006485351, -2002286680, -1988519799, -1733781385]]\n"
     ]
    }
   ],
   "source": [
    "model_dir = Path(\"~/models/casperhansen-deepseek-r1-distill-qwen-1.5b-awq\").expanduser()\n",
    "tensor_file = model_dir / \"model.safetensors\"\n",
    "\n",
    "layer_name = \"model.layers.12.mlp.gate_proj\"\n",
    "\n",
    "\n",
    "print(\"q weights\")\n",
    "print(\n",
    "    inspect_tensor_with_preview(\n",
    "        tensor_file, \".\".join([layer_name, \"qweight\"]), max_preview=5\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nscales\")\n",
    "print(\n",
    "    inspect_tensor_with_preview(\n",
    "        tensor_file, \".\".join([layer_name, \"scales\"]), max_preview=5\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nzero points\")\n",
    "print(\n",
    "    inspect_tensor_with_preview(\n",
    "        tensor_file, \".\".join([layer_name, \"qzeros\"]), max_preview=5\n",
    "    )\n",
    ")\n",
    "\n",
    "zero_ratio = (qzeros == 0).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c208c",
   "metadata": {},
   "source": [
    "## Check for quant and unquantized layers dim - are they flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e4b30f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tensor 'model.layers.12.mlp.gate_proj.qweight' found in model.safetensors:\n",
      "   shape: (1536, 1120)\n",
      "   dtype: torch.int32\n",
      "   min: -2.1475e+09\n",
      "   max: 2.1475e+09\n",
      "   mean: -5.1006e+07\n",
      "   preview (first 10 per dimension):\n",
      "[[-1533306763, 2087025544, 692627494, -1720351045, -1970759351, 1819972742, -2053719694, -1216207279, -2072332408, 1719171416], [2020056282, -1734774665, -1802487723, 1991667324, -1012423563, 2140497029, -1515945870, 2012382393, 174532521, -1420132987], [1936371131, -1196127596, 982148987, 1404598931, -697714536, -1989843094, -2018878714, 628652229, 1734781256, 1886038597], [-1717998971, -1955162004, 1958445400, 1665793879, -1134077114, 1753253563, -1450790212, 1939228842, 1785255831, -2055510122], [1952147082, -711421368, 1384789107, 1985490330, 1517852465, 429438815, -1431918808, -1751632758, 714958393, -1506712243], [-1705153991, 2009704072, 1662539930, -1791460989, 2069156508, -661223545, 650420589, -1550478267, -1749387351, 960783723], [-1721346629, -1436985448, -1753451177, -1525072543, 1464424072, 1804973222, 1686189899, -1216968823, 1871463895, 596204765], [1720087977, -1684917112, -1469549738, -945515195, 2004367173, -1417895276, 1400227916, 1568829806, 1290430053, 1748527475], [2053065743, 1938598024, -1768731800, 641533298, 1135070329, -611757149, 1925679003, -1048163862, 2022352170, 1518028456], [1969776733, 1458938740, -1451776635, -1272362583, -1567921096, 2058659515, 1743161173, 1235843944, -1453828743, 2016898198]]\n"
     ]
    }
   ],
   "source": [
    "model_dir = Path(\"~/models/casperhansen-deepseek-r1-distill-qwen-1.5b-awq\").expanduser()\n",
    "tensor_file = model_dir / \"model.safetensors\"\n",
    "\n",
    "layer_name = \"model.layers.12.mlp.gate_proj.qweight\"\n",
    "\n",
    "print(inspect_tensor_with_preview(path=tensor_file, tensor_name=layer_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361d5dd",
   "metadata": {},
   "source": [
    "## Inspect Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68931db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "scales\n",
      "‚úÖ Tensor 'model.layers.12.mlp.gate_proj.scales' found in model.safetensors:\n",
      "   shape: (12, 8960)\n",
      "   dtype: torch.float16\n",
      "   min: 4.3602e-03\n",
      "   max: 9.9487e-02\n",
      "   mean: 1.5744e-02\n",
      "   preview (first 20 rows/slices):\n",
      "[0.0227813720703125, 0.01666259765625, 0.01497650146484375, 0.016632080078125, 0.01477813720703125, 0.0119171142578125, 0.0177154541015625, 0.01666259765625, 0.023895263671875, 0.01409149169921875, 0.01763916015625, 0.017578125, 0.0164031982421875, 0.0128936767578125, 0.01380157470703125, 0.02044677734375, 0.0163116455078125, 0.01302337646484375, 0.013214111328125, 0.0212860107421875]\n",
      "[0.0286407470703125, 0.0124969482421875, 0.0119781494140625, 0.0100250244140625, 0.0153656005859375, 0.0159454345703125, 0.0159149169921875, 0.027923583984375, 0.024810791015625, 0.01425933837890625, 0.0173187255859375, 0.0193939208984375, 0.0158233642578125, 0.0300140380859375, 0.0143890380859375, 0.0212249755859375, 0.0177764892578125, 0.0186920166015625, 0.0188751220703125, 0.01947021484375]\n",
      "[0.0159149169921875, 0.01236724853515625, 0.014129638671875, 0.014678955078125, 0.01471710205078125, 0.0164031982421875, 0.01081085205078125, 0.0120086669921875, 0.021484375, 0.0153350830078125, 0.01549530029296875, 0.0148468017578125, 0.0157928466796875, 0.014129638671875, 0.0153350830078125, 0.0173797607421875, 0.017974853515625, 0.0134124755859375, 0.01360321044921875, 0.0148773193359375]\n",
      "[0.016082763671875, 0.015655517578125, 0.0162811279296875, 0.016510009765625, 0.01357269287109375, 0.01800537109375, 0.0193328857421875, 0.017578125, 0.0197906494140625, 0.0139617919921875, 0.01477813720703125, 0.01308441162109375, 0.0159454345703125, 0.00989532470703125, 0.014190673828125, 0.0188446044921875, 0.0163116455078125, 0.01399993896484375, 0.01666259765625, 0.022003173828125]\n",
      "[0.021881103515625, 0.016143798828125, 0.01428985595703125, 0.014190673828125, 0.01328277587890625, 0.015594482421875, 0.015594482421875, 0.01354217529296875, 0.0186920166015625, 0.021087646484375, 0.01708984375, 0.017547607421875, 0.01666259765625, 0.01474761962890625, 0.012237548828125, 0.015625, 0.01497650146484375, 0.01357269287109375, 0.0188140869140625, 0.0276031494140625]\n",
      "[0.01500701904296875, 0.0119781494140625, 0.01220703125, 0.012237548828125, 0.013671875, 0.014129638671875, 0.0166015625, 0.0232391357421875, 0.0261077880859375, 0.01529693603515625, 0.02294921875, 0.01474761962890625, 0.0183563232421875, 0.01751708984375, 0.0139312744140625, 0.021484375, 0.0182342529296875, 0.0159759521484375, 0.0143890380859375, 0.019073486328125]\n",
      "[0.02142333984375, 0.01555633544921875, 0.0114593505859375, 0.013214111328125, 0.0203094482421875, 0.0159149169921875, 0.01285552978515625, 0.024871826171875, 0.02349853515625, 0.01666259765625, 0.0167999267578125, 0.018096923828125, 0.0149383544921875, 0.01087188720703125, 0.01568603515625, 0.02239990234375, 0.0261688232421875, 0.013214111328125, 0.0164642333984375, 0.01861572265625]\n",
      "[0.0202484130859375, 0.01285552978515625, 0.01331329345703125, 0.01207733154296875, 0.01497650146484375, 0.0134735107421875, 0.01526641845703125, 0.01959228515625, 0.0205078125, 0.0144500732421875, 0.0146484375, 0.0182342529296875, 0.01416015625, 0.0133819580078125, 0.01311492919921875, 0.0177154541015625, 0.017974853515625, 0.01403045654296875, 0.0153961181640625, 0.024017333984375]\n",
      "[0.01763916015625, 0.0139312744140625, 0.0134735107421875, 0.0114288330078125, 0.01107025146484375, 0.01471710205078125, 0.0158538818359375, 0.0196685791015625, 0.0192718505859375, 0.01399993896484375, 0.0124969482421875, 0.0163726806640625, 0.0159759521484375, 0.01474761962890625, 0.01207733154296875, 0.0182647705078125, 0.01959228515625, 0.01158905029296875, 0.01500701904296875, 0.01555633544921875]\n",
      "[0.0207366943359375, 0.01546478271484375, 0.0134124755859375, 0.01262664794921875, 0.01654052734375, 0.0159149169921875, 0.0183258056640625, 0.0266265869140625, 0.02142333984375, 0.01666259765625, 0.01480865478515625, 0.01428985595703125, 0.0159759521484375, 0.011932373046875, 0.01351165771484375, 0.023834228515625, 0.022430419921875, 0.011688232421875, 0.0201873779296875, 0.02239990234375]\n",
      "[0.0163421630859375, 0.016021728515625, 0.0164031982421875, 0.0153350830078125, 0.016693115234375, 0.01800537109375, 0.0149383544921875, 0.021575927734375, 0.0153656005859375, 0.01568603515625, 0.0144195556640625, 0.01210784912109375, 0.01457977294921875, 0.01282501220703125, 0.01406097412109375, 0.0159454345703125, 0.0166015625, 0.01529693603515625, 0.0138702392578125, 0.019927978515625]\n",
      "[0.017059326171875, 0.01497650146484375, 0.01262664794921875, 0.017578125, 0.014617919921875, 0.0144500732421875, 0.01282501220703125, 0.0212249755859375, 0.019989013671875, 0.01497650146484375, 0.0149078369140625, 0.019927978515625, 0.0177459716796875, 0.0105438232421875, 0.0169219970703125, 0.0173187255859375, 0.019927978515625, 0.01207733154296875, 0.01503753662109375, 0.0168609619140625]\n"
     ]
    }
   ],
   "source": [
    "model_dir = Path(\"~/models/casperhansen-deepseek-r1-distill-qwen-1.5b-awq\").expanduser()\n",
    "tensor_file = model_dir / \"model.safetensors\"\n",
    "layer_name = \"model.layers.12.mlp.gate_proj.scales\"\n",
    "\n",
    "print(\"\\nscales\")\n",
    "print(inspect_tensor_with_preview_pretty(tensor_file, layer_name, max_preview=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "scales\n",
      "‚úÖ Tensor 'model.layers.0.self_attn.q_proj.scales' found in model.safetensors:\n",
      "   shape: (12, 1536)\n",
      "   dtype: torch.float16\n",
      "   min: 2.2621e-03\n",
      "   max: 1.3391e-01\n",
      "   mean: 1.6508e-02\n",
      "   preview (first 20 rows/slices):\n",
      "[0.007080078125, 0.00823211669921875, 0.00600433349609375, 0.006870269775390625, 0.0078582763671875, 0.00939178466796875, 0.0090484619140625, 0.0058746337890625, 0.01351165771484375, 0.00798797607421875, 0.01123046875, 0.00891876220703125, 0.01230621337890625, 0.00548553466796875, 0.011260986328125, 0.010284423828125, 0.01331329345703125, 0.0077972412109375, 0.0126953125, 0.01549530029296875]\n",
      "[0.00647735595703125, 0.00963592529296875, 0.00698089599609375, 0.00894927978515625, 0.00693511962890625, 0.009246826171875, 0.00969696044921875, 0.0068359375, 0.00885772705078125, 0.01139068603515625, 0.007503509521484375, 0.00989532470703125, 0.0080718994140625, 0.007633209228515625, 0.00894927978515625, 0.009307861328125, 0.009307861328125, 0.00791168212890625, 0.00916290283203125, 0.00820159912109375]\n",
      "[0.00911712646484375, 0.00986480712890625, 0.0085296630859375, 0.008056640625, 0.00788116455078125, 0.01064300537109375, 0.01113128662109375, 0.00634765625, 0.007762908935546875, 0.0080718994140625, 0.00913238525390625, 0.00862884521484375, 0.00861358642578125, 0.005779266357421875, 0.0099639892578125, 0.00989532470703125, 0.010711669921875, 0.00939178466796875, 0.00894927978515625, 0.01032257080078125]\n",
      "[0.0075531005859375, 0.0095367431640625, 0.00696563720703125, 0.01084136962890625, 0.007617950439453125, 0.0088348388671875, 0.009307861328125, 0.006511688232421875, 0.0095672607421875, 0.010009765625, 0.0090179443359375, 0.010711669921875, 0.009307861328125, 0.007045745849609375, 0.0077972412109375, 0.01009368896484375, 0.00914764404296875, 0.0080413818359375, 0.01158905029296875, 0.00769805908203125]\n",
      "[0.006381988525390625, 0.0081024169921875, 0.01137542724609375, 0.009002685546875, 0.007568359375, 0.00836944580078125, 0.0115203857421875, 0.006427764892578125, 0.00963592529296875, 0.01039886474609375, 0.00960540771484375, 0.009796142578125, 0.01155853271484375, 0.007534027099609375, 0.0095672607421875, 0.010284423828125, 0.009796142578125, 0.010711669921875, 0.013153076171875, 0.0107421875]\n",
      "[0.006916046142578125, 0.01018524169921875, 0.0070648193359375, 0.00789642333984375, 0.006397247314453125, 0.00795745849609375, 0.010711669921875, 0.007373809814453125, 0.006900787353515625, 0.00963592529296875, 0.01025390625, 0.0100555419921875, 0.0098114013671875, 0.006656646728515625, 0.010284423828125, 0.01084136962890625, 0.0100555419921875, 0.006755828857421875, 0.013153076171875, 0.0114288330078125]\n",
      "[0.00693511962890625, 0.0073394775390625, 0.0114898681640625, 0.010772705078125, 0.007289886474609375, 0.00917816162109375, 0.010772705078125, 0.007015228271484375, 0.01067352294921875, 0.0110321044921875, 0.01378631591796875, 0.0085601806640625, 0.01015472412109375, 0.007843017578125, 0.01107025146484375, 0.011199951171875, 0.0133819580078125, 0.01447296142578125, 0.0093231201171875, 0.01473236083984375]\n",
      "[0.00672149658203125, 0.009979248046875, 0.007617950439453125, 0.007404327392578125, 0.00914764404296875, 0.0095672607421875, 0.0085296630859375, 0.00698089599609375, 0.01357269287109375, 0.0099639892578125, 0.0105438232421875, 0.01064300537109375, 0.00963592529296875, 0.0085296630859375, 0.00943756103515625, 0.00843048095703125, 0.009796142578125, 0.00872039794921875, 0.01311492919921875, 0.0107421875]\n",
      "[0.00865936279296875, 0.006771087646484375, 0.00830078125, 0.00887298583984375, 0.00624847412109375, 0.01087188720703125, 0.0090484619140625, 0.007030487060546875, 0.007144927978515625, 0.0114593505859375, 0.009735107421875, 0.0078277587890625, 0.007617950439453125, 0.0085906982421875, 0.0080718994140625, 0.01058197021484375, 0.00821685791015625, 0.009307861328125, 0.00830078125, 0.0115203857421875]\n",
      "[0.00823211669921875, 0.01064300537109375, 0.007244110107421875, 0.007568359375, 0.007129669189453125, 0.01081085205078125, 0.00888824462890625, 0.007404327392578125, 0.009307861328125, 0.009490966796875, 0.0099945068359375, 0.0080108642578125, 0.010528564453125, 0.005565643310546875, 0.0095062255859375, 0.0100555419921875, 0.00940704345703125, 0.007373809814453125, 0.0090484619140625, 0.007762908935546875]\n",
      "[0.00821685791015625, 0.007633209228515625, 0.007389068603515625, 0.008056640625, 0.006900787353515625, 0.01064300537109375, 0.013671875, 0.00794219970703125, 0.00797271728515625, 0.00969696044921875, 0.0110321044921875, 0.0100555419921875, 0.00966644287109375, 0.007389068603515625, 0.009246826171875, 0.012725830078125, 0.00959014892578125, 0.007568359375, 0.00914764404296875, 0.0081024169921875]\n",
      "[0.00841522216796875, 0.00870513916015625, 0.0085601806640625, 0.01015472412109375, 0.00646209716796875, 0.01018524169921875, 0.01113128662109375, 0.00844573974609375, 0.01165008544921875, 0.0099945068359375, 0.00989532470703125, 0.00911712646484375, 0.01285552978515625, 0.007602691650390625, 0.011199951171875, 0.00865936279296875, 0.0085906982421875, 0.01285552978515625, 0.01178741455078125, 0.009796142578125]\n"
     ]
    }
   ],
   "source": [
    "model_dir = Path(\"~/models/casperhansen-deepseek-r1-distill-qwen-1.5b-awq\").expanduser()\n",
    "tensor_file = model_dir / \"model.safetensors\"\n",
    "layer_name = \"model.layers.0.self_attn.q_proj.scales\"\n",
    "\n",
    "print(\"\\nscales\")\n",
    "print(inspect_tensor_with_preview_pretty(tensor_file, layer_name, max_preview=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20264855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_mean = [[[0.50], [0.50]], [[0.58], [0.61]], [[0.62], [0.54]], [[0.92], [0.62]]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
