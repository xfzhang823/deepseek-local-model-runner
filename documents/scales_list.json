{
  "scales_list": [
    {
      "prev_op": "input_layernorm",
      "layers": ["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj"],
      "scales": [
        1.0625, 0.8438, 0.9336, 0.9102, 0.8711, 0.8711
      ],
      "dtype": "bfloat16"
    },
    {
      "prev_op": "post_attention_layernorm",
      "layers": ["mlp.gate_proj", "mlp.up_proj"],
      "scales": [
        2.2969, 1.8828, 1.7812, 1.8750, 1.9766, 1.9375
      ],
      "dtype": "bfloat16"
    },
    {
      "prev_op": "mlp.up_proj",
      "layers": ["mlp.down_proj"],
      "scales": [
        0.9219, 0.9297, 0.9648, 0.9570, 0.9570, 0.9141
      ],
      "dtype": "bfloat16"
    }
  ]
}
