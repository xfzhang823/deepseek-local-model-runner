Great ‚Äî given your current project and the new async-first design, here‚Äôs a **step-by-step refactor plan** that transforms your existing sync code into the proposed modular async version without breaking compatibility:

---

## ‚úÖ Phase 1: Scaffold New Async Modules (Non-breaking)

### 1. **Create `llm_inference_async.py`**
- üîÅ Wrap your existing `generate_text` and `generate_text_with_thinking` with `run_in_executor`
- ‚úÖ Place sync logic inside `sync_llm_call()` and `sync_llm_call_with_thinking()`

```python
# llm_inference_async.py
import asyncio
from task_manager import generate_text, generate_text_with_thinking

async def llm_call_async(*args, **kwargs):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, generate_text, *args, **kwargs)

async def llm_call_with_thinking_async(*args, **kwargs):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, generate_text_with_thinking, *args, **kwargs)
```

---

## ‚úÖ Phase 2: Port Task Logic to Async

### 2. **Create `task_manager_async.py`**
- Port functions like `summarize`, `translate`, `extract_keywords`, etc.
- Replace `generate_text(...)` with `await llm_call_async(...)`
- Replace `generate_text_with_thinking(...)` with `await llm_call_with_thinking_async(...)`

```python
from llm_inference_async import llm_call_async, llm_call_with_thinking_async

async def summarize_async(text: str):
    prompt = ...
    output = await llm_call_async(prompt, **sampling_params)
    ...
```

---

## ‚úÖ Phase 3: Replace Router

### 3. **Create `task_router_async.py`**
- Rewire task names to `async def` functions in `task_manager_async.py`

```python
from task_manager_async import summarize_async, translate_async, ...

TASK_DISPATCH = {
    "summarization": summarize_async,
    "translation": translate_async,
    ...
}

async def run_llm_task_async(task_type, tokenizer, model, config, **kwargs):
    handler = TASK_DISPATCH[task_type]
    return await handler(**kwargs)
```

---

## ‚úÖ Phase 4: Add Async API Interface

### 4. **Create `api_async.py`**
- Mimic your current `api.py`, but wrap config + model load + dispatch

```python
from project_config import get_config
from model_loader import ModelLoader  # or dispatch_loader if unified
from task_router_async import run_llm_task_async

async def run_llm_task_with_config_async(task_type: str, task_kwargs: dict, mode: str = "balanced"):
    config = get_config(task_type, mode)
    tokenizer, model = ModelLoader.load_model()
    return await run_llm_task_async(task_type, tokenizer, model, config, **task_kwargs)
```

---

## ‚úÖ Phase 5: Refactor Batch Logic

### 5. **Create `batch_manager_async.py`**
- Use `asyncio.gather(...)`
- Replace `executor.submit(...)` with a list of `await run_llm_task_with_config_async(...)`

```python
from api_async import run_llm_task_with_config_async

async def run_batch_tasks_async(batch_requests: List[dict]):
    tasks = [
        run_llm_task_with_config_async(req["task_type"], req["kwargs"])
        for req in batch_requests
    ]
    return await asyncio.gather(*tasks)
```

---

## ‚úÖ Phase 6: Test Runners

### 6. **Create `main_async.py` and `main_test_async.py`**
- Use `async def main()` with `asyncio.run(...)`

```python
# main_async.py
import asyncio
from api_async import run_llm_task_with_config_async

async def main():
    result = await run_llm_task_with_config_async("summarization", {"text": "..."})
    print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

---

## üßπ Phase 7: Cleanup & Backward Compatibility

- ‚úÖ Keep sync entrypoints (e.g., `main.py`, `api.py`, `task_manager.py`) intact
- ‚úÖ Add warnings: ‚ÄúDeprecated, use `*_async.py` instead‚Äù
- ‚úÖ Replace shared logic with utility imports if needed
- ‚úÖ Move `generate_text` to a private `_sync_inference.py` if you want to isolate it cleanly

---

## üèÅ Bonus (After Working State)

- Unify loaders under `model_loader/dispatch_loader.py` (already done)
- Add retry + logging middleware for `llm_call_async`
- Add semaphore to batch manager to avoid exhausting GPU

---

