### âœ… **Implementation Steps for Scrooge AWQ Quantizer Refactor**

### **1. Initialization Phase â€” `__init__()`**

* **Changes:**

  * Add new attributes to track paths for each quantized layer and calibration stats.
  * Add a new parameter `save_per_layer` to toggle the layer-by-layer saving mechanism.

**Implementation:**

* Add the following attributes in the `__init__` method:

```python
self.save_per_layer = save_per_layer
self.layer_paths = []
self.calib_stats_paths = []
self.quantized_layer_dir = os.path.join(self.save_dir, "quantized_layers")
os.makedirs(self.quantized_layer_dir, exist_ok=True)
```

---

### **2. Calibration and Quantization Phase â€” `calibrate()`**

* **Changes:**

  * Integrate the quantization step directly after calibration for each layer.
  * Save each quantized layer to disk before moving to the next layer.

**Implementation:**

* Update the existing `calibrate()` method as follows:

```python
def calibrate(
    self,
    save_path: Optional[str] = None,
    clear_inps: bool = True,
    save_per_layer: bool = False,
):
    """
    Calibrate and quantize the model layer-by-layer, saving each quantized layer immediately.
    """

    start_time = time.time()
    logger.info("ðŸš€ Starting calibration and quantization...")

    # Initialization
    device = get_best_device()
    self.model = self.model.to(device)
    self.model.model.embed_tokens = self.model.model.embed_tokens.to(device)

    # Generate calibration inputs
    try:
        self.modules, self.module_kwargs, self.inps = self.init_quant(
            n_samples=self.max_calib_samples,
            max_seq_len=self.max_calib_seq_len,
        )
    except Exception as e:
        logger.error(f"Failed to initialize calibration: {e}")
        raise

    # Layer-by-layer processing
    for i, module in enumerate(self.modules):
        layer_name = f"layer_{i}"
        logger.info(f"Processing {layer_name}...")

        # Move layer to GPU
        module = module.to(device)

        # Collect input activations
        named_linears = get_named_linears(module)
        input_feat = self._get_input_feat(module, named_linears)

        # Apply calibration and quantization
        module_scales, module_clips = self._calibrate_and_quantize(
            module, input_feat, layer_name
        )

        # Save layer and calibration stats
        if save_per_layer:
            layer_path = os.path.join(self.quantized_layer_dir, f"{layer_name}.pt")
            torch.save(module.state_dict(), layer_path)
            self.layer_paths.append(layer_path)

            calib_path = os.path.join(self.quantized_layer_dir, f"{layer_name}_calib.pt")
            torch.save({"scales": module_scales, "clips": module_clips}, calib_path)
            self.calib_stats_paths.append(calib_path)

        # Clear memory
        self.modules[i] = module.cpu()
        clear_memory()

    # Save aggregated stats if `save_path` is provided
    if save_path:
        torch.save(
            {"scales": self.all_scales, "clips": self.all_clips},
            save_path
        )
        logger.info(f"Calibration stats saved to {save_path}")

    elapsed = time.time() - start_time
    logger.info(f"Calibration and quantization completed in {elapsed:.2f} seconds.")
```

---

### **3. New Method â€” `_calibrate_and_quantize()`**

* **Purpose:** Combine calibration and quantization for each layer.

**Implementation:**

```python
def _calibrate_and_quantize(
    self, module: nn.Module, input_feat: Dict[str, torch.Tensor], layer_name: str
) -> Tuple[List[Tuple[str, torch.Tensor]], List[Tuple[str, torch.Tensor]]]:
    """
    Calibrate and quantize a single layer, returning scales and clips.

    Args:
        module (nn.Module): Transformer layer.
        input_feat (Dict[str, torch.Tensor]): Captured input activations.
        layer_name (str): Layer identifier for logging and saving.

    Returns:
        Tuple: (scales, clips) - Lists of calibration data.
    """

    logger.info(f"Calibrating and quantizing {layer_name}...")

    # Collect scaling data
    module_scales = []
    module_clips = []

    # Get scaling configuration for each layer
    module_config = self.get_layers_for_scaling(module, input_feat, self.module_kwargs)

    # Iterate over each group within the layer
    for group in module_config:
        scale_data = self._search_best_scale(module, **group)

        if scale_data:
            module_scales.append(scale_data)

        if self.apply_clip:
            clip_data = self._search_best_clip(module, group['layers'], group['inp'])
            module_clips.extend(clip_data)

    # Flatten and store calibration data
    flat_scales = flatten_scales_or_clip_list(module_scales)
    flat_clips = flatten_scales_or_clip_list(module_clips)

    # Update global scales and clips
    self.all_scales.extend(flat_scales)
    self.all_clips.extend(flat_clips)

    return flat_scales, flat_clips
```

---

### **4. New Method â€” `save_layer()`**

* **Purpose:** Save quantized layers individually after each pass.

**Implementation:**

```python
def save_layer(self, module: nn.Module, layer_name: str) -> None:
    """
    Save a quantized layer to disk.

    Args:
        module (nn.Module): Quantized module to save.
        layer_name (str): Layer identifier for the filename.
    """
    layer_path = os.path.join(self.quantized_layer_dir, f"{layer_name}.pt")
    torch.save(module.state_dict(), layer_path)
    self.layer_paths.append(layer_path)

    logger.info(f"Layer {layer_name} saved to {layer_path}")
```

---

### **5. New Method â€” `save_aggregate_configs()`**

* **Purpose:** Save all calibration stats, config files, and quantized weights after the entire calibration/quantization process.

**Implementation:**

```python
def save_aggregate_configs(self, save_dir: str) -> None:
    """
    Save aggregated calibration stats, quantized weights, and config files.
    """
    os.makedirs(save_dir, exist_ok=True)

    # Save calibration stats
    calib_path = os.path.join(save_dir, "calib_stats.pt")
    torch.save({"scales": self.all_scales, "clips": self.all_clips}, calib_path)
    logger.info(f"Calibration stats saved to {calib_path}")

    # Save quantized model
    model_path = os.path.join(save_dir, "quantized_model.pt")
    torch.save(self.model.state_dict(), model_path)
    logger.info(f"Quantized model saved to {model_path}")

    # Save layer paths for reassembly
    layer_paths_file = os.path.join(save_dir, "layer_paths.json")
    with open(layer_paths_file, "w") as f:
        json.dump(self.layer_paths, f)

    logger.info(f"Layer paths saved to {layer_paths_file}")
```

---

### **6. Memory Optimization and Logging Enhancements:**

* Add GPU memory logging at the start and end of each calibration/quantization step.
* Include more granular logging for each layer, showing:

  * Time taken for calibration.
  * Time taken for quantization.
  * Memory usage before and after each layer is processed.

---

### âœ… **Summary of New Structure:**

* **Initialization:**

  * Prepare directory structure for layer-by-layer saving.
  * Initialize attributes to track layer paths and calibration paths.

* **Calibration and Quantization:**

  * Integrate quantization directly into calibration.
  * Apply calibration and quantization in one pass per layer.
  * Save each layer immediately after quantization to minimize GPU memory usage.

* **Saving and Aggregation:**

  * Save each layer individually during calibration.
  * Save final model and config files at the end of the process.

---

Would you like me to proceed with implementing this structure or adjust anything before proceeding?
