{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2065fa2e",
   "metadata": {},
   "source": [
    "# Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ceec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def show_scrollable(content, height=\"300px\"):\n",
    "    \"\"\"\n",
    "    Display content in a scrollable box in Jupyter Notebook.\n",
    "\n",
    "    Args:\n",
    "        content: Content to display (string, list, dict, or any printable object)\n",
    "        height: Height of the scrollable box (e.g., '300px', '500px', '50%')\n",
    "\n",
    "    Examples:\n",
    "        >>> # Display a long list\n",
    "        >>> show_scrollable([f\"Item {i}\" for i in range(100)])\n",
    "\n",
    "        >>> # Display a dictionary with custom height\n",
    "        >>> big_dict = {i: f\"Value {i}\" for i in range(50)}\n",
    "        >>> show_scrollable(big_dict, height='400px')\n",
    "\n",
    "        >>> # Display string output\n",
    "        >>> show_scrollable(\"Lorem ipsum...\\\\n\" * 50)\n",
    "    \"\"\"\n",
    "    out = widgets.Output(\n",
    "        layout={\n",
    "            \"height\": height,\n",
    "            \"overflow\": \"auto\",\n",
    "            \"border\": \"1px solid #ddd\",\n",
    "            \"padding\": \"5px\",\n",
    "        }\n",
    "    )\n",
    "    display(out)\n",
    "\n",
    "    with out:\n",
    "        if isinstance(content, (list, dict)):\n",
    "            pprint(content)\n",
    "        else:\n",
    "            print(content) if content is not None else print(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cc70e6",
   "metadata": {},
   "source": [
    "# Inspecting DS R1 Qwen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f31124c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Config {\n",
      "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(config)  # Full architecture details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae52d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "Qwen2ForCausalLM                                   [1, 2, 128, 128]          --\n",
       "├─Qwen2Model: 1-1                                  [1, 2, 128, 128]          --\n",
       "│    └─Embedding: 2-1                              [1, 128, 1536]            233,373,696\n",
       "│    └─Qwen2RotaryEmbedding: 2-2                   [1, 128, 128]             --\n",
       "│    └─ModuleList: 2-3                             --                        --\n",
       "│    │    └─Qwen2DecoderLayer: 3-1                 [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-2                 [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-3                 [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-4                 [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-5                 [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-6                 [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-7                 [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-8                 [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-9                 [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-10                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-11                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-12                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-13                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-14                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-15                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-16                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-17                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-18                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-19                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-20                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-21                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-22                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-23                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-24                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-25                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-26                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-27                [1, 128, 1536]            46,797,824\n",
       "│    │    └─Qwen2DecoderLayer: 3-28                [1, 128, 1536]            46,797,824\n",
       "│    └─Qwen2RMSNorm: 2-4                           [1, 128, 1536]            1,536\n",
       "├─Linear: 1-2                                      [1, 128, 151936]          233,373,696\n",
       "====================================================================================================\n",
       "Total params: 1,777,088,000\n",
       "Trainable params: 1,777,088,000\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.78\n",
       "====================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 907.41\n",
       "Params size (MB): 7108.35\n",
       "Estimated Total Size (MB): 8015.76\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "# Load models\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "summary(model, input_size=(1, 128), dtypes=[torch.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    # if \"layers\" in name and \"proj\" in name:\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d538ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen1.5-1.8B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "original_config = AutoConfig.from_pretrained(\"Qwen/Qwen1.5-1.8B\")\n",
    "print(original_config)  # Compare hidden_size, layers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9d445",
   "metadata": {},
   "source": [
    "## Inspect Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f18e3646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Layer: model.layers.0.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0300,  0.0226,  0.0251],\n",
      "        [-0.0177, -0.0050,  0.0713],\n",
      "        [-0.0033, -0.0170,  0.0043]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.0.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0645,  0.0148, -0.1377],\n",
      "        [ 0.0254, -0.0625,  0.0957],\n",
      "        [ 0.0068, -0.0386, -0.0035]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.0.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0123, -0.0309,  0.0085],\n",
      "        [ 0.0025, -0.0030,  0.0259],\n",
      "        [ 0.0214,  0.0449, -0.0035]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.0.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0015,  0.0013, -0.0114],\n",
      "        [-0.0214,  0.0020,  0.0044],\n",
      "        [ 0.0430,  0.0229, -0.0354]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.0.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0266,  0.0369, -0.0342],\n",
      "        [-0.0210, -0.0135, -0.0405],\n",
      "        [ 0.0566, -0.0588,  0.0315]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.0.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0603, -0.0179,  0.0378],\n",
      "        [ 0.0104,  0.0085,  0.0270],\n",
      "        [-0.0035,  0.0195,  0.0361]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.0.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0064, -0.0107,  0.0212],\n",
      "        [ 0.0330, -0.0011,  0.0017],\n",
      "        [-0.0187,  0.0300,  0.0574]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.0.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.1.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0005,  0.0057, -0.0159],\n",
      "        [-0.0295,  0.0113, -0.0129],\n",
      "        [ 0.0532, -0.0128,  0.0243]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.1.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0084, -0.0461,  0.0123],\n",
      "        [ 0.0371,  0.0181,  0.0071],\n",
      "        [-0.0149,  0.0830,  0.0215]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.1.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0232, -0.0292, -0.0019],\n",
      "        [-0.0031,  0.0065, -0.0422],\n",
      "        [-0.0444, -0.0132, -0.0148]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.1.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0146,  0.0160,  0.0330],\n",
      "        [-0.0148, -0.0791,  0.0255],\n",
      "        [ 0.0060,  0.0364,  0.0513]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.1.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0162,  0.0233, -0.0415],\n",
      "        [ 0.0015,  0.0139, -0.0022],\n",
      "        [ 0.0991,  0.0510, -0.0162]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.1.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0283, -0.0063,  0.0708],\n",
      "        [ 0.0063,  0.0037,  0.0126],\n",
      "        [-0.0437, -0.0128, -0.0049]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.1.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0063,  0.0026, -0.0933],\n",
      "        [-0.0332, -0.0021, -0.0320],\n",
      "        [ 0.0269,  0.0066, -0.0050]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.1.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.2.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0072,  0.0032,  0.0640],\n",
      "        [-0.0139, -0.0393, -0.0135],\n",
      "        [-0.0107, -0.0649, -0.0908]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.2.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 1.7944e-02,  5.6458e-03,  3.2471e-02],\n",
      "        [-1.4954e-03,  3.5889e-02,  4.7302e-03],\n",
      "        [ 1.7212e-02,  3.5400e-02, -6.7711e-05]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.2.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0143, -0.0591, -0.0874],\n",
      "        [-0.0023,  0.0114,  0.0099],\n",
      "        [-0.0055, -0.0214, -0.0037]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.2.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0022, -0.0664, -0.0364],\n",
      "        [ 0.0201, -0.0128,  0.0381],\n",
      "        [-0.0036,  0.0569, -0.0403]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.2.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0278,  0.0457, -0.0388],\n",
      "        [ 0.0430, -0.0204,  0.0117],\n",
      "        [ 0.0540, -0.0310, -0.0576]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.2.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0554,  0.0233,  0.0515],\n",
      "        [ 0.0025, -0.0588, -0.0334],\n",
      "        [-0.0991,  0.0393, -0.0264]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.2.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0211,  0.0188,  0.0552],\n",
      "        [ 0.0250, -0.0569, -0.0177],\n",
      "        [ 0.0127,  0.0097,  0.0430]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.2.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.3.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0190, -0.0003,  0.0381],\n",
      "        [ 0.0022,  0.0099,  0.0649],\n",
      "        [-0.0041,  0.0549,  0.0654]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.3.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0247, -0.0074, -0.0118],\n",
      "        [ 0.0037,  0.0227,  0.0172],\n",
      "        [-0.0236, -0.0096, -0.0322]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.3.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0413,  0.0388, -0.0229],\n",
      "        [ 0.0742, -0.0649, -0.0265],\n",
      "        [ 0.0649,  0.0645,  0.0138]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.3.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0106, -0.0126,  0.0265],\n",
      "        [ 0.0236, -0.0261, -0.0649],\n",
      "        [ 0.0026,  0.0356, -0.0459]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.3.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0508,  0.0459, -0.0532],\n",
      "        [-0.0018, -0.0126,  0.1011],\n",
      "        [ 0.0262,  0.0184,  0.0522]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.3.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0437, -0.0442,  0.0100],\n",
      "        [ 0.0466,  0.0469,  0.0493],\n",
      "        [ 0.0840, -0.0459, -0.0388]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.3.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-3.7842e-02,  5.2979e-02, -5.4932e-02],\n",
      "        [-1.8188e-02, -3.2959e-02, -2.4292e-02],\n",
      "        [-5.3467e-02, -6.4453e-02,  2.7061e-05]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.3.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.4.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0369, -0.0036,  0.0339],\n",
      "        [-0.0212,  0.0226, -0.0137],\n",
      "        [ 0.0135, -0.0175, -0.0454]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.4.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0205, -0.0278,  0.0129],\n",
      "        [ 0.0339,  0.0194,  0.0160],\n",
      "        [ 0.0041,  0.0107,  0.0298]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.4.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0171, -0.0364,  0.0073],\n",
      "        [ 0.0027,  0.1328, -0.0376],\n",
      "        [ 0.0144,  0.0544, -0.0168]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.4.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0320, -0.0134,  0.0160],\n",
      "        [-0.0613,  0.0522, -0.0110],\n",
      "        [-0.0522,  0.1245, -0.0410]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.4.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0879, -0.0022, -0.0498],\n",
      "        [ 0.0056,  0.0518, -0.0234],\n",
      "        [-0.0579, -0.0325, -0.0354]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.4.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0234,  0.0615,  0.0189],\n",
      "        [ 0.0116, -0.0275,  0.0236],\n",
      "        [-0.0071,  0.0062, -0.0104]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.4.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0192,  0.0020,  0.0322],\n",
      "        [-0.0079,  0.0302,  0.0410],\n",
      "        [ 0.0009,  0.0231, -0.0054]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.4.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.5.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-7.1049e-05,  5.8289e-03,  2.5513e-02],\n",
      "        [-6.1340e-03, -3.6865e-02, -1.1658e-02],\n",
      "        [-3.1738e-03,  1.5259e-03,  3.8818e-02]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.5.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0040, -0.0233,  0.0422],\n",
      "        [ 0.0018,  0.0019,  0.0042],\n",
      "        [-0.0032, -0.0476,  0.0178]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.5.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0115,  0.0864, -0.0212],\n",
      "        [ 0.0084,  0.0164,  0.0630],\n",
      "        [-0.0151, -0.0542, -0.0432]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.5.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0334,  0.0038,  0.0211],\n",
      "        [-0.0160,  0.0364, -0.0371],\n",
      "        [-0.0027,  0.0981,  0.0840]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.5.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0327, -0.0123, -0.0688],\n",
      "        [-0.0403, -0.0476, -0.0654],\n",
      "        [ 0.0430,  0.0198, -0.0400]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.5.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0258, -0.0094, -0.0562],\n",
      "        [ 0.0518,  0.0347, -0.0245],\n",
      "        [ 0.0051, -0.0124,  0.0288]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.5.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0074, -0.0123,  0.0767],\n",
      "        [-0.0256,  0.0095, -0.0305],\n",
      "        [-0.0320,  0.0791,  0.0383]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.5.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.6.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0047, -0.0166,  0.0226],\n",
      "        [-0.0051,  0.0674,  0.0366],\n",
      "        [ 0.0149,  0.0068, -0.0208]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.6.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0109,  0.0035, -0.0364],\n",
      "        [ 0.0124, -0.0226,  0.0442],\n",
      "        [ 0.0110, -0.0110,  0.0081]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.6.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0120,  0.1226,  0.0571],\n",
      "        [ 0.0077, -0.0664,  0.0168],\n",
      "        [-0.0095,  0.0026,  0.0260]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.6.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0031, -0.0262,  0.0452],\n",
      "        [-0.0020,  0.0278, -0.0859],\n",
      "        [-0.0330, -0.0383,  0.0913]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.6.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0942,  0.0130,  0.0845],\n",
      "        [-0.0101, -0.0066,  0.0057],\n",
      "        [-0.0332, -0.0245, -0.0400]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.6.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0728, -0.0515,  0.0510],\n",
      "        [-0.0148, -0.0055,  0.0283],\n",
      "        [-0.0052, -0.0192,  0.0091]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.6.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0223,  0.0011, -0.0106],\n",
      "        [-0.0488, -0.0291,  0.0018],\n",
      "        [ 0.0608, -0.0349, -0.0376]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.6.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.7.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0054, -0.0256, -0.0029],\n",
      "        [ 0.0138, -0.0547,  0.0171],\n",
      "        [ 0.0074,  0.0154, -0.0109]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.7.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0256,  0.0243, -0.0089],\n",
      "        [ 0.0505,  0.0038, -0.0037],\n",
      "        [-0.0068, -0.0057, -0.0278]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.7.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0479,  0.0850,  0.0139],\n",
      "        [ 0.0732, -0.0034,  0.0122],\n",
      "        [ 0.0034, -0.0366,  0.0352]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.7.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0801, -0.0461, -0.0430],\n",
      "        [-0.1104,  0.0413,  0.0669],\n",
      "        [-0.0330,  0.0281, -0.0630]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.7.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0302, -0.0293,  0.0240],\n",
      "        [ 0.0123,  0.0334, -0.0559],\n",
      "        [-0.0280, -0.0659, -0.0142]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.7.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0099, -0.0023,  0.0067],\n",
      "        [ 0.0464, -0.0400,  0.0034],\n",
      "        [ 0.0007,  0.0099,  0.0020]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.7.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0148, -0.0344, -0.0103],\n",
      "        [ 0.0107, -0.0728,  0.0277],\n",
      "        [ 0.0058,  0.0029,  0.0254]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.7.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.8.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0640,  0.0820,  0.0747],\n",
      "        [-0.0408, -0.0047, -0.1084],\n",
      "        [ 0.0596, -0.0737, -0.0254]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.8.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0161, -0.0239,  0.0040],\n",
      "        [-0.0129, -0.0211, -0.0098],\n",
      "        [-0.0110,  0.0030,  0.0035]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.8.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0015, -0.0505,  0.0061],\n",
      "        [-0.0003,  0.0240, -0.0835],\n",
      "        [-0.0002,  0.0225, -0.0120]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.8.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0232, -0.0179,  0.0315],\n",
      "        [ 0.0131,  0.0042,  0.0776],\n",
      "        [-0.1118, -0.0140, -0.0464]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.8.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0564,  0.0227,  0.0625],\n",
      "        [-0.0388,  0.0206, -0.0320],\n",
      "        [-0.0036,  0.0184,  0.0014]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.8.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0371, -0.0835,  0.0605],\n",
      "        [ 0.0146,  0.0359, -0.0015],\n",
      "        [-0.0437, -0.0031,  0.0356]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.8.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0635, -0.0479,  0.0201],\n",
      "        [-0.0284,  0.0630, -0.0510],\n",
      "        [ 0.0130, -0.0825,  0.0381]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.8.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.9.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0039,  0.0864, -0.0078],\n",
      "        [-0.0280,  0.0547, -0.0104],\n",
      "        [-0.0031, -0.0059,  0.0515]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.9.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0248, -0.0210, -0.0045],\n",
      "        [-0.0344, -0.0061,  0.0028],\n",
      "        [ 0.0161, -0.0229,  0.0107]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.9.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-2.2583e-02, -5.3711e-02,  1.3428e-02],\n",
      "        [ 6.4697e-03, -4.6387e-03,  1.7762e-05],\n",
      "        [-6.4087e-03,  5.2002e-02, -1.7090e-02]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.9.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0304, -0.0381,  0.0226],\n",
      "        [ 0.0933,  0.0483, -0.0625],\n",
      "        [-0.0344,  0.0217, -0.0025]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.9.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0057, -0.0112,  0.0228],\n",
      "        [-0.0065,  0.0123, -0.0356],\n",
      "        [ 0.0294,  0.0261,  0.0161]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.9.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0291, -0.0537,  0.0425],\n",
      "        [ 0.0029,  0.0476, -0.0250],\n",
      "        [ 0.0059, -0.0347,  0.0025]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.9.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0221,  0.0181, -0.0449],\n",
      "        [-0.0008,  0.0199, -0.0294],\n",
      "        [-0.0359, -0.0137, -0.1021]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.9.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.10.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0049, -0.0034, -0.0193],\n",
      "        [ 0.0221, -0.0059, -0.0106],\n",
      "        [-0.0063,  0.0330, -0.0184]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.10.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0264, -0.0040,  0.0071],\n",
      "        [ 0.0074, -0.0123, -0.0117],\n",
      "        [-0.0150, -0.0149,  0.0153]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.10.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0087,  0.0840,  0.0271],\n",
      "        [ 0.0469, -0.0025, -0.0164],\n",
      "        [-0.0058, -0.0254, -0.0142]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.10.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0315, -0.0023,  0.0011],\n",
      "        [-0.0422,  0.0038, -0.0033],\n",
      "        [-0.0583, -0.0103, -0.0581]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.10.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0349,  0.0635,  0.0153],\n",
      "        [ 0.0547, -0.0554, -0.0635],\n",
      "        [ 0.0317, -0.0240, -0.0135]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.10.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0491, -0.0219, -0.0332],\n",
      "        [-0.0208,  0.0547, -0.0369],\n",
      "        [-0.0040,  0.0110,  0.0239]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.10.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0004, -0.0322, -0.0112],\n",
      "        [-0.0742,  0.0145,  0.0415],\n",
      "        [ 0.0237, -0.0165,  0.0282]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.10.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.11.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0286, -0.0160,  0.0161],\n",
      "        [ 0.0164,  0.0236, -0.0165],\n",
      "        [-0.0047,  0.0054,  0.0060]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.11.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0210, -0.0204,  0.0081],\n",
      "        [-0.0057, -0.0050, -0.0047],\n",
      "        [ 0.0022, -0.0166,  0.0045]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.11.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0068, -0.0037,  0.0104],\n",
      "        [ 0.0012, -0.0270, -0.0260],\n",
      "        [-0.0586,  0.0366, -0.0156]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.11.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0070, -0.0143, -0.0099],\n",
      "        [ 0.0136,  0.0308,  0.0811],\n",
      "        [ 0.0703, -0.0021,  0.0654]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.11.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0527, -0.0359,  0.0413],\n",
      "        [-0.0625, -0.0096,  0.0098],\n",
      "        [-0.0415, -0.0154,  0.0140]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.11.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0152,  0.0874,  0.0369],\n",
      "        [-0.0420, -0.0117, -0.0334],\n",
      "        [-0.0107, -0.0005,  0.0486]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.11.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0091, -0.0315, -0.0160],\n",
      "        [ 0.0635, -0.0015,  0.0425],\n",
      "        [-0.0398, -0.0447,  0.0079]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.11.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.12.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0112,  0.0123,  0.0003],\n",
      "        [ 0.0068,  0.0466, -0.0034],\n",
      "        [-0.0176,  0.0244, -0.0310]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.12.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0033,  0.0065,  0.0032],\n",
      "        [-0.0029, -0.0092,  0.0253],\n",
      "        [-0.0102,  0.0131,  0.0211]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.12.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0027, -0.0187, -0.0170],\n",
      "        [-0.0006,  0.0114, -0.0361],\n",
      "        [-0.0011, -0.0488,  0.0415]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.12.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0009,  0.0026,  0.0713],\n",
      "        [-0.0654,  0.0156, -0.0063],\n",
      "        [ 0.0090,  0.0019, -0.0493]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.12.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0913,  0.0150,  0.0459],\n",
      "        [ 0.0439, -0.0162,  0.0312],\n",
      "        [-0.0177,  0.0613,  0.0352]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.12.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0211,  0.0085, -0.0210],\n",
      "        [-0.0015,  0.0270,  0.0330],\n",
      "        [ 0.0491,  0.0046, -0.0270]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.12.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0417,  0.0374, -0.0269],\n",
      "        [-0.0153, -0.0048,  0.0532],\n",
      "        [-0.0439, -0.0016, -0.0209]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.12.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.13.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0255,  0.0225, -0.0024],\n",
      "        [ 0.0261,  0.0019,  0.0011],\n",
      "        [ 0.0152,  0.0032,  0.0635]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.13.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0074,  0.0092,  0.0060],\n",
      "        [ 0.0198, -0.0114, -0.0166],\n",
      "        [-0.0012, -0.0019, -0.0096]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.13.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0205, -0.0186,  0.0139],\n",
      "        [-0.0275, -0.0228, -0.0513],\n",
      "        [-0.0052,  0.0275, -0.0095]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.13.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0052,  0.0146, -0.0034],\n",
      "        [ 0.0226, -0.0708, -0.0381],\n",
      "        [ 0.0110,  0.0991,  0.0645]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.13.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0415, -0.1235,  0.0732],\n",
      "        [-0.0574,  0.0786, -0.0126],\n",
      "        [-0.0291, -0.0552,  0.0254]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.13.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0104,  0.0510,  0.0068],\n",
      "        [-0.0515, -0.0430,  0.0216],\n",
      "        [-0.0422, -0.0297, -0.0049]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.13.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0330,  0.0234, -0.0208],\n",
      "        [ 0.0149, -0.0062, -0.0947],\n",
      "        [-0.0013,  0.0064, -0.0093]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.13.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.14.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0083,  0.0228,  0.0102],\n",
      "        [-0.0028,  0.0277,  0.0017],\n",
      "        [ 0.0150,  0.0036, -0.0028]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.14.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0320, -0.0084, -0.0015],\n",
      "        [-0.0001,  0.0058, -0.0036],\n",
      "        [-0.0141,  0.0012, -0.0005]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.14.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0080, -0.0098, -0.0107],\n",
      "        [ 0.0120, -0.0082,  0.0195],\n",
      "        [-0.0559,  0.0315, -0.0312]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.14.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0276, -0.0019,  0.0322],\n",
      "        [ 0.0237,  0.0112,  0.0245],\n",
      "        [ 0.0840, -0.0053, -0.0437]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.14.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0938,  0.0437,  0.0603],\n",
      "        [-0.0430,  0.0405, -0.0260],\n",
      "        [-0.0270, -0.0391,  0.0815]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.14.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0117,  0.0212, -0.0052],\n",
      "        [-0.0598, -0.0310, -0.0254],\n",
      "        [ 0.0265,  0.0566,  0.0459]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.14.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0020,  0.0132,  0.0327],\n",
      "        [-0.0294, -0.0327,  0.0874],\n",
      "        [-0.0374, -0.0337,  0.0245]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.14.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.15.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0405,  0.0215, -0.0347],\n",
      "        [ 0.0142, -0.0154,  0.0107],\n",
      "        [ 0.0188,  0.0708, -0.0052]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.15.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0311,  0.0432,  0.0082],\n",
      "        [-0.0679, -0.0115,  0.0102],\n",
      "        [-0.0182, -0.0060, -0.0175]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.15.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0161, -0.0183, -0.0540],\n",
      "        [-0.0231, -0.0261,  0.0206],\n",
      "        [ 0.0356, -0.0610,  0.0199]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.15.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0113, -0.0019, -0.0209],\n",
      "        [ 0.0204,  0.0356,  0.0435],\n",
      "        [-0.0898,  0.0400,  0.0243]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.15.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0337,  0.0374,  0.0162],\n",
      "        [-0.0364,  0.0518,  0.0310],\n",
      "        [-0.0064,  0.0444,  0.0752]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.15.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0151,  0.0045, -0.0032],\n",
      "        [-0.0359, -0.0386,  0.0400],\n",
      "        [-0.0376, -0.1035, -0.0004]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.15.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0021, -0.0099, -0.0723],\n",
      "        [ 0.0132, -0.0503, -0.0903],\n",
      "        [ 0.0325,  0.0576, -0.0415]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.15.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.16.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0292,  0.0189, -0.0071],\n",
      "        [ 0.0219,  0.0109, -0.0099],\n",
      "        [ 0.0310, -0.0225,  0.0181]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.16.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 9.5215e-03,  2.4719e-03,  9.5215e-03],\n",
      "        [-4.8584e-02, -1.1536e-02,  1.1902e-02],\n",
      "        [-1.3489e-02, -1.4832e-02, -1.1802e-05]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.16.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0014,  0.0208,  0.0122],\n",
      "        [ 0.0234, -0.0114,  0.0069],\n",
      "        [ 0.0889, -0.0211,  0.0253]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.16.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0223, -0.0334, -0.0068],\n",
      "        [ 0.0137, -0.0179, -0.0991],\n",
      "        [-0.0192, -0.0133,  0.0464]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.16.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0869,  0.0581,  0.0669],\n",
      "        [-0.0165,  0.0309,  0.0396],\n",
      "        [-0.0014, -0.0054,  0.0237]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.16.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0361, -0.0549, -0.0420],\n",
      "        [-0.0229,  0.0601, -0.0386],\n",
      "        [ 0.0422, -0.1025, -0.0747]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.16.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-2.8320e-02, -2.2583e-02, -2.8320e-02],\n",
      "        [-8.3496e-02, -3.7354e-02, -1.8555e-02],\n",
      "        [ 6.4850e-05,  2.5513e-02,  3.5889e-02]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.16.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.17.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0630,  0.0415,  0.0193],\n",
      "        [-0.0300,  0.0189,  0.0417],\n",
      "        [ 0.0302,  0.0161, -0.0422]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.17.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0194, -0.0024,  0.0028],\n",
      "        [-0.0057,  0.0172,  0.0190],\n",
      "        [-0.0068, -0.0177, -0.0108]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.17.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0002, -0.0300, -0.0216],\n",
      "        [-0.0036, -0.1191, -0.0283],\n",
      "        [ 0.0092, -0.1108, -0.0432]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.17.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0142,  0.0271, -0.0085],\n",
      "        [ 0.0535, -0.0688, -0.0253],\n",
      "        [-0.0493,  0.0248, -0.1270]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.17.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0344, -0.0069,  0.0175],\n",
      "        [ 0.0055,  0.0432,  0.0532],\n",
      "        [ 0.0233,  0.0085,  0.0151]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.17.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0356,  0.0391,  0.0688],\n",
      "        [-0.0115, -0.0562,  0.0242],\n",
      "        [-0.0277, -0.0280,  0.0405]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.17.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0040,  0.0159, -0.0004],\n",
      "        [ 0.0214,  0.0032,  0.0260],\n",
      "        [ 0.0105, -0.0654, -0.0165]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.17.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.18.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0630, -0.0317, -0.0084],\n",
      "        [-0.0542,  0.0129, -0.0309],\n",
      "        [ 0.0179,  0.0051,  0.0170]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.18.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0233, -0.0005,  0.0089],\n",
      "        [ 0.0281,  0.0054,  0.0074],\n",
      "        [ 0.0059, -0.0145, -0.0007]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.18.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0640,  0.0020,  0.0522],\n",
      "        [ 0.0923, -0.0247, -0.0688],\n",
      "        [-0.0713, -0.0083, -0.0081]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.18.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0229,  0.0014,  0.0150],\n",
      "        [ 0.0420, -0.0723,  0.0366],\n",
      "        [ 0.0996, -0.1436, -0.1533]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.18.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0020,  0.0398, -0.0391],\n",
      "        [-0.0150,  0.0432,  0.0118],\n",
      "        [ 0.0128,  0.0493, -0.0165]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.18.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-3.1006e-02,  9.9609e-02,  1.7700e-02],\n",
      "        [-6.3782e-03,  1.7822e-02,  2.2949e-02],\n",
      "        [ 1.9165e-02, -6.6895e-02,  7.2479e-05]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.18.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0177, -0.0344,  0.0025],\n",
      "        [ 0.0718, -0.0435, -0.0275],\n",
      "        [-0.0286,  0.0104, -0.0227]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.18.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.19.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0540,  0.0171,  0.0203],\n",
      "        [ 0.0400,  0.0089, -0.0135],\n",
      "        [-0.0030,  0.0322, -0.0044]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.19.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0081,  0.0136, -0.0064],\n",
      "        [-0.0003,  0.0019,  0.0002],\n",
      "        [ 0.0243,  0.0012,  0.0113]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.19.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0022,  0.0493, -0.0659],\n",
      "        [-0.0112, -0.0266, -0.0171],\n",
      "        [ 0.2910,  0.0388,  0.0244]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.19.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0248,  0.0447, -0.0410],\n",
      "        [-0.0033,  0.0139, -0.0879],\n",
      "        [-0.1719,  0.0781,  0.0444]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.19.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0466, -0.0092,  0.0483],\n",
      "        [-0.0378, -0.0298,  0.0518],\n",
      "        [ 0.0159,  0.0020,  0.2109]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.19.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0491, -0.0211, -0.0085],\n",
      "        [-0.0525, -0.0369,  0.0698],\n",
      "        [ 0.0097, -0.0154, -0.2734]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.19.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0608, -0.0067, -0.0288],\n",
      "        [ 0.0050, -0.0226,  0.0454],\n",
      "        [-0.0201,  0.0457, -0.0864]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.19.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.20.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0386,  0.0356,  0.0095],\n",
      "        [ 0.0146,  0.0493,  0.0349],\n",
      "        [-0.0588, -0.0479,  0.0220]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.20.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 3.1586e-03,  5.8289e-03,  4.3335e-03],\n",
      "        [ 6.1035e-05,  1.0742e-02, -2.4414e-03],\n",
      "        [-7.6904e-03,  1.8555e-02,  4.3945e-03]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.20.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0820, -0.0229, -0.0306],\n",
      "        [ 0.0203,  0.0845,  0.0098],\n",
      "        [-0.0014, -0.0640,  0.0133]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.20.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-1.8799e-02,  4.2969e-02, -4.9072e-02],\n",
      "        [-9.7168e-02,  7.1716e-03,  1.2695e-01],\n",
      "        [-1.0204e-04, -1.2695e-01, -1.8677e-02]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.20.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0327, -0.0177, -0.0527],\n",
      "        [ 0.0732, -0.0067, -0.0308],\n",
      "        [-0.0031,  0.0776, -0.0530]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.20.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0190,  0.0020,  0.0618],\n",
      "        [-0.0752,  0.0051,  0.0161],\n",
      "        [-0.0194,  0.0320, -0.0742]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.20.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0125,  0.0013,  0.0058],\n",
      "        [-0.0752,  0.0148,  0.0147],\n",
      "        [-0.0255,  0.0104,  0.0498]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.20.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.21.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0469, -0.0393,  0.0120],\n",
      "        [ 0.0311,  0.0435,  0.0037],\n",
      "        [ 0.0137,  0.0079, -0.0003]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.21.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0018, -0.0083, -0.0154],\n",
      "        [-0.0131, -0.0459,  0.0009],\n",
      "        [-0.0117,  0.0277, -0.0107]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.21.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.1157,  0.0281,  0.0135],\n",
      "        [-0.0035, -0.0356, -0.0435],\n",
      "        [ 0.0273,  0.0143, -0.0175]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.21.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0008,  0.0339, -0.0254],\n",
      "        [ 0.0312,  0.1201,  0.0820],\n",
      "        [ 0.0136,  0.0317, -0.0457]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.21.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0022,  0.0232,  0.0393],\n",
      "        [-0.0486, -0.0342,  0.0261],\n",
      "        [-0.0259, -0.0002,  0.0242]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.21.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0244, -0.0181,  0.0938],\n",
      "        [ 0.0322, -0.0067, -0.0295],\n",
      "        [ 0.0105, -0.0544,  0.0067]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.21.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0330, -0.0029, -0.0366],\n",
      "        [-0.0054,  0.0405, -0.0535],\n",
      "        [ 0.0347, -0.0245,  0.0053]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.21.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.22.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0059,  0.0261, -0.0225],\n",
      "        [ 0.0161,  0.0183, -0.0160],\n",
      "        [ 0.0251, -0.0118, -0.0369]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.22.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0038, -0.0025, -0.0020],\n",
      "        [-0.0060,  0.0288,  0.0077],\n",
      "        [-0.0067,  0.0093,  0.0110]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.22.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0297, -0.0420, -0.0072],\n",
      "        [-0.0110, -0.0115,  0.0236],\n",
      "        [-0.0249,  0.1953, -0.0220]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.22.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0459, -0.0051,  0.0102],\n",
      "        [-0.0154, -0.0405,  0.0942],\n",
      "        [ 0.0261,  0.0483, -0.0640]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.22.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0928, -0.0527, -0.0010],\n",
      "        [ 0.0087,  0.0117, -0.0408],\n",
      "        [ 0.0033,  0.0277, -0.0166]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.22.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0684,  0.0120, -0.0205],\n",
      "        [-0.0339, -0.0610, -0.0128],\n",
      "        [ 0.0003,  0.0210, -0.0081]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.22.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0173,  0.0221,  0.0014],\n",
      "        [-0.0383, -0.0165, -0.0415],\n",
      "        [-0.0303,  0.0210,  0.0022]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.22.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.23.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0237,  0.0242, -0.0134],\n",
      "        [ 0.0134, -0.0225, -0.0129],\n",
      "        [-0.0019,  0.0079,  0.0347]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.23.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0065, -0.0024,  0.0150],\n",
      "        [-0.0074, -0.0188,  0.0137],\n",
      "        [ 0.0051, -0.0039, -0.0040]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.23.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.1328,  0.0150, -0.0060],\n",
      "        [ 0.0159,  0.0120, -0.0015],\n",
      "        [-0.1396, -0.0005,  0.0009]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.23.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0265, -0.0320, -0.0118],\n",
      "        [ 0.0752, -0.0381,  0.0601],\n",
      "        [-0.0071, -0.0176,  0.0039]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.23.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0413, -0.0183,  0.0479],\n",
      "        [ 0.0074,  0.0356,  0.0718],\n",
      "        [-0.0396,  0.0291,  0.0040]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.23.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0354,  0.0615,  0.0237],\n",
      "        [ 0.0505, -0.0549, -0.0251],\n",
      "        [-0.0449, -0.0157, -0.0737]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.23.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0417, -0.0330,  0.0101],\n",
      "        [ 0.0525, -0.0430,  0.0107],\n",
      "        [ 0.0474,  0.0091, -0.0275]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.23.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.24.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[0.0024, 0.0109, 0.0066],\n",
      "        [0.0034, 0.0079, 0.0187],\n",
      "        [0.0107, 0.0067, 0.0284]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.24.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0063, -0.0447, -0.0042],\n",
      "        [-0.0079,  0.0242,  0.0010],\n",
      "        [-0.0027,  0.0255,  0.0232]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.24.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0267,  0.0747,  0.0239],\n",
      "        [ 0.0201,  0.0496, -0.0256],\n",
      "        [-0.0001, -0.0016, -0.0059]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.24.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0175, -0.0094,  0.0354],\n",
      "        [ 0.0698, -0.0025,  0.0530],\n",
      "        [-0.0496, -0.0295, -0.0879]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.24.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0019, -0.0231,  0.0664],\n",
      "        [ 0.0347,  0.0649,  0.0347],\n",
      "        [-0.0262, -0.0292,  0.0270]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.24.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0017, -0.0244, -0.0170],\n",
      "        [ 0.0613, -0.0320, -0.0645],\n",
      "        [ 0.0820,  0.0747,  0.0339]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.24.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0291,  0.0227, -0.0146],\n",
      "        [ 0.0693,  0.0166, -0.0098],\n",
      "        [ 0.0069, -0.0028, -0.0041]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.24.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.25.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0042, -0.0001,  0.0151],\n",
      "        [ 0.0069, -0.0466, -0.0339],\n",
      "        [-0.0117,  0.0493, -0.0250]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.25.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0048, -0.0010, -0.0095],\n",
      "        [-0.0029,  0.0292,  0.0062],\n",
      "        [-0.0043,  0.0034, -0.0238]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.25.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.1455, -0.1172,  0.0216],\n",
      "        [-0.0234, -0.0320, -0.0120],\n",
      "        [-0.1089,  0.0002,  0.0645]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.25.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0327,  0.0476,  0.0308],\n",
      "        [-0.0630, -0.0510,  0.0238],\n",
      "        [ 0.0112,  0.0537, -0.0713]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.25.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0083,  0.0192,  0.0091],\n",
      "        [ 0.0752,  0.0051, -0.0031],\n",
      "        [ 0.0659,  0.0593, -0.0261]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.25.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0236, -0.0194, -0.0315],\n",
      "        [-0.0366,  0.0308,  0.0479],\n",
      "        [ 0.0981,  0.0049, -0.0349]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.25.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0522,  0.0457, -0.0588],\n",
      "        [-0.0125, -0.0220, -0.0500],\n",
      "        [-0.0359, -0.0337,  0.0189]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.25.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.26.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0178, -0.0400, -0.0179],\n",
      "        [ 0.0150,  0.0199,  0.0091],\n",
      "        [ 0.0062, -0.0269,  0.0087]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.26.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0116, -0.0172,  0.0052],\n",
      "        [ 0.0025, -0.0034,  0.0072],\n",
      "        [ 0.0060,  0.0160, -0.0177]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.26.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0728,  0.0009, -0.0123],\n",
      "        [ 0.0347, -0.0170, -0.0009],\n",
      "        [ 0.1494, -0.0049,  0.0189]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.26.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0238,  0.0075,  0.0415],\n",
      "        [ 0.1021, -0.0535,  0.0325],\n",
      "        [ 0.0078,  0.0469, -0.0156]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.26.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0089,  0.0112, -0.0435],\n",
      "        [ 0.0791,  0.0089,  0.0486],\n",
      "        [-0.0026,  0.0620,  0.0079]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.26.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0461, -0.0195,  0.0806],\n",
      "        [-0.0544,  0.0115,  0.0122],\n",
      "        [ 0.0271,  0.0160, -0.0332]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.26.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0703, -0.0933,  0.0145],\n",
      "        [-0.0032, -0.0398,  0.0576],\n",
      "        [ 0.0210, -0.0145, -0.0309]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.26.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: model.layers.27.self_attn.q_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([1536])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0032,  0.0564, -0.0186],\n",
      "        [ 0.0011,  0.0236, -0.0047],\n",
      "        [-0.0090,  0.0094,  0.0018]])\n",
      "  [This appears to be a query projection]\n",
      "\n",
      "--- Layer: model.layers.27.self_attn.k_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0022, -0.0342,  0.0020],\n",
      "        [-0.0115, -0.0083, -0.0125],\n",
      "        [ 0.0086, -0.0041, -0.0225]])\n",
      "  [This appears to be a key projection]\n",
      "\n",
      "--- Layer: model.layers.27.self_attn.v_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([256, 1536]) (out_features, in_features)\n",
      "  Bias shape: torch.Size([256])\n",
      "  First weight values (3x3):\n",
      "tensor([[ 8.1055e-02, -7.7637e-02,  1.2665e-03],\n",
      "        [-9.2773e-02, -1.3733e-02, -2.2217e-02],\n",
      "        [ 2.0874e-02, -8.3008e-03,  1.3888e-05]])\n",
      "  [This appears to be a value projection]\n",
      "\n",
      "--- Layer: model.layers.27.self_attn.o_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0150,  0.0214, -0.0442],\n",
      "        [-0.0398, -0.0359,  0.0044],\n",
      "        [-0.0620,  0.0620,  0.0757]])\n",
      "  [This appears to be an output projection]\n",
      "\n",
      "--- Layer: model.layers.27.mlp.gate_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.1118,  0.0111, -0.0679],\n",
      "        [-0.0400, -0.0168,  0.0040],\n",
      "        [ 0.0640,  0.0067, -0.0479]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.27.mlp.up_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([8960, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0260,  0.0830,  0.0854],\n",
      "        [-0.0090, -0.0069, -0.0060],\n",
      "        [ 0.0376, -0.0270,  0.0786]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.27.mlp.down_proj ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([1536, 8960]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[-0.0447,  0.0231,  0.0383],\n",
      "        [ 0.0300, -0.0322, -0.0359],\n",
      "        [-0.0244, -0.0444, -0.0605]])\n",
      "  [This appears to be a feed-forward layer component]\n",
      "\n",
      "--- Layer: model.layers.27.post_attention_layernorm ---\n",
      "Type: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm'>\n",
      "\n",
      "--- Layer: lm_head ---\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "  Weight shape: torch.Size([151936, 1536]) (out_features, in_features)\n",
      "  First weight values (3x3):\n",
      "tensor([[ 0.0776,  0.0293, -0.0143],\n",
      "        [ 0.0518, -0.0444,  0.0284],\n",
      "        [ 0.0269, -0.0486, -0.0099]])\n",
      "\n",
      "=== Detailed First Layer Inspection ===\n",
      "\n",
      "Parameter: model.layers.0.self_attn.q_proj.weight\n",
      "Shape: torch.Size([1536, 1536])\n",
      "First few values:\n",
      "tensor([[-0.0300,  0.0226,  0.0251,  0.0065,  0.0058],\n",
      "        [-0.0177, -0.0050,  0.0713,  0.0150,  0.0315]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch.nn as nn\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "def inspect_layer(name, module):\n",
    "    \"\"\"Helper function to inspect a single layer\"\"\"\n",
    "    print(f\"\\n--- Layer: {name} ---\")\n",
    "    print(f\"Type: {type(module)}\")\n",
    "\n",
    "    # Check if it's a standard linear layer\n",
    "    if isinstance(module, nn.Linear):\n",
    "        print(f\"  Weight shape: {module.weight.shape} (out_features, in_features)\")\n",
    "        if module.bias is not None:\n",
    "            print(f\"  Bias shape: {module.bias.shape}\")\n",
    "        print(f\"  First weight values (3x3):\\n{module.weight.data[:3, :3]}\")\n",
    "\n",
    "    # Check for common patterns in transformer layers\n",
    "    if \"q_proj\" in name:\n",
    "        print(\"  [This appears to be a query projection]\")\n",
    "    elif \"k_proj\" in name:\n",
    "        print(\"  [This appears to be a key projection]\")\n",
    "    elif \"v_proj\" in name:\n",
    "        print(\"  [This appears to be a value projection]\")\n",
    "    elif \"o_proj\" in name:\n",
    "        print(\"  [This appears to be an output projection]\")\n",
    "    elif \"gate_proj\" in name or \"up_proj\" in name or \"down_proj\" in name:\n",
    "        print(\"  [This appears to be a feed-forward layer component]\")\n",
    "\n",
    "\n",
    "# Iterate through all layers\n",
    "for name, module in model.named_modules():\n",
    "    # Skip very high-level modules to reduce output\n",
    "    if len(name.split(\".\")) > 6:  # Adjust this number as needed\n",
    "        continue\n",
    "\n",
    "    # Only inspect certain types of layers\n",
    "    if isinstance(module, nn.Linear) or \"proj\" in name or \"attention\" in name:\n",
    "        inspect_layer(name, module)\n",
    "\n",
    "# Additional inspection of the first layer's weights\n",
    "print(\"\\n=== Detailed First Layer Inspection ===\")\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layers.0\" in name and \"weight\" in name:\n",
    "        print(f\"\\nParameter: {name}\")\n",
    "        print(f\"Shape: {param.shape}\")\n",
    "        print(\n",
    "            f\"First few values:\\n{param.data[:2, :5] if len(param.shape) > 1 else param.data[:5]}\"\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f9e29",
   "metadata": {},
   "source": [
    "# LLAMA3 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6249a5ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B.\n401 Client Error. (Request ID: Root=1-683353fb-51039aed3df2f93708f1d9e0;d16a0d44-2fe3-471d-beec-565097c2ccd1)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:406\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGatedRepoError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    402\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     resolved_file = \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:860\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    859\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m860\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m    862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:967\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m    966\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1482\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1480\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[32m   1481\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1483\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1484\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1374\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1294\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[39m\n\u001b[32m   1293\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1294\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1303\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:278\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:302\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    301\u001b[39m response = get_session().request(method=method, url=url, **params)\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:423\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    420\u001b[39m     message = (\n\u001b[32m    421\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    422\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_message == \u001b[33m\"\u001b[39m\u001b[33mAccess to this resource is disabled.\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mGatedRepoError\u001b[39m: 401 Client Error. (Request ID: Root=1-683353fb-51039aed3df2f93708f1d9e0;d16a0d44-2fe3-471d-beec-565097c2ccd1)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoConfig\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m original_config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta-llama/Meta-Llama-3-8B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(original_config)  \u001b[38;5;66;03m# Compare hidden_size, layers, etc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1017\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1014\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1015\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1017\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1018\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1019\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/transformers/configuration_utils.py:574\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    573\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/transformers/configuration_utils.py:633\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    629\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    632\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    648\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:421\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[32m    420\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    422\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure to have access to it at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    423\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    424\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    427\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a local folder and is not a valid model identifier \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    428\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlisted on \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    429\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    430\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`token=<your_token>`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    431\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B.\n401 Client Error. (Request ID: Root=1-683353fb-51039aed3df2f93708f1d9e0;d16a0d44-2fe3-471d-beec-565097c2ccd1)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "original_config = AutoConfig.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "print(original_config)  # Compare hidden_size, layers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Config {\n",
      "  \"_name_or_path\": \"Qwen/Qwen1.5-1.8B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5504,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "original_config = AutoConfig.from_pretrained(\"meta-llama/Meta-Llama-3-70B\")\n",
    "print(original_config)  # Compare hidden_size, layers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75561121",
   "metadata": {},
   "source": [
    "# DeepSeek V3 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c899877c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Loading deepseek-ai/deepseek-v3 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdeepseek-ai/deepseek-v3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(config)  \u001b[38;5;66;03m# Compare hidden_size, layers, etc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1020\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1018\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1019\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m trust_remote_code = \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_remote_code\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m   1025\u001b[39m     class_ref = config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/deepseek_local_runner/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:678\u001b[39m, in \u001b[36mresolve_trust_remote_code\u001b[39m\u001b[34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001b[39m\n\u001b[32m    675\u001b[39m         _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_local_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    679\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires you to execute the configuration file in that\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    680\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m repo on your local machine. Make sure you have read the code there to avoid malicious use, then\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m set the option `trust_remote_code=True` to remove this error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m     )\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trust_remote_code\n",
      "\u001b[31mValueError\u001b[39m: Loading deepseek-ai/deepseek-v3 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"deepseek-ai/deepseek-v3\")\n",
    "print(config)  # Compare hidden_size, layers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e40f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "# Load models\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "summary(model, input_size=(1, 128), dtypes=[torch.int64])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3fe33",
   "metadata": {},
   "source": [
    "# Inspect Layer Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4341fa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Inspecting: model.layers.0.self_attn.k_proj.pt\n",
      "==================================================\n",
      "[Type] <class 'dict'>\n",
      "\n",
      "[Quantized Weight Structure]\n",
      "Keys: ['qweight', 'qzeros', 'scales', 'bias']\n",
      "\n",
      "[ERROR] Failed to inspect /home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.0.self_attn.k_proj.pt: 'weight'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def inspect_quantized_file(file_path: Path, sample_size: int = 5):\n",
    "    \"\"\"\n",
    "    Inspects a quantized model file (.pt) and prints key information.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the .pt file\n",
    "        sample_size: Number of elements to display for sampling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load data\n",
    "        data = torch.load(file_path)\n",
    "        print(f\"\\n{'='*50}\\nInspecting: {file_path.name}\\n{'='*50}\")\n",
    "\n",
    "        # Basic info\n",
    "        print(f\"[Type] {type(data)}\")\n",
    "\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            # Handle raw tensor case\n",
    "            print(\"\\n[Raw Tensor]\")\n",
    "            print(f\"Shape: {data.shape}\")\n",
    "            print(f\"dtype: {data.dtype}\")\n",
    "            print(f\"Device: {data.device}\")\n",
    "\n",
    "            # Statistics\n",
    "            if data.dtype in (torch.float16, torch.float32, torch.bfloat16):\n",
    "                data_float = data.float()\n",
    "                print(f\"\\n[Statistics]\")\n",
    "                print(f\"Min: {data_float.min().item():.4f}\")\n",
    "                print(f\"Max: {data_float.max().item():.4f}\")\n",
    "                print(f\"Mean: {data_float.mean().item():.4f}\")\n",
    "                print(f\"Std: {data_float.std().item():.4f}\")\n",
    "\n",
    "            # Sample values\n",
    "            print(f\"\\n[Sample Values (first {sample_size} elements)]\")\n",
    "            print(data.flatten()[:sample_size].tolist())\n",
    "\n",
    "            # Plot histogram if reasonable size\n",
    "            if data.numel() < 1e6:  # Don't plot for huge tensors\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                plt.hist(data.float().cpu().numpy().flatten(), bins=50)\n",
    "                plt.title(f\"Value Distribution - {file_path.name}\")\n",
    "                plt.xlabel(\"Value\")\n",
    "                plt.ylabel(\"Frequency\")\n",
    "                plt.show()\n",
    "\n",
    "        elif isinstance(data, dict):\n",
    "            # Handle quantized weight dictionary\n",
    "            print(\"\\n[Quantized Weight Structure]\")\n",
    "            print(\"Keys:\", list(data.keys()))\n",
    "\n",
    "            # Required fields\n",
    "            qweight = data[\"weight\"]\n",
    "            print(f\"\\n[Weight Tensor]\")\n",
    "            print(f\"Shape: {qweight.shape}\")\n",
    "            print(f\"dtype: {qweight.dtype}\")\n",
    "            print(f\"Device: {qweight.device}\")\n",
    "\n",
    "            # Sample values\n",
    "            print(f\"\\n[Sample Weight Values (first {sample_size} elements)]\")\n",
    "            print(qweight.flatten()[:sample_size].tolist())\n",
    "\n",
    "            # Check for quantization parameters\n",
    "            if \"scales\" in data:\n",
    "                scales = data[\"scales\"]\n",
    "                print(f\"\\n[Scales]\")\n",
    "                print(f\"Shape: {scales.shape}\")\n",
    "                print(f\"Min: {scales.min().item():.4f}\")\n",
    "                print(f\"Max: {scales.max().item():.4f}\")\n",
    "                print(f\"Sample: {scales.flatten()[:sample_size].tolist()}\")\n",
    "\n",
    "            if \"zeros\" in data:\n",
    "                zeros = data[\"zeros\"]\n",
    "                print(f\"\\n[Zeros]\")\n",
    "                print(f\"Shape: {zeros.shape}\")\n",
    "                print(f\"Sample: {zeros.flatten()[:sample_size].tolist()}\")\n",
    "\n",
    "            if \"bias\" in data:\n",
    "                bias = data[\"bias\"]\n",
    "                print(f\"\\n[Bias]\")\n",
    "                print(f\"Shape: {bias.shape}\")\n",
    "                print(f\"Sample: {bias.flatten()[:sample_size].tolist()}\")\n",
    "\n",
    "            # Special handling for packed 4-bit weights\n",
    "            if qweight.dtype == torch.int32:\n",
    "                print(\"\\n[4-bit Packed Weights]\")\n",
    "                packed_val = qweight[0, 0].item()\n",
    "                unpacked = [(packed_val >> (4 * i)) & 0xF for i in range(8)]\n",
    "                print(f\"First packed int32: {packed_val} → Unpacked 4-bit: {unpacked}\")\n",
    "\n",
    "        else:\n",
    "            print(\"\\n[Unknown data format]\")\n",
    "            print(data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Failed to inspect {file_path}: {str(e)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Configure these paths\n",
    "    base_dir = Path(\"/home/xzhang/models/deepseek-awq-scrooge/quantized_layers\")\n",
    "    target_file = \"model.layers.0.self_attn.k_proj.pt\"  # Or use *.pt to process all\n",
    "\n",
    "    # Single file inspection\n",
    "    inspect_quantized_file(base_dir / target_file)\n",
    "\n",
    "    # Uncomment to process all .pt files in directory\n",
    "    # for pt_file in base_dir.glob(\"*.pt\"):\n",
    "    #     inspect_quantized_file(pt_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ec2550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files: [PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.19.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.27.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.2.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.20.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.8.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.12.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.22.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.8.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.18.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.26.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.1.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.18.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.16.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.10.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.21.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.13.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.2.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.5.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.25.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.8.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.2.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.6.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.1.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.19.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.25.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.18.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.22.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.9.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.18.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.6.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.1.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.6.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.10.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.15.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.2.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.3.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.13.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.9.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.17.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.0.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.24.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.15.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.7.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.5.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.10.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.15.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.23.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.24.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.7.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.26.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.3.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.13.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.26.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.1.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.21.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.3.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.21.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.18.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.9.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.10.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.7.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.11.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.26.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.14.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.6.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.5.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.7.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.2.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.14.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.13.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.9.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.4.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.19.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.15.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.12.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.9.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.16.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.20.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.17.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.27.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.4.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.8.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.14.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.15.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.5.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.13.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.25.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.20.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.11.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.16.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.19.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.17.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.11.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.6.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.22.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.25.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.7.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.0.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.4.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.25.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.24.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.6.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.7.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.26.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.23.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.13.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.20.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.27.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.21.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.23.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.0.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.14.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.12.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.16.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.11.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.27.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.0.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.22.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.20.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.17.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.21.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.17.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.14.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.19.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.24.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.11.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.18.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.15.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.3.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.22.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.3.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.11.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.1.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.12.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.12.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.23.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.26.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.24.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.25.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.19.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.10.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.4.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.24.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.17.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.16.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.23.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.0.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.9.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.5.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.22.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.27.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.8.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.2.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.4.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.8.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.23.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.0.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.5.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.4.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.12.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.16.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.14.self_attn.q_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.1.self_attn.v_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.21.mlp.up_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.27.self_attn.k_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.10.mlp.down_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.3.mlp.gate_proj.pt'), PosixPath('/home/xzhang/models/deepseek-awq-scrooge/quantized_layers/model.layers.20.self_attn.k_proj.pt')]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(\"/home/xzhang/models/deepseek-awq-scrooge/quantized_layers\")\n",
    "print(\"Available files:\", list(base_dir.glob(\"*\")))\n",
    "\n",
    "layer_file = \"model.layers.0.self_attn.q_proj.pt\"  # Change as needed\n",
    "target_file = base_dir / layer_file\n",
    "data = torch.load(target_file)\n",
    "\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47044212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary contents:\n",
      "\n",
      "qweight:\n",
      "Shape: torch.Size([8960, 192])\n",
      "Dtype: torch.int32\n",
      "First few rows:\n",
      "tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "\n",
      "qzeros:\n",
      "Shape: torch.Size([70, 192])\n",
      "Dtype: torch.int32\n",
      "First few rows:\n",
      "tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.int32)\n",
      "\n",
      "scales:\n",
      "Shape: torch.Size([70, 1536])\n",
      "Dtype: torch.float16\n",
      "First few rows:\n",
      "tensor([[4.6250e+00, 3.6875e+00, 6.1562e+00, 3.7656e+00, 3.3750e+00],\n",
      "        [7.7820e-03, 8.0566e-03, 5.9509e-03, 9.2163e-03, 6.8054e-03],\n",
      "        [1.0014e-04, 1.0014e-04, 1.0014e-04, 1.0014e-04, 1.0014e-04],\n",
      "        [1.7452e-04, 2.8038e-04, 2.6894e-04, 1.7262e-04, 2.5749e-04],\n",
      "        [1.0014e-04, 1.0014e-04, 1.0014e-04, 1.0014e-04, 1.0014e-04]],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(\"/home/xzhang/models/deepseek-awq-scrooge/quantized_layers\")\n",
    "\n",
    "layer_file = \"model.layers.9.mlp.down_proj.pt\"  # Change as needed\n",
    "target_file = base_dir / layer_file\n",
    "data = torch.load(target_file)\n",
    "\n",
    "\n",
    "if isinstance(data, torch.Tensor):\n",
    "    print(f\"Tensor shape: {data.shape}\")\n",
    "    print(f\"Tensor dtype: {data.dtype}\")\n",
    "    print(\"\\nValues:\")\n",
    "    print(data)\n",
    "\n",
    "elif isinstance(data, dict):\n",
    "    print(\"Dictionary contents:\")\n",
    "    for key, value in data.items():\n",
    "        print(f\"\\n{key}:\")\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"Shape: {value.shape}\")\n",
    "            print(f\"Dtype: {value.dtype}\")\n",
    "            if value.numel() <= 10:  # Print full tensor if small\n",
    "                print(\"Values:\")\n",
    "                print(value)\n",
    "            else:\n",
    "                print(\n",
    "                    \"First few values:\" if len(value.shape) == 1 else \"First few rows:\"\n",
    "                )\n",
    "                print(value[:5] if len(value.shape) == 1 else value[:5, :5])\n",
    "        else:\n",
    "            print(value)\n",
    "else:\n",
    "    print(\"Unknown data type:\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a9401e",
   "metadata": {},
   "source": [
    "## Qweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1973cf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Inspecting 168 layer files in: /home/xzhang/models/deepseek-awq-scrooge/quantized_layers\n",
      "\n",
      "model.layers.0.mlp.down_proj.pt          | shape: (8960, 192)          | zeros: 31.48% | unique[:10]: [-2147482912, -2147477591, -2147474320, -2147471587, -2147471339, -2147469824, -2147355904, -2147305013, -2147254939, -2147245090] ✅\n",
      "model.layers.0.mlp.gate_proj.pt          | shape: (1536, 1120)         | zeros: 20.27% | unique[:10]: [-2147483648, -2147483551, -2147483522, -2147483493, -2147483076, -2147483016, -2147482477, -2147481730, -2147480688, -2147476564] ✅\n",
      "model.layers.0.mlp.up_proj.pt            | shape: (1536, 1120)         | zeros: 38.93% | unique[:10]: [-2147483648, -2147483606, -2147483536, -2147483449, -2147482695, -2147482592, -2147481602, -2147474684, -2147473856, -2147471600] ✅\n",
      "model.layers.0.self_attn.k_proj.pt       | shape: (1536, 32)           | zeros: 7.35% | unique[:10]: [-2147200187, -2130016890, -2121405444, -2095701717, -2093492976, -2089025545, -2047559711, -2022025157, -2006903808, -1992362129] ✅\n",
      "model.layers.0.self_attn.q_proj.pt       | shape: (1536, 192)          | zeros: 8.42% | unique[:10]: [-2147483648, -2147483640, -2147483518, -2147483200, -2147482096, -2147482048, -2147481430, -2147475406, -2147467532, -2147430400] ✅\n",
      "model.layers.0.self_attn.v_proj.pt       | shape: (1536, 32)           | zeros: 8.01% | unique[:10]: [-2147450736, -2147255344, -2144748586, -2143551496, -2143223774, -2140954050, -2139095031, -2137956352, -2137221496, -2131130045] ✅\n",
      "model.layers.1.mlp.down_proj.pt          | shape: (8960, 192)          | zeros: 35.74% | unique[:10]: [-2147483569, -2147483557, -2147483551, -2147468336, -2147460320, -2147453692, -2147446944, -2147446932, -2147443840, -2147443359] ✅\n",
      "model.layers.1.mlp.gate_proj.pt          | shape: (1536, 1120)         | zeros: 16.66% | unique[:10]: [-2147482816, -2147480819, -2147479031, -2147477675, -2147477472, -2147474653, -2147474383, -2147473790, -2147473344, -2147469392] ✅\n",
      "model.layers.1.mlp.up_proj.pt            | shape: (1536, 1120)         | zeros: 41.66% | unique[:10]: [-2147482865, -2147481822, -2147480832, -2147475550, -2147420410, -2147419268, -2147417056, -2147412800, -2147357960, -2147350816] ✅\n",
      "model.layers.1.self_attn.k_proj.pt       | shape: (1536, 32)           | zeros: 1.00% | unique[:10]: [-2147483648, -2147469568, -2147037179, -2141768448, -2141192093, -2139762621, -2139757629, -2139132416, -2137933328, -2137632912] ✅\n",
      "model.layers.1.self_attn.q_proj.pt       | shape: (1536, 192)          | zeros: 3.06% | unique[:10]: [-2147483648, -2147483541, -2147462656, -2147461052, -2147458560, -2147439616, -2147418433, -2147287200, -2147264498, -2147252064] ✅\n",
      "model.layers.1.self_attn.v_proj.pt       | shape: (1536, 32)           | zeros: 1.53% | unique[:10]: [-2147047936, -2146224128, -2141136633, -2137760560, -2134504016, -2132625477, -2095202480, -2030043300, -2005827961, -2005151344] ✅\n",
      "model.layers.10.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 33.43% | unique[:10]: [-2147482461, -2147482456, -2147481440, -2147474686, -2147463392, -2147461579, -2147454928, -2147453353, -2147451600, -2147430395] ✅\n",
      "model.layers.10.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 58.32% | unique[:10]: [-2147437483, -2147418458, -2147200247, -2147199930, -2147155795, -2147077347, -2147024617, -2146995744, -2146889462, -2146807712] ✅\n",
      "model.layers.10.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 66.67% | unique[:10]: [-2147483648, -2147392385, -2147325136, -2147009941, -2146926848, -2146836768, -2146571883, -2146435071, -2146044781, -2144963592] ✅\n",
      "model.layers.10.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 0.05% | unique[:10]: [-2147421257, -2145288192, -2144864271, -2139213770, -2137244064, -2132990563, -2109243439, -2096103424, -2091888637, -2081440512] ✅\n",
      "model.layers.10.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 0.11% | unique[:10]: [-2147483648, -2147482756, -2147482288, -2147464960, -2147420672, -2147330573, -2146738254, -2146731060, -2146403456, -2145497073] ✅\n",
      "model.layers.10.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 0.33% | unique[:10]: [-2147336424, -2147269371, -2144301003, -2144297841, -2139864016, -2139422766, -2133508107, -2131398986, -2123890823, -2122375168] ✅\n",
      "model.layers.11.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 31.21% | unique[:10]: [-2147482780, -2147469766, -2147469582, -2147445817, -2147409920, -2147401688, -2147383258, -2147378688, -2147372096, -2147298446] ✅\n",
      "model.layers.11.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 25.00% | unique[:10]: [-2147483648, -2147483582, -2147482902, -2147475440, -2147468352, -2147452342, -2147446682, -2147352544, -2147266467, -2147217146] ✅\n",
      "model.layers.11.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 49.99% | unique[:10]: [-2147483647, -2147483524, -2147482503, -2147480780, -2147479943, -2147477835, -2147475888, -2147460082, -2147449187, -2147435474] ✅\n",
      "model.layers.11.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 16.59% | unique[:10]: [-2125989402, -2122422401, -2117601417, -2103476326, -2096992204, -2093179109, -2085317715, -2043086541, -2039395066, -2030088202] ✅\n",
      "model.layers.11.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 16.49% | unique[:10]: [-2147483648, -2147469257, -2147449590, -2146957563, -2146928644, -2146336816, -2146299854, -2145382619, -2145308666, -2144251373] ✅\n",
      "model.layers.11.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 15.37% | unique[:10]: [-2147476487, -2147471318, -2146878576, -2143306265, -2103443470, -2085017676, -2080203523, -2072098816, -2065272018, -2046011831] ✅\n",
      "model.layers.12.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 21.98% | unique[:10]: [-2147483526, -2147481037, -2147480832, -2147471341, -2147469954, -2147469555, -2147464939, -2147461302, -2147460093, -2147457226] ✅\n",
      "model.layers.12.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 58.21% | unique[:10]: [-2147483518, -2147396275, -2147369089, -2147337735, -2147223866, -2147061365, -2146917790, -2146911502, -2145556054, -2144856839] ✅\n",
      "model.layers.12.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 83.21% | unique[:10]: [-2147483519, -2147444928, -2147033858, -2145913088, -2145650725, -2145037568, -2143223903, -2139111488, -2137022576, -2131004291] ✅\n",
      "model.layers.12.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 8.52% | unique[:10]: [-2147221372, -2141622268, -2141190028, -2128708016, -2128218395, -2123434290, -2123219463, -2121016144, -2113915935, -2109741279] ✅\n",
      "model.layers.12.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 10.49% | unique[:10]: [-2147483648, -2147483168, -2147483082, -2147473664, -2147459616, -2147442560, -2147426592, -2147355758, -2147327456, -2147221500] ✅\n",
      "model.layers.12.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 8.79% | unique[:10]: [-2147483648, -2147267136, -2144600206, -2105814180, -2097150746, -2092979636, -2088763712, -2047868950, -2031228708, -2018123861] ✅\n",
      "model.layers.13.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 32.24% | unique[:10]: [-2147442331, -2147438384, -2147404784, -2147388368, -2147281216, -2147266560, -2147208189, -2147179713, -2147174612, -2147170496] ✅\n",
      "model.layers.13.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 58.29% | unique[:10]: [-2147255052, -2147221696, -2147131904, -2146845137, -2146832389, -2146714633, -2146675214, -2145779984, -2145377966, -2145190265] ✅\n",
      "model.layers.13.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 74.87% | unique[:10]: [-2147481296, -2147481168, -2147460768, -2147460295, -2147451424, -2147446694, -2147429120, -2147415440, -2147414416, -2147412736] ✅\n",
      "model.layers.13.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 0.00% | unique[:10]: [-2147483644, -2147411664, -2145865746, -2140970048, -2124693763, -2114255744, -2096984050, -2096869169, -2068234142, -2063663840] ✅\n",
      "model.layers.13.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 0.00% | unique[:10]: [-2147483618, -2147482078, -2147480304, -2147472001, -2147438586, -2147411286, -2147395328, -2147142393, -2147114676, -2147024968] ✅\n",
      "model.layers.13.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 0.28% | unique[:10]: [-2147483648, -2147482768, -2147454700, -2147442704, -2146441119, -2141785356, -2141780255, -2133708537, -2123522112, -2119221899] ✅\n",
      "model.layers.14.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 38.86% | unique[:10]: [-2147480308, -2147442688, -2147418754, -2147381243, -2147312790, -2147268773, -2147200793, -2147155707, -2147153651, -2147143560] ✅\n",
      "model.layers.14.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 66.60% | unique[:10]: [-2147483479, -2147464853, -2147454971, -2147437562, -2147427595, -2147393559, -2147151860, -2147079840, -2146987488, -2146983938] ✅\n",
      "model.layers.14.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 83.23% | unique[:10]: [-2147423937, -2147420299, -2147270452, -2147114592, -2146971129, -2146892792, -2146823350, -2146760272, -2146743520, -2146730059] ✅\n",
      "model.layers.14.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 0.87% | unique[:10]: [-2147445689, -2147418179, -2145951946, -2141292448, -2139639790, -2125593241, -2123260630, -2112747095, -2103526544, -2080308991] ✅\n",
      "model.layers.14.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 0.10% | unique[:10]: [-2147483648, -2147463815, -2147419312, -2147394704, -2146988562, -2146897920, -2146885632, -2146885193, -2146875403, -2146643133] ✅\n",
      "model.layers.14.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 0.66% | unique[:10]: [-2147481174, -2147336384, -2144480618, -2139159008, -2137980924, -2137322989, -2132864208, -2130132855, -2127515207, -2114502537] ✅\n",
      "model.layers.15.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 26.53% | unique[:10]: [-2147483435, -2147483326, -2147481616, -2147481251, -2147459418, -2147454963, -2147439776, -2147429244, -2147417947, -2147406675] ✅\n",
      "model.layers.15.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 41.52% | unique[:10]: [-2147482624, -2147481680, -2147476809, -2147468800, -2147462037, -2147452396, -2147438547, -2147426592, -2147421940, -2147410203] ✅\n",
      "model.layers.15.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 49.97% | unique[:10]: [-2147483648, -2147481885, -2147479856, -2147464812, -2147464304, -2147442571, -2147438587, -2147413810, -2147392000, -2147371864] ✅\n",
      "model.layers.15.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 0.00% | unique[:10]: [-2147126200, -2147104784, -2145193456, -2143287546, -2142812939, -2135903124, -2133260372, -2133168124, -2126474255, -2125257703] ✅\n",
      "model.layers.15.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 0.26% | unique[:10]: [-2147483648, -2147483639, -2147482432, -2147478593, -2147478528, -2147476200, -2147467260, -2147450665, -2147425280, -2147401728] ✅\n",
      "model.layers.15.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 0.29% | unique[:10]: [-2147483648, -2147297351, -2140208832, -2140145842, -2139109636, -2117630064, -2117451152, -2115905696, -2111823096, -2111111115] ✅\n",
      "model.layers.16.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 29.56% | unique[:10]: [-2147483525, -2147482721, -2147482336, -2147481532, -2147481195, -2147481170, -2147480371, -2147469772, -2147469114, -2147462052] ✅\n",
      "model.layers.16.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 83.14% | unique[:10]: [-2147483360, -2147483118, -2147479488, -2147475781, -2147416816, -2147316768, -2145585307, -2138751580, -2135982111, -2131247280] ✅\n",
      "model.layers.16.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 66.62% | unique[:10]: [-2146361326, -2144378947, -2144243737, -2143439881, -2142666888, -2142264715, -2140954664, -2139587789, -2130701824, -2120271460] ✅\n",
      "model.layers.16.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 0.34% | unique[:10]: [-2147469520, -2145811970, -2143322826, -2139410598, -2127101899, -2122582248, -2110661081, -2091593090, -2079845967, -2047911905] ✅\n",
      "model.layers.16.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 0.01% | unique[:10]: [-2147483648, -2147483584, -2147412082, -2147399693, -2147351774, -2146913485, -2146763382, -2146171069, -2145838574, -2145551616] ✅\n",
      "model.layers.16.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 0.33% | unique[:10]: [-2147483060, -2147481158, -2146872320, -2145131136, -2144305148, -2124227808, -2105936824, -2095055056, -2051859280, -2034761968] ✅\n",
      "model.layers.17.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 27.60% | unique[:10]: [-2147482803, -2147482311, -2147481269, -2147474576, -2147463373, -2147450879, -2147442576, -2147440784, -2147437783, -2147432064] ✅\n",
      "model.layers.17.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 58.26% | unique[:10]: [-2147481344, -2147471410, -2147303119, -2147255296, -2146967584, -2146369520, -2146365384, -2145966256, -2143305728, -2143125938] ✅\n",
      "model.layers.17.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 49.99% | unique[:10]: [-2147482260, -2147475559, -2147454048, -2147453096, -2147451968, -2147451451, -2147440999, -2147432907, -2147415293, -2147360208] ✅\n",
      "model.layers.17.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 0.10% | unique[:10]: [-2147032060, -2146151424, -2144322777, -2144169914, -2136341353, -2118862048, -2100419882, -2093580175, -2092959250, -2092105801] ✅\n",
      "model.layers.17.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 0.05% | unique[:10]: [-2147057538, -2147041538, -2147024841, -2146820096, -2146701913, -2146482065, -2146303355, -2146207479, -2145369920, -2145237918] ✅\n",
      "model.layers.17.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 0.30% | unique[:10]: [-2147471496, -2147446640, -2147407040, -2147406080, -2145456130, -2140121081, -2139156308, -2135083708, -2133916862, -2123276288] ✅\n",
      "model.layers.18.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 34.74% | unique[:10]: [-2147483624, -2147483132, -2147482980, -2147482096, -2147480336, -2147469307, -2147467228, -2147456651, -2147445136, -2147436941] ✅\n",
      "model.layers.18.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 33.30% | unique[:10]: [-2147483552, -2147483392, -2147479552, -2147469657, -2147463165, -2147446485, -2147418112, -2147418096, -2147417855, -2147414016] ✅\n",
      "model.layers.18.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 25.03% | unique[:10]: [-2147480268, -2147462397, -2147454936, -2147449600, -2147448832, -2147441153, -2147417343, -2147319499, -2147148166, -2147129766] ✅\n",
      "model.layers.18.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 2.99% | unique[:10]: [-2147478572, -2147360931, -2141148064, -2140402688, -2080909376, -2076530828, -2064973806, -2063533721, -2047950869, -2045701840] ✅\n",
      "model.layers.18.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 8.31% | unique[:10]: [-2147482912, -2147482896, -2147083767, -2147011432, -2146967546, -2146564079, -2146470314, -2146453119, -2144786309, -2143355437] ✅\n",
      "model.layers.18.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 4.24% | unique[:10]: [-2147470496, -2147385390, -2144348397, -2144338951, -2143356044, -2136762093, -2112507397, -2096530532, -2052555888, -2038579200] ✅\n",
      "model.layers.19.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 41.72% | unique[:10]: [-2147483494, -2147483296, -2147482032, -2147481344, -2147481184, -2147474632, -2147466749, -2147463166, -2147459176, -2147459038] ✅\n",
      "model.layers.19.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 74.91% | unique[:10]: [-2147483483, -2147480248, -2147471006, -2147467680, -2147463152, -2147429885, -2147427052, -2147355814, -2147342768, -2147286118] ✅\n",
      "model.layers.19.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 50.01% | unique[:10]: [-2147483536, -2147483416, -2147483392, -2147482316, -2147481754, -2147465436, -2147232766, -2147217653, -2147148544, -2147143927] ✅\n",
      "model.layers.19.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 0.01% | unique[:10]: [-2146850048, -2146769038, -2145773312, -2142813184, -2134899025, -2134140496, -2131554296, -2128916096, -2128234795, -2126459648] ✅\n",
      "model.layers.19.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 0.00% | unique[:10]: [-2147483576, -2147481607, -2147085796, -2146998277, -2146694387, -2146624619, -2146566314, -2146434589, -2146339849, -2146225924] ✅\n",
      "model.layers.19.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 0.31% | unique[:10]: [-2147483648, -2147483630, -2147479172, -2147438064, -2147218019, -2146869872, -2145511936, -2144380704, -2144283470, -2143967560] ✅\n",
      "model.layers.2.mlp.down_proj.pt          | shape: (8960, 192)          | zeros: 11.47% | unique[:10]: [-2147483648, -2147477201, -2147467872, -2147320519, -2147090672, -2147024911, -2146972416, -2146962981, -2146903088, -2146454171] ✅\n",
      "model.layers.2.mlp.gate_proj.pt          | shape: (1536, 1120)         | zeros: 33.41% | unique[:10]: [-2147483637, -2147483404, -2147477187, -2147466750, -2147461614, -2147459713, -2147417326, -2147405179, -2147393557, -2147390606] ✅\n",
      "model.layers.2.mlp.up_proj.pt            | shape: (1536, 1120)         | zeros: 25.70% | unique[:10]: [-2147483648, -2147483517, -2147481408, -2147480288, -2147458560, -2147454057, -2147451152, -2147444613, -2147443344, -2147430400] ✅\n",
      "model.layers.2.self_attn.k_proj.pt       | shape: (1536, 32)           | zeros: 0.02% | unique[:10]: [-2147483642, -2146917264, -2143091072, -2143055022, -2137608174, -2132794079, -2132117781, -2110577216, -2096102347, -2095349683] ✅\n",
      "model.layers.2.self_attn.q_proj.pt       | shape: (1536, 192)          | zeros: 0.03% | unique[:10]: [-2147483648, -2147483496, -2147483056, -2147369105, -2146830176, -2146303632, -2145934864, -2145910569, -2145704674, -2145213614] ✅\n",
      "model.layers.2.self_attn.v_proj.pt       | shape: (1536, 32)           | zeros: 0.46% | unique[:10]: [-2146599296, -2145750502, -2135780683, -2128609838, -2127491053, -2123862519, -2117026279, -2111796984, -2110277344, -2095188112] ✅\n",
      "model.layers.20.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 38.90% | unique[:10]: [-2147482320, -2147475525, -2147467256, -2147441895, -2147417197, -2147319836, -2147318619, -2147293384, -2147262420, -2147219629] ✅\n",
      "model.layers.20.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 41.82% | unique[:10]: [-2147483392, -2147483376, -2147479551, -2147479296, -2147470327, -2147469504, -2147450064, -2147432857, -2147418096, -2147344347] ✅\n",
      "model.layers.20.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 58.38% | unique[:10]: [-2147483648, -2147483611, -2147467227, -2147407708, -2147391168, -2147350794, -2147334317, -2147330986, -2147330813, -2147291117] ✅\n",
      "model.layers.20.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 1.31% | unique[:10]: [-2147475102, -2141768363, -2140200203, -2136867914, -2116875248, -2111620817, -2104755360, -2080372093, -2055401456, -2039471832] ✅\n",
      "model.layers.20.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 0.01% | unique[:10]: [-2147483616, -2147483482, -2147483392, -2147482632, -2147482448, -2147482364, -2147480928, -2147459267, -2147452928, -2147328624] ✅\n",
      "model.layers.20.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 0.28% | unique[:10]: [-2147483648, -2147458048, -2147287136, -2147266560, -2146439984, -2137653248, -2133918725, -2130706432, -2129514203, -2120471392] ✅\n",
      "model.layers.21.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 40.30% | unique[:10]: [-2147483614, -2147479232, -2147477712, -2147476208, -2147472519, -2147471357, -2147403309, -2147386508, -2147365376, -2147358464] ✅\n",
      "model.layers.21.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 75.20% | unique[:10]: [-2147483648, -2147479552, -2147466734, -2147414013, -2147409904, -2147396572, -2147261149, -2147014752, -2147007725, -2146496127] ✅\n",
      "model.layers.21.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 51.07% | unique[:10]: [-2147483621, -2147477552, -2147450878, -2147437533, -2147352539, -2147348386, -2147343774, -2147340207, -2147097850, -2147015424] ✅\n",
      "model.layers.21.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 0.00% | unique[:10]: [-2146436139, -2145962592, -2133577972, -2130570749, -2116939938, -2115698688, -2097148968, -2096891080, -2085098692, -2051180933] ✅\n",
      "model.layers.21.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 0.61% | unique[:10]: [-2147482690, -2147480592, -2147288096, -2147256832, -2146904789, -2146893300, -2146832050, -2145315821, -2145058301, -2144886208] ✅\n",
      "model.layers.21.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 0.26% | unique[:10]: [-2146758592, -2141948672, -2141844482, -2141554359, -2139358049, -2138557544, -2130110198, -2129541376, -2127306480, -2116349420] ✅\n",
      "model.layers.22.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 34.97% | unique[:10]: [-2147481609, -2147476736, -2147473647, -2147453623, -2147443280, -2147436743, -2147422319, -2147420298, -2147402753, -2147402200] ✅\n",
      "model.layers.22.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 68.44% | unique[:10]: [-2147352064, -2146369536, -1879048192, -1610612736, -1610547200, -1610539008, -1610416128, -1593835264, -1577058304, -1342177280] ✅\n",
      "model.layers.22.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 58.58% | unique[:10]: [-2147483392, -2147478630, -2147478480, -2147401801, -2147401180, -2147347712, -2147287017, -2147286781, -2147284368, -2147276719] ✅\n",
      "model.layers.22.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 8.11% | unique[:10]: [-2147482784, -2145294249, -2141217099, -2139949062, -2137965846, -2129725440, -2126546091, -2124278905, -2084793760, -2076674060] ✅\n",
      "model.layers.22.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 8.33% | unique[:10]: [-2147483648, -2147483497, -2147483152, -2147482512, -2147481603, -2147481184, -2147479600, -2147430912, -2147423104, -2147403680] ✅\n",
      "model.layers.22.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 8.44% | unique[:10]: [-2147483648, -2147483488, -2147483241, -2147475664, -2147471560, -2147434537, -2147410439, -2146799616, -2146113280, -2141648151] ✅\n",
      "model.layers.23.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 29.75% | unique[:10]: [-2147483647, -2147477547, -2147459888, -2147440400, -2147425328, -2147423142, -2147416886, -2147366400, -2147339466, -2146992192] ✅\n",
      "model.layers.23.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 60.48% | unique[:10]: [-2147482377, -2147480328, -2147421332, -2147404997, -2147403822, -2147324872, -2147025408, -2147020590, -2147019344, -2147000394] ✅\n",
      "model.layers.23.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 52.62% | unique[:10]: [-2147483170, -2147482624, -2147482284, -2147474876, -2147438517, -2147336160, -2147327904, -2147282336, -2147270652, -2147265503] ✅\n",
      "model.layers.23.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 0.04% | unique[:10]: [-2147482155, -2147450880, -2147450752, -2147440896, -2145370080, -2145176341, -2144323765, -2136820992, -2135639867, -2130673664] ✅\n",
      "model.layers.23.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 0.20% | unique[:10]: [-2147483648, -2147483510, -2147482528, -2147479825, -2147473645, -2147471315, -2147449347, -2147442688, -2146934630, -2146714765] ✅\n",
      "model.layers.23.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 0.44% | unique[:10]: [-2147483648, -2147483571, -2147483392, -2147479785, -2147478672, -2147477248, -2147462656, -2147460464, -2147450952, -2147424848] ✅\n",
      "model.layers.24.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 42.73% | unique[:10]: [-2147480841, -2147479794, -2147473752, -2147458647, -2147445256, -2147409920, -2147354893, -2147345290, -2147343595, -2147319955] ✅\n",
      "model.layers.24.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 91.66% | unique[:10]: [-2147429766, -2147265780, -2146029809, -2145550336, -2145536234, -2145100894, -2144915280, -2144890713, -2144749808, -2143942217] ⚠️ HIGH ZERO RATIO!\n",
      "model.layers.24.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 62.77% | unique[:10]: [-2147483648, -2147466688, -2147450877, -2147446736, -2147446005, -2147440241, -2147409544, -2147283456, -2147278784, -2147204936] ✅\n",
      "model.layers.24.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 0.00% | unique[:10]: [-2147483424, -2147478724, -2147448576, -2147444736, -2147380266, -2147191564, -2146434541, -2142546176, -2138355712, -2134892680] ✅\n",
      "model.layers.24.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 0.02% | unique[:10]: [-2147483648, -2147483639, -2147483136, -2147482736, -2147481459, -2147467231, -2147454887, -2147436684, -2147425536, -2147239232] ✅\n",
      "model.layers.24.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 0.51% | unique[:10]: [-2147483648, -2147483339, -2147481952, -2147480320, -2147474692, -2147426416, -2147028878, -2146680832, -2145824711, -2145799968] ✅\n",
      "model.layers.25.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 41.36% | unique[:10]: [-2147453496, -2147287064, -2147273051, -2147190170, -2147046416, -2146978714, -2146953696, -2146890272, -2146829504, -2146799608] ✅\n",
      "model.layers.25.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 79.44% | unique[:10]: [-2147483631, -2147483391, -2147483120, -2147479551, -2147476707, -2146477357, -2146435070, -2146286342, -2143915528, -2142374071] ✅\n",
      "model.layers.25.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 58.99% | unique[:10]: [-2147397897, -2146927110, -2146765236, -2146363023, -2145549316, -2144640070, -2142999562, -2141489924, -2139275610, -2138414848] ✅\n",
      "model.layers.25.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 0.01% | unique[:10]: [-2147483079, -2147330395, -2147196670, -2146182702, -2140439084, -2130706507, -2124944272, -2113855037, -2113732349, -2108592128] ✅\n",
      "model.layers.25.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 0.13% | unique[:10]: [-2147483648, -2147483635, -2147483633, -2147483627, -2147483623, -2147483504, -2147483476, -2147483296, -2147482398, -2147482112] ✅\n",
      "model.layers.25.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 0.27% | unique[:10]: [-2147483648, -2147449088, -2147445502, -2146827724, -2146749591, -2146040704, -2145648257, -2143832480, -2143418609, -2143079424] ✅\n",
      "model.layers.26.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 38.62% | unique[:10]: [-2147483584, -2147482057, -2147466576, -2147461533, -2147461349, -2147389696, -2147385504, -2147350923, -2147266091, -2147233723] ✅\n",
      "model.layers.26.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 55.52% | unique[:10]: [-2147483632, -2147483373, -2147480832, -2147459058, -2147452934, -2147450207, -2147352304, -2147255549, -2147236352, -2147090393] ✅\n",
      "model.layers.26.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 64.10% | unique[:10]: [-2147483061, -2147481749, -2147480683, -2147480672, -2147462218, -2147447213, -2147441320, -2147430048, -2147429585, -2147361777] ✅\n",
      "model.layers.26.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 8.39% | unique[:10]: [-2147480912, -2147155872, -2146328576, -2144122872, -2141913088, -2141351754, -2140151808, -2139488128, -2127450112, -2110384336] ✅\n",
      "model.layers.26.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 8.40% | unique[:10]: [-2147483648, -2147483515, -2147483508, -2147483431, -2147481776, -2147479440, -2147479312, -2147477769, -2147451406, -2147451088] ✅\n",
      "model.layers.26.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 8.42% | unique[:10]: [-2147483648, -2147479340, -2147426348, -2147421296, -2147279112, -2146718965, -2146539529, -2137779079, -2133195840, -2129792816] ✅\n",
      "model.layers.27.mlp.down_proj.pt         | shape: (8960, 192)          | zeros: 39.87% | unique[:10]: [-2147483648, -2147483627, -2147483520, -2147483468, -2147473918, -2147471792, -2147469802, -2147466496, -2147457058, -2147441152] ✅\n",
      "model.layers.27.mlp.gate_proj.pt         | shape: (1536, 1120)         | zeros: 65.80% | unique[:10]: [-2147483376, -2147479552, -2147469770, -2147467264, -2147418111, -2147414016, -2147409920, -2147348063, -2146426079, -2146418688] ✅\n",
      "model.layers.27.mlp.up_proj.pt           | shape: (1536, 1120)         | zeros: 65.92% | unique[:10]: [-2147483648, -2147483632, -2147475453, -2147450112, -2147449732, -2147446716, -2147417583, -2147286987, -2147258632, -2146944944] ✅\n",
      "model.layers.27.self_attn.k_proj.pt      | shape: (1536, 32)           | zeros: 0.02% | unique[:10]: [-2147483360, -2146519074, -2145652736, -2145452001, -2145189834, -2135555152, -2131484680, -2122369056, -2118029312, -2113929083] ✅\n",
      "model.layers.27.self_attn.q_proj.pt      | shape: (1536, 192)          | zeros: 0.10% | unique[:10]: [-2147483648, -2147483644, -2147483632, -2147483613, -2147483593, -2147483579, -2147483501, -2147483467, -2147483408, -2147483136] ✅\n",
      "model.layers.27.self_attn.v_proj.pt      | shape: (1536, 32)           | zeros: 0.05% | unique[:10]: [-2147479840, -2147448864, -2147323904, -2146590727, -2145929136, -2145315141, -2143361200, -2137450531, -2132853950, -2127567361] ✅\n",
      "model.layers.3.mlp.down_proj.pt          | shape: (8960, 192)          | zeros: 26.53% | unique[:10]: [-2147483419, -2147483360, -2147480935, -2147477157, -2147433921, -2147412486, -2147404288, -2147401699, -2147394598, -2147393191] ✅\n",
      "model.layers.3.mlp.gate_proj.pt          | shape: (1536, 1120)         | zeros: 16.72% | unique[:10]: [-2147476633, -2147470800, -2147469003, -2147422448, -2147404972, -2147398363, -2147378188, -2147377170, -2147350784, -2147347935] ✅\n",
      "model.layers.3.mlp.up_proj.pt            | shape: (1536, 1120)         | zeros: 33.72% | unique[:10]: [-2147483648, -2147483389, -2147481408, -2147462604, -2147457072, -2147449854, -2147410068, -2147405520, -2147213032, -2147208365] ✅\n",
      "model.layers.3.self_attn.k_proj.pt       | shape: (1536, 32)           | zeros: 0.00% | unique[:10]: [-2146543571, -2146436496, -2120237635, -2110431136, -2080770579, -2080057418, -2069258372, -2063391195, -2054439296, -2052062632] ✅\n",
      "model.layers.3.self_attn.q_proj.pt       | shape: (1536, 192)          | zeros: 0.01% | unique[:10]: [-2147482368, -2147440113, -2147422352, -2147178630, -2146519744, -2146438256, -2145701594, -2145125184, -2144655577, -2143176178] ✅\n",
      "model.layers.3.self_attn.v_proj.pt       | shape: (1536, 32)           | zeros: 0.28% | unique[:10]: [-2128898056, -2118123525, -2114191503, -2097184911, -2089802752, -2086663344, -2063205419, -2062034950, -2055211205, -2044067920] ✅\n",
      "model.layers.4.mlp.down_proj.pt          | shape: (8960, 192)          | zeros: 29.77% | unique[:10]: [-2147483648, -2147483389, -2147481856, -2147476993, -2147472429, -2147466830, -2147422168, -2147405310, -2147405213, -2147335390] ✅\n",
      "model.layers.4.mlp.gate_proj.pt          | shape: (1536, 1120)         | zeros: 49.98% | unique[:10]: [-2147475422, -2147471325, -2147452765, -2147447728, -2147438790, -2147420295, -2147344672, -2147287136, -2147286208, -2147281529] ✅\n",
      "model.layers.4.mlp.up_proj.pt            | shape: (1536, 1120)         | zeros: 50.06% | unique[:10]: [-2147483633, -2147482361, -2147478987, -2147476706, -2147476614, -2147475386, -2147471358, -2147471114, -2147457192, -2147445424] ✅\n",
      "model.layers.4.self_attn.k_proj.pt       | shape: (1536, 32)           | zeros: 0.00% | unique[:10]: [-2146101708, -2143232432, -2137728071, -2137129024, -2136253995, -2134377984, -2131982443, -2127085531, -2118667496, -2117726912] ✅\n",
      "model.layers.4.self_attn.q_proj.pt       | shape: (1536, 192)          | zeros: 0.03% | unique[:10]: [-2147483648, -2147443189, -2147323649, -2147262720, -2147245836, -2146963563, -2146956989, -2146946726, -2146695461, -2146647968] ✅\n",
      "model.layers.4.self_attn.v_proj.pt       | shape: (1536, 32)           | zeros: 0.30% | unique[:10]: [-2139410439, -2135009408, -2133714733, -2130752000, -2127561095, -2126045158, -2124701664, -2117139968, -2101502510, -2091401236] ✅\n",
      "model.layers.5.mlp.down_proj.pt          | shape: (8960, 192)          | zeros: 41.93% | unique[:10]: [-2147479977, -2147471589, -2147465843, -2147462368, -2147459213, -2147455785, -2147407903, -2147371461, -2147364083, -2147329319] ✅\n",
      "model.layers.5.mlp.gate_proj.pt          | shape: (1536, 1120)         | zeros: 62.05% | unique[:10]: [-2147483648, -2147480264, -2147469552, -2147437229, -2147434629, -2147405790, -2147352890, -2147331936, -2147317504, -2147213178] ✅\n",
      "model.layers.5.mlp.up_proj.pt            | shape: (1536, 1120)         | zeros: 62.99% | unique[:10]: [-2147483633, -2147483584, -2147483165, -2147482553, -2147465230, -2147453432, -2147439914, -2147426804, -2147420080, -2147373053] ✅\n",
      "model.layers.5.self_attn.k_proj.pt       | shape: (1536, 32)           | zeros: 0.00% | unique[:10]: [-2147335680, -2136416576, -2132852736, -2096693242, -2046805645, -2046029724, -2043464180, -2023554992, -2014364774, -2005698841] ✅\n",
      "model.layers.5.self_attn.q_proj.pt       | shape: (1536, 192)          | zeros: 0.06% | unique[:10]: [-2147483538, -2147483454, -2147482152, -2147258608, -2147127194, -2147073992, -2146924896, -2146551439, -2146442755, -2146353072] ✅\n",
      "model.layers.5.self_attn.v_proj.pt       | shape: (1536, 32)           | zeros: 0.37% | unique[:10]: [-2147483648, -2147483198, -2147464192, -2147452400, -2146953584, -2146903383, -2143080442, -2142339936, -2140141552, -2137915392] ✅\n",
      "model.layers.6.mlp.down_proj.pt          | shape: (8960, 192)          | zeros: 41.85% | unique[:10]: [-2147477040, -2147437160, -2147413967, -2147386384, -2147303906, -2147216752, -2146963664, -2146861199, -2146828613, -2146732032] ✅\n",
      "model.layers.6.mlp.gate_proj.pt          | shape: (1536, 1120)         | zeros: 33.37% | unique[:10]: [-2147481760, -2147480825, -2147480012, -2147479920, -2147479296, -2147475389, -2147454855, -2147454557, -2147436560, -2147413070] ✅\n",
      "model.layers.6.mlp.up_proj.pt            | shape: (1536, 1120)         | zeros: 75.06% | unique[:10]: [-2147471302, -2147445408, -2147436459, -2147352055, -2147332044, -2147325104, -2147317280, -2147286461, -2147273897, -2147003382] ✅\n",
      "model.layers.6.self_attn.k_proj.pt       | shape: (1536, 32)           | zeros: 8.36% | unique[:10]: [-2147403544, -2147355053, -2146810360, -2146582535, -2142240716, -2128871942, -2128343006, -2123647255, -2123295408, -2084868104] ✅\n",
      "model.layers.6.self_attn.q_proj.pt       | shape: (1536, 192)          | zeros: 16.46% | unique[:10]: [-2147483648, -2147394976, -2147392774, -2147336186, -2147258312, -2147240164, -2147024752, -2146862147, -2146836977, -2146667876] ✅\n",
      "model.layers.6.self_attn.v_proj.pt       | shape: (1536, 32)           | zeros: 6.53% | unique[:10]: [-2146862848, -2146276872, -2139802424, -2130672384, -2129923584, -2122982624, -2122599136, -2113658111, -2113372160, -2112194823] ✅\n",
      "model.layers.7.mlp.down_proj.pt          | shape: (8960, 192)          | zeros: 39.29% | unique[:10]: [-2147483623, -2147483468, -2147482876, -2147482345, -2147481870, -2147475424, -2147475229, -2147474704, -2147474620, -2147474383] ✅\n",
      "model.layers.7.mlp.gate_proj.pt          | shape: (1536, 1120)         | zeros: 66.66% | unique[:10]: [-2147483392, -2147478006, -2147473914, -2147464184, -2147462476, -2147351281, -2147318328, -2147151866, -2147061752, -2146959216] ✅\n",
      "model.layers.7.mlp.up_proj.pt            | shape: (1536, 1120)         | zeros: 58.47% | unique[:10]: [-2147483645, -2147479545, -2147473758, -2147452160, -2147418112, -2147377736, -2147287008, -2147184320, -2147090381, -2147089019] ✅\n",
      "model.layers.7.self_attn.k_proj.pt       | shape: (1536, 32)           | zeros: 0.00% | unique[:10]: [-2147176484, -2127552495, -2110260480, -2105573456, -2096844640, -2087587328, -2048983081, -2030040889, -2027963031, -2019430018] ✅\n",
      "model.layers.7.self_attn.q_proj.pt       | shape: (1536, 192)          | zeros: 0.02% | unique[:10]: [-2147483648, -2147483621, -2147482373, -2147477856, -2147447296, -2147375808, -2147311196, -2147307216, -2146844328, -2146837276] ✅\n",
      "model.layers.7.self_attn.v_proj.pt       | shape: (1536, 32)           | zeros: 0.19% | unique[:10]: [-2146863566, -2143289344, -2141869280, -2138729216, -2130747099, -2130583563, -2128839680, -2104512624, -2034813192, -2034597929] ✅\n",
      "model.layers.8.mlp.down_proj.pt          | shape: (8960, 192)          | zeros: 32.54% | unique[:10]: [-2147483615, -2147483612, -2147483553, -2147482061, -2147471355, -2147469952, -2147469785, -2147457988, -2147445150, -2147405306] ✅\n",
      "model.layers.8.mlp.gate_proj.pt          | shape: (1536, 1120)         | zeros: 91.66% | unique[:10]: [-1879047935, -1862270974, -1610612448, -1609560064, -1342177022, -1342176751, -1342111743, -1342111472, -1342103551, -1341128703] ⚠️ HIGH ZERO RATIO!\n",
      "model.layers.8.mlp.up_proj.pt            | shape: (1536, 1120)         | zeros: 42.27% | unique[:10]: [-2147451136, -2147373056, -2147372048, -2147344440, -2147327075, -2147242080, -2147164005, -2147094181, -2144381985, -2144004064] ✅\n",
      "model.layers.8.self_attn.k_proj.pt       | shape: (1536, 32)           | zeros: 8.45% | unique[:10]: [-2147406933, -2147082240, -2146703552, -2134905040, -2126276112, -2118124749, -2070945895, -2024873648, -2015915088, -2011868682] ✅\n",
      "model.layers.8.self_attn.q_proj.pt       | shape: (1536, 192)          | zeros: 8.37% | unique[:10]: [-2147483648, -2147483521, -2147262656, -2146992128, -2146981552, -2142829344, -2140364976, -2138306240, -2137662879, -2135411905] ✅\n",
      "model.layers.8.self_attn.v_proj.pt       | shape: (1536, 32)           | zeros: 8.71% | unique[:10]: [-2147483648, -2147483386, -2147400333, -2147209216, -2144837632, -2142003339, -2141227796, -2141046989, -2131525830, -2129854600] ✅\n",
      "model.layers.9.mlp.down_proj.pt          | shape: (8960, 192)          | zeros: 28.23% | unique[:10]: [-2147483648, -2147483073, -2147481776, -2147481761, -2147474873, -2147474091, -2147473785, -2147465984, -2147464272, -2147457979] ✅\n",
      "model.layers.9.mlp.gate_proj.pt          | shape: (1536, 1120)         | zeros: 49.98% | unique[:10]: [-2147483097, -2147481237, -2147481096, -2147475015, -2147467149, -2147463853, -2147450880, -2147444805, -2147442721, -2147390107] ✅\n",
      "model.layers.9.mlp.up_proj.pt            | shape: (1536, 1120)         | zeros: 41.81% | unique[:10]: [-2147478240, -2147472842, -2147469767, -2147466183, -2147464227, -2147448464, -2147429068, -2147394762, -2147364480, -2147351216] ✅\n",
      "model.layers.9.self_attn.k_proj.pt       | shape: (1536, 32)           | zeros: 0.10% | unique[:10]: [-2147442864, -2146762652, -2142262422, -2141891781, -2139427274, -2138322146, -2130579456, -2124548872, -2124291335, -2123373702] ✅\n",
      "model.layers.9.self_attn.q_proj.pt       | shape: (1536, 192)          | zeros: 0.02% | unique[:10]: [-2147483648, -2147476703, -2147449600, -2147353198, -2147315902, -2147219344, -2147098624, -2147028884, -2147026899, -2147020373] ✅\n",
      "model.layers.9.self_attn.v_proj.pt       | shape: (1536, 32)           | zeros: 0.34% | unique[:10]: [-2147179008, -2147146926, -2146151187, -2146025472, -2145881649, -2133983298, -2130609298, -2104476848, -2097143806, -2080508085] ✅\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def inspect_qweights_in_dir(layer_dir: Path, zero_threshold: float = 0.9):\n",
    "    \"\"\"\n",
    "    Inspect qweight statistics for all saved quantized layer files in a directory.\n",
    "\n",
    "    Args:\n",
    "        layer_dir (Path): Directory containing *.pt quantized layer files.\n",
    "        zero_threshold (float): Warn if percentage of zeros in qweight exceeds this.\n",
    "    \"\"\"\n",
    "    layer_dir = Path(layer_dir).expanduser()\n",
    "    if not layer_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {layer_dir}\")\n",
    "\n",
    "    pt_files = sorted(layer_dir.glob(\"*.pt\"))\n",
    "    if not pt_files:\n",
    "        print(\"❌ No .pt layer files found in directory.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n🔍 Inspecting {len(pt_files)} layer files in: {layer_dir}\\n\")\n",
    "\n",
    "    for f in pt_files:\n",
    "        try:\n",
    "            state_dict = torch.load(f, map_location=\"cpu\")\n",
    "            qweight = state_dict.get(\"qweight\", None)\n",
    "\n",
    "            if qweight is None:\n",
    "                print(f\"⚠️  {f.name}: Missing `qweight` key.\")\n",
    "                continue\n",
    "\n",
    "            zeros = (qweight == 0).sum().item()\n",
    "            total = qweight.numel()\n",
    "            zero_pct = zeros / total\n",
    "\n",
    "            unique_vals = torch.unique(qweight)\n",
    "            preview_vals = unique_vals.tolist()[:10]\n",
    "\n",
    "            flag = \"⚠️ HIGH ZERO RATIO!\" if zero_pct > zero_threshold else \"✅\"\n",
    "\n",
    "            # ✅ FIX: Convert tuple to string before formatting\n",
    "            shape_str = str(tuple(qweight.shape))\n",
    "\n",
    "            print(\n",
    "                f\"{f.name:<40} | shape: {shape_str:<20} | \"\n",
    "                f\"zeros: {zero_pct:.2%} | unique[:10]: {preview_vals} {flag}\"\n",
    "            )\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {f.name}: Error loading or parsing file — {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage — adjust this path if needed\n",
    "    quantized_dir = Path(\"~/models/deepseek-awq-scrooge/quantized_layers\")\n",
    "    inspect_qweights_in_dir(quantized_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba852846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contents of model.layers.0.self_attn.q_proj.pt:\n",
      "  qweight    → shape: (1536, 192), dtype: torch.int32\n",
      "  qzeros     → shape: (12, 192), dtype: torch.int32\n",
      "  scales     → shape: (12, 1536), dtype: torch.float16\n",
      "  bias       → shape: (1536,), dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the file\n",
    "base_dir = Path(\"/home/xzhang/models/deepseek-awq-scrooge/quantized_layers\")\n",
    "layer_file = \"model.layers.0.self_attn.q_proj.pt\"\n",
    "target_file = base_dir / layer_file\n",
    "\n",
    "# Load the file\n",
    "data = torch.load(target_file, map_location=\"cpu\")\n",
    "\n",
    "print(f\"\\nContents of {layer_file}:\")\n",
    "\n",
    "if isinstance(data, dict):\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"  {key:<10} → shape: {tuple(value.shape)}, dtype: {value.dtype}\")\n",
    "        else:\n",
    "            print(f\"  {key:<10} → type: {type(value).__name__}\")\n",
    "else:\n",
    "    print(\"File content is not a dict.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ea438b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model.layers.0.self_attn.q_proj.pt\n",
      "--------------------------------------------------\n",
      "Group size: 128\n",
      "Scales range: 0.1963 - 3.5938\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 128, 1536]' is invalid for input of size 18432",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m base_dir = Path(\u001b[33m\"\u001b[39m\u001b[33m/home/xzhang/models/deepseek-awq-scrooge/quantized_layers\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m layer_file = \u001b[33m\"\u001b[39m\u001b[33mmodel.layers.0.self_attn.q_proj.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43manalyze_quantization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36manalyze_quantization\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScales range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscales.min().item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscales.max().item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Properly expand dimensions for broadcasting\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m scales = \u001b[43mscales\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscales\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [groups, group_size, out_features]\u001b[39;00m\n\u001b[32m     30\u001b[39m scales = scales.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# [groups, out_features, group_size]\u001b[39;00m\n\u001b[32m     31\u001b[39m scales = scales.reshape(-\u001b[32m1\u001b[39m, scales.shape[-\u001b[32m1\u001b[39m])  \u001b[38;5;66;03m# [groups*out_features, group_size]\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[-1, 128, 1536]' is invalid for input of size 18432"
     ]
    }
   ],
   "source": [
    "# check_quantization_efficiency.py\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def analyze_quantization(file_path):\n",
    "    data = torch.load(file_path)\n",
    "\n",
    "    if not isinstance(data, dict):\n",
    "        print(\"Error: Expected quantized layer dictionary\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nAnalyzing {file_path.name}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Extract parameters\n",
    "    qweight = data[\"qweight\"]  # shape: [in_features, out_features//8]\n",
    "    scales = data[\"scales\"]  # shape: [in_features//group_size, out_features]\n",
    "    qzeros = data[\"qzeros\"]  # shape: [in_features//group_size, out_features//8]\n",
    "\n",
    "    group_size = qweight.shape[0] // scales.shape[0]\n",
    "    print(f\"Group size: {group_size}\")\n",
    "    print(f\"Scales range: {scales.min().item():.4f} - {scales.max().item():.4f}\")\n",
    "\n",
    "    # Properly expand dimensions for broadcasting\n",
    "    scales = scales.view(\n",
    "        -1, group_size, scales.shape[-1]\n",
    "    )  # [groups, group_size, out_features]\n",
    "    scales = scales.transpose(1, 2)  # [groups, out_features, group_size]\n",
    "    scales = scales.reshape(-1, scales.shape[-1])  # [groups*out_features, group_size]\n",
    "\n",
    "    qzeros = qzeros.view(-1, 1, qzeros.shape[-1])  # [groups, 1, out_features//8]\n",
    "    qzeros = qzeros.expand(-1, group_size, -1)  # [groups, group_size, out_features//8]\n",
    "    qzeros = qzeros.reshape(-1, qzeros.shape[-1])  # [in_features, out_features//8]\n",
    "\n",
    "    # Dequantize sample weights\n",
    "    sample_qweight = qweight[:group_size]  # First group only for demo\n",
    "    sample_qzeros = qzeros[:group_size]\n",
    "    sample_scales = scales[:group_size]\n",
    "\n",
    "    dequant_weight = (sample_qweight - sample_qzeros) * sample_scales\n",
    "    print(\n",
    "        f\"Sample dequantized range: {dequant_weight.min().item():.4f} - {dequant_weight.max().item():.4f}\"\n",
    "    )\n",
    "\n",
    "    # Check 4-bit utilization\n",
    "    quantized_values = qweight.unique(sorted=True)\n",
    "    print(f\"Unique 4-bit values: {len(quantized_values)}/16\")\n",
    "    print(\n",
    "        f\"Value range: {quantized_values.min().item()} to {quantized_values.max().item()}\"\n",
    "    )\n",
    "\n",
    "    # Plot first group's weights\n",
    "    plt.hist(qweight[:group_size].cpu().flatten().numpy(), bins=16)\n",
    "    plt.title(\"4-bit Weight Values (First Group)\")\n",
    "    plt.xlabel(\"Quantized Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = Path(\"/home/xzhang/models/deepseek-awq-scrooge/quantized_layers\")\n",
    "    layer_file = \"model.layers.0.self_attn.q_proj.pt\"\n",
    "    analyze_quantization(base_dir / layer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8c0a8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model.layers.0.self_attn.q_proj.pt\n",
      "--------------------------------------------------\n",
      "Group size: 1536\n",
      "Scales range: 0.1963 - 3.5938\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1536) must match the size of tensor b (12) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     49\u001b[39m base_dir = Path(\u001b[33m\"\u001b[39m\u001b[33m/home/xzhang/models/deepseek-awq-scrooge/quantized_layers\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m layer_file = \u001b[33m\"\u001b[39m\u001b[33mmodel.layers.0.self_attn.q_proj.pt\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Change as needed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43manalyze_quantization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36manalyze_quantization\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScales range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscales.min().item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscales.max().item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Simulate dequantization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m dequant_weight = (\u001b[43mqweight\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mqzeros\u001b[49m) * scales\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     29\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDequantized weight range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdequant_weight.min().item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdequant_weight.max().item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m )\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Check 4-bit utilization\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (1536) must match the size of tensor b (12) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# check_quantization_efficiency.py\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def analyze_quantization(file_path):\n",
    "    data = torch.load(file_path)\n",
    "\n",
    "    if not isinstance(data, dict):\n",
    "        print(\"Error: Expected quantized layer dictionary (qweight, scales, qzeros)\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nAnalyzing {file_path.name}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Extract quantization parameters\n",
    "    qweight = data[\"qweight\"]\n",
    "    scales = data[\"scales\"]\n",
    "    qzeros = data[\"qzeros\"]\n",
    "    group_size = scales.shape[0] * (qweight.shape[0] // scales.shape[0])\n",
    "\n",
    "    print(f\"Group size: {group_size}\")\n",
    "    print(f\"Scales range: {scales.min().item():.4f} - {scales.max().item():.4f}\")\n",
    "\n",
    "    # Simulate dequantization\n",
    "    dequant_weight = (qweight - qzeros) * scales\n",
    "    print(\n",
    "        f\"Dequantized weight range: {dequant_weight.min().item():.4f} - {dequant_weight.max().item():.4f}\"\n",
    "    )\n",
    "\n",
    "    # Check 4-bit utilization\n",
    "    quantized_values = qweight.unique(sorted=True)\n",
    "    print(f\"Unique 4-bit values used: {len(quantized_values)}/16 possible\")\n",
    "    print(\n",
    "        f\"Value range: {quantized_values.min().item()} to {quantized_values.max().item()}\"\n",
    "    )\n",
    "\n",
    "    # Plot value distribution\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.hist(qweight.cpu().flatten().numpy(), bins=50)\n",
    "    plt.title(f\"4-bit Weight Distribution\\n{file_path.name}\")\n",
    "    plt.xlabel(\"Quantized Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = Path(\"/home/xzhang/models/deepseek-awq-scrooge/quantized_layers\")\n",
    "    layer_file = \"model.layers.0.self_attn.q_proj.pt\"  # Change as needed\n",
    "    analyze_quantization(base_dir / layer_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128964e",
   "metadata": {},
   "source": [
    "# Inspect Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40d0023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file filtered successfully. Output saved to /home/xzhang/dev/deepseek_local_runner/documents/filtered_rcs.log\n"
     ]
    }
   ],
   "source": [
    "def filter_log_file(input_file, output_file, keyword):\n",
    "    try:\n",
    "        with open(input_file, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        filtered_lines = [line for line in lines if keyword not in line]\n",
    "\n",
    "        with open(output_file, \"w\") as new_file:\n",
    "            new_file.writelines(filtered_lines)\n",
    "\n",
    "        print(f\"Log file filtered successfully. Output saved to {output_file}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Input file {input_file} not found.\")\n",
    "\n",
    "\n",
    "log_file = \"/home/xzhang/dev/deepseek_local_runner/documents/full_log_20250529_183204_resource.log\"\n",
    "filtered_log_file = \"/home/xzhang/dev/deepseek_local_runner/documents/filtered_rcs.log\"\n",
    "\n",
    "# Usage\n",
    "input_file = log_file\n",
    "output_file = filtered_log_file\n",
    "keyword = \"[AutoMonitor]\"\n",
    "\n",
    "filter_log_file(input_file, output_file, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61c892b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m hybrid_mode = \u001b[43mconfig\u001b[49m.get(\u001b[33m\"\u001b[39m\u001b[33mhybrid_mode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      2\u001b[39m full_gpu = config.get(\u001b[33m\"\u001b[39m\u001b[33mfull_gpu\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      4\u001b[39m hybrid_mode\n",
      "\u001b[31mNameError\u001b[39m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "hybrid_mode = config.get(\"hybrid_mode\", False)\n",
    "full_gpu = config.get(\"full_gpu\", False)\n",
    "\n",
    "hybrid_mode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
